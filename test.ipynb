{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n"
     ]
    }
   ],
   "source": [
    "trainDF = pd.read_csv('train.csv')\n",
    "trainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28000 entries, 0 to 27999\n",
      "Columns: 784 entries, pixel0 to pixel783\n",
      "dtypes: int64(784)\n",
      "memory usage: 167.5 MB\n"
     ]
    }
   ],
   "source": [
    "testDF = pd.read_csv('test.csv')\n",
    "testDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 70000 entries, 0 to 27999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: float64(1), int64(784)\n",
      "memory usage: 419.8 MB\n"
     ]
    }
   ],
   "source": [
    "concatDF = pd.concat([trainDF,testDF])\n",
    "concatDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, DF):\n",
    "        if 'label' in DF.columns:\n",
    "            self.label = pd.get_dummies(DF['label']).values\n",
    "            DF = DF.drop(columns=['label'])\n",
    "        self.data = DF.values/255\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x=self.data[idx].reshape(1,28,28)\n",
    "        x=torch.FloatTensor(x)\n",
    "\n",
    "        if hasattr(self,'label'):\n",
    "            y=self.label[idx]\n",
    "            y=torch.FloatTensor(y)\n",
    "\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "dataSet= MyDataset(DF=trainDF)\n",
    "testSet = MyDataset(DF= testDF)\n",
    "dataSet[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataset.Subset at 0x102caba30>,\n",
       " <torch.utils.data.dataset.Subset at 0x13f40b280>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitSet = torch.utils.data.random_split(dataSet,(0.8,0.2))\n",
    "splitSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x13f622730>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader=torch.utils.data.DataLoader(splitSet[0],batch_size=1024,sampler=torch.utils.data.RandomSampler(splitSet[0]))\n",
    "valLoader=torch.utils.data.DataLoader(splitSet[1],batch_size=1024,sampler=torch.utils.data.RandomSampler(splitSet[1]))\n",
    "testLoader = torch.utils.data.DataLoader(testSet,batch_size=1024)\n",
    "trainLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13f8ba4f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcT0lEQVR4nO3df3DU9b3v8deGJCtisjSGZJMSaPghWJF4REhzUMSSS0jPdQC5HfzRueBwcKDBKVB/nPQqaNszaXHGOnqp3jvTQr1X8EdHyJVruaPBhNImeEAYLqeaktwoYUhC4R52Q5AQks/9g+vWlQT6Dbt5Z8PzMbMzZvN9Z99+XX267PLF55xzAgBggCVZLwAAuDYRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCLZeoGv6unp0fHjx5WWliafz2e9DgDAI+ec2tvblZubq6Skvl/nDLoAHT9+XHl5edZrAACuUnNzs0aPHt3n9wddgNLS0iRJd+o7SlaK8TYAAK8uqEt79G7kv+d9iVuANm7cqOeee06tra0qKCjQSy+9pBkzZlxx7otfdktWipJ9BAgAEs7/v8Lold5GicuHEN544w2tXbtW69ev10cffaSCggKVlJToxIkT8Xg4AEACikuAnn/+eS1fvlwPP/ywvvnNb+qVV17R9ddfr1//+tfxeDgAQAKKeYDOnz+v/fv3q7i4+K8PkpSk4uJi1dbWXnJ8Z2enwuFw1A0AMPTFPEAnT55Ud3e3srOzo+7Pzs5Wa2vrJcdXVFQoEAhEbnwCDgCuDea/EbW8vFyhUChya25utl4JADAAYv4puMzMTA0bNkxtbW1R97e1tSkYDF5yvN/vl9/vj/UaAIBBLuavgFJTUzVt2jRVVVVF7uvp6VFVVZWKiopi/XAAgAQVl98HtHbtWi1ZskR33HGHZsyYoRdeeEEdHR16+OGH4/FwAIAEFJcALV68WH/5y1+0bt06tba26rbbbtPOnTsv+WACAODa5XPOOeslviwcDisQCGi25nMlBABIQBdcl6pVqVAopPT09D6PM/8UHADg2kSAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMJFsvgNj686ZpnmeSUnr69VgPTNnneeanWf/b80y3699+6L9v7lnqeSZ7y3WeZ4ZXfuh5BkMHr4AAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABNcjHQQ890xxfPMR8UveZ65Icnveaa/utyAPRSuwuE7N3me+dcZFzzPrO5a5XnG/+6/eJ7B4MQrIACACQIEADAR8wA988wz8vl8UbfJkyfH+mEAAAkuLu8B3XLLLXr//ff/+iDJvNUEAIgWlzIkJycrGAzG40cDAIaIuLwHdOTIEeXm5mrcuHF66KGHdPTo0T6P7ezsVDgcjroBAIa+mAeosLBQmzdv1s6dO/Xyyy+rqalJd911l9rb23s9vqKiQoFAIHLLy8uL9UoAgEEo5gEqLS3Vd7/7XU2dOlUlJSV69913dfr0ab355pu9Hl9eXq5QKBS5NTc3x3olAMAgFPdPB4wcOVI33XSTGhoaev2+3++X3z9wvxESADA4xP33AZ05c0aNjY3KycmJ90MBABJIzAP02GOPqaamRp9++qn++Mc/auHChRo2bJgeeOCBWD8UACCBxfyX4I4dO6YHHnhAp06d0qhRo3TnnXeqrq5Oo0aNivVDAQASWMwD9Prrr8f6R16z3L7DnmfmHHjY88xLU7Z6npGk99u9Xyx1a+XdnmfG/+qY5xn3byHPMwPp/LQJnmeO3eP9vdLF83d7npGkpzIPeZ65JdX7f04yy5s8z7S/63kEgxTXggMAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPicc856iS8Lh8MKBAKarflK9qVYr3NNGHbT+H7Ndf+5McabINZOfP/v+zX34X96Kcab9G5/p/eZ9eOmxX4RxNQF16VqVSoUCik9Pb3P43gFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABPJ1gvAHle1HrpO/12X9QqX9dD2Ms8zE1QXh01ggVdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJLkYKJIjOf5jueea3xRv7+WjDPE/Udnqfmfzcp55nLniewGDFKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQXI8WAGjYy4Hmm/duTPc+0zRjc/2+VMjHseeZ/3PELzzNjkod7npGkk92fe55Z9tvHPc+Ma6n1PIOhY3D/WwoAGLIIEADAhOcA7d69W/fee69yc3Pl8/m0ffv2qO8757Ru3Trl5ORo+PDhKi4u1pEjR2K1LwBgiPAcoI6ODhUUFGjjxt7/oKsNGzboxRdf1CuvvKK9e/dqxIgRKikp0blz5656WQDA0OH5QwilpaUqLS3t9XvOOb3wwgt66qmnNH/+fEnSq6++quzsbG3fvl3333//1W0LABgyYvoeUFNTk1pbW1VcXBy5LxAIqLCwULW1vX/apbOzU+FwOOoGABj6Yhqg1tZWSVJ2dnbU/dnZ2ZHvfVVFRYUCgUDklpeXF8uVAACDlPmn4MrLyxUKhSK35uZm65UAAAMgpgEKBoOSpLa2tqj729raIt/7Kr/fr/T09KgbAGDoi2mA8vPzFQwGVVVVFbkvHA5r7969KioqiuVDAQASnOdPwZ05c0YNDQ2Rr5uamnTw4EFlZGRozJgxWr16tX76059q4sSJys/P19NPP63c3FwtWLAglnsDABKc5wDt27dP99xzT+TrtWvXSpKWLFmizZs364knnlBHR4ceeeQRnT59Wnfeead27typ6667LnZbAwASns8556yX+LJwOKxAIKDZmq9kX4r1OriM5Jze39e7nEd//4HnmTnDz3qewUUfdvr6NffMkmWeZ5J+f6Bfj4Wh54LrUrUqFQqFLvu+vvmn4AAA1yYCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY8PzHMQBfcCOGe54p9P+b55kzPZ5H+i3F5/3/yfyD+KrtGUnn+jXXMtP7P9uv/2GY9wfq6fY+gyGDV0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAmfc85ZL/Fl4XBYgUBAszVfyYP4Io8YmlxRgeeZ1r8f4Xlm9L//1PPMP4191/NMkX/gLvZ5c/U/ep6ZuOxjzzM95/p3gVUMnAuuS9WqVCgUUnp6ep/H8QoIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBxUiBBDHs5omeZ27f+km/Hmv9qIP9mvNqUtVyzzMT/+NHcdgEscTFSAEAgxoBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKLkQJDmC8ltV9zbmeW55kdkys9z5zs/tzzzJz/8oTnmbx//qPnGfQfFyMFAAxqBAgAYMJzgHbv3q17771Xubm58vl82r59e9T3ly5dKp/PF3WbN29erPYFAAwRngPU0dGhgoICbdy4sc9j5s2bp5aWlsht69atV7UkAGDoSfY6UFpaqtLS0sse4/f7FQwG+70UAGDoi8t7QNXV1crKytKkSZO0cuVKnTp1qs9jOzs7FQ6Ho24AgKEv5gGaN2+eXn31VVVVVennP/+5ampqVFpaqu7u7l6Pr6ioUCAQiNzy8vJivRIAYBDy/EtwV3L//fdH/vrWW2/V1KlTNX78eFVXV2vOnDmXHF9eXq61a9dGvg6Hw0QIAK4Bcf8Y9rhx45SZmamGhoZev+/3+5Wenh51AwAMfXEP0LFjx3Tq1Cnl5OTE+6EAAAnE8y/BnTlzJurVTFNTkw4ePKiMjAxlZGTo2Wef1aJFixQMBtXY2KgnnnhCEyZMUElJSUwXBwAkNs8B2rdvn+65557I11+8f7NkyRK9/PLLOnTokH7zm9/o9OnTys3N1dy5c/WTn/xEfr8/dlsDABIeFyMFcImeO2/zPPP45tc8z9wz/JznmY+7ujzP/NO/e9DzjCR1H/k//Zq71nExUgDAoEaAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATMf8juQEkvqQ9Bz3PrHtmmeeZ93/2gueZm1O8XyX/s/8Q9DwjSaMruBp2PPEKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwcVIAcRE4L/XeZ75u6lrPM/86aH/7HnmX8pe8DwjSXOPPOp5ZsRv9/brsa5FvAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwMVIAZiZuOul96CHvIym+Yd6HJJ3J9T43ol+PdG3iFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKLkQJfMuym8Z5nuv/c6P1xJo7zPBO6bZTnmYEU/ob3C3cuW/puHDZBouAVEADABAECAJjwFKCKigpNnz5daWlpysrK0oIFC1RfXx91zLlz51RWVqYbb7xRN9xwgxYtWqS2traYLg0ASHyeAlRTU6OysjLV1dXpvffeU1dXl+bOnauOjo7IMWvWrNE777yjt956SzU1NTp+/Ljuu+++mC8OAEhsnj6EsHPnzqivN2/erKysLO3fv1+zZs1SKBTSr371K23ZskXf/va3JUmbNm3SzTffrLq6On3rW9+K3eYAgIR2Ve8BhUIhSVJGRoYkaf/+/erq6lJxcXHkmMmTJ2vMmDGqra3t9Wd0dnYqHA5H3QAAQ1+/A9TT06PVq1dr5syZmjJliiSptbVVqampGjlyZNSx2dnZam1t7fXnVFRUKBAIRG55eXn9XQkAkED6HaCysjIdPnxYr7/++lUtUF5erlAoFLk1Nzdf1c8DACSGfv1G1FWrVmnHjh3avXu3Ro8eHbk/GAzq/PnzOn36dNSroLa2NgWDwV5/lt/vl9/v788aAIAE5ukVkHNOq1at0rZt27Rr1y7l5+dHfX/atGlKSUlRVVVV5L76+nodPXpURUVFsdkYADAkeHoFVFZWpi1btqiyslJpaWmR93UCgYCGDx+uQCCgZcuWae3atcrIyFB6eroeffRRFRUV8Qk4AEAUTwF6+eWXJUmzZ8+Oun/Tpk1aunSpJOkXv/iFkpKStGjRInV2dqqkpES//OUvY7IsAGDo8DnnnPUSXxYOhxUIBDRb85XsS7FeJ+GcXVjoeebE7f37LMoNt53yPPPoxOp+PdZAmZza4nnmk/M5nmcmpvb+qdDLmeEfVP+qJpQVzXf3a67lH1I9z3Sf+r/9eqyh5ILrUrUqFQqFlJ6e3udxXAsOAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJvr1J6Ji8Pra6s88z+ya8D/jsMm1Y5rf+xW0h6J/PX/B88x/Pen9KtX/6/e3eZ6Z9M9/9jwjcWXreOMVEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABggouRDjHH/1u+55nJd/9jHDbp3YTcv3ie2TG50vPM4sZ5nmck6Zkx73ieuTklpV+P5VV/Lva5aM+KOGzSu6//1vt5GF75oeeZCarzPNPteQIDgVdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJn3POWS/xZeFwWIFAQLM1X8m+gbnIIwAgdi64LlWrUqFQSOnp6X0exysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYMJTgCoqKjR9+nSlpaUpKytLCxYsUH19fdQxs2fPls/ni7qtWLEipksDABKfpwDV1NSorKxMdXV1eu+999TV1aW5c+eqo6Mj6rjly5erpaUlctuwYUNMlwYAJL5kLwfv3Lkz6uvNmzcrKytL+/fv16xZsyL3X3/99QoGg7HZEAAwJF3Ve0ChUEiSlJGREXX/a6+9pszMTE2ZMkXl5eU6e/Zsnz+js7NT4XA46gYAGPo8vQL6sp6eHq1evVozZ87UlClTIvc/+OCDGjt2rHJzc3Xo0CE9+eSTqq+v19tvv93rz6moqNCzzz7b3zUAAAnK55xz/RlcuXKlfve732nPnj0aPXp0n8ft2rVLc+bMUUNDg8aPH3/J9zs7O9XZ2Rn5OhwOKy8vT7M1X8m+lP6sBgAwdMF1qVqVCoVCSk9P7/O4fr0CWrVqlXbs2KHdu3dfNj6SVFhYKEl9Bsjv98vv9/dnDQBAAvMUIOecHn30UW3btk3V1dXKz8+/4szBgwclSTk5Of1aEAAwNHkKUFlZmbZs2aLKykqlpaWptbVVkhQIBDR8+HA1NjZqy5Yt+s53vqMbb7xRhw4d0po1azRr1ixNnTo1Ln8DAIDE5Ok9IJ/P1+v9mzZt0tKlS9Xc3Kzvfe97Onz4sDo6OpSXl6eFCxfqqaeeuuyvA35ZOBxWIBDgPSAASFBxeQ/oSq3Ky8tTTU2Nlx8JALhGcS04AIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJZOsFvso5J0m6oC7JGS8DAPDsgrok/fW/530ZdAFqb2+XJO3Ru8abAACuRnt7uwKBQJ/f97krJWqA9fT06Pjx40pLS5PP54v6XjgcVl5enpqbm5Wenm60oT3Ow0Wch4s4DxdxHi4aDOfBOaf29nbl5uYqKanvd3oG3SugpKQkjR49+rLHpKenX9NPsC9wHi7iPFzEebiI83CR9Xm43CufL/AhBACACQIEADCRUAHy+/1av369/H6/9SqmOA8XcR4u4jxcxHm4KJHOw6D7EAIA4NqQUK+AAABDBwECAJggQAAAEwQIAGAiYQK0ceNGfeMb39B1112nwsJCffjhh9YrDbhnnnlGPp8v6jZ58mTrteJu9+7duvfee5Wbmyufz6ft27dHfd85p3Xr1iknJ0fDhw9XcXGxjhw5YrNsHF3pPCxduvSS58e8efNslo2TiooKTZ8+XWlpacrKytKCBQtUX18fdcy5c+dUVlamG2+8UTfccIMWLVqktrY2o43j4285D7Nnz77k+bBixQqjjXuXEAF64403tHbtWq1fv14fffSRCgoKVFJSohMnTlivNuBuueUWtbS0RG579uyxXinuOjo6VFBQoI0bN/b6/Q0bNujFF1/UK6+8or1792rEiBEqKSnRuXPnBnjT+LrSeZCkefPmRT0/tm7dOoAbxl9NTY3KyspUV1en9957T11dXZo7d646Ojoix6xZs0bvvPOO3nrrLdXU1Oj48eO67777DLeOvb/lPEjS8uXLo54PGzZsMNq4Dy4BzJgxw5WVlUW+7u7udrm5ua6iosJwq4G3fv16V1BQYL2GKUlu27Ztka97enpcMBh0zz33XOS+06dPO7/f77Zu3Wqw4cD46nlwzrklS5a4+fPnm+xj5cSJE06Sq6mpcc5d/GefkpLi3nrrrcgxH3/8sZPkamtrrdaMu6+eB+ecu/vuu90PfvADu6X+BoP+FdD58+e1f/9+FRcXR+5LSkpScXGxamtrDTezceTIEeXm5mrcuHF66KGHdPToUeuVTDU1Nam1tTXq+REIBFRYWHhNPj+qq6uVlZWlSZMmaeXKlTp16pT1SnEVCoUkSRkZGZKk/fv3q6urK+r5MHnyZI0ZM2ZIPx++eh6+8NprrykzM1NTpkxReXm5zp49a7FenwbdxUi/6uTJk+ru7lZ2dnbU/dnZ2frkk0+MtrJRWFiozZs3a9KkSWppadGzzz6ru+66S4cPH1ZaWpr1eiZaW1slqdfnxxffu1bMmzdP9913n/Lz89XY2Kgf/ehHKi0tVW1trYYNG2a9Xsz19PRo9erVmjlzpqZMmSLp4vMhNTVVI0eOjDp2KD8fejsPkvTggw9q7Nixys3N1aFDh/Tkk0+qvr5eb7/9tuG20QZ9gPBXpaWlkb+eOnWqCgsLNXbsWL355ptatmyZ4WYYDO6///7IX996662aOnWqxo8fr+rqas2ZM8dws/goKyvT4cOHr4n3QS+nr/PwyCOPRP761ltvVU5OjubMmaPGxkaNHz9+oNfs1aD/JbjMzEwNGzbskk+xtLW1KRgMGm01OIwcOVI33XSTGhoarFcx88VzgOfHpcaNG6fMzMwh+fxYtWqVduzYoQ8++CDqj28JBoM6f/68Tp8+HXX8UH0+9HUeelNYWChJg+r5MOgDlJqaqmnTpqmqqipyX09Pj6qqqlRUVGS4mb0zZ86osbFROTk51quYyc/PVzAYjHp+hMNh7d2795p/fhw7dkynTp0aUs8P55xWrVqlbdu2adeuXcrPz4/6/rRp05SSkhL1fKivr9fRo0eH1PPhSuehNwcPHpSkwfV8sP4UxN/i9ddfd36/323evNn96U9/co888ogbOXKka21ttV5tQP3whz901dXVrqmpyf3hD39wxcXFLjMz0504ccJ6tbhqb293Bw4ccAcOHHCS3PPPP+8OHDjgPvvsM+eccz/72c/cyJEjXWVlpTt06JCbP3++y8/Pd59//rnx5rF1ufPQ3t7uHnvsMVdbW+uamprc+++/726//XY3ceJEd+7cOevVY2blypUuEAi46upq19LSErmdPXs2csyKFSvcmDFj3K5du9y+fftcUVGRKyoqMtw69q50HhoaGtyPf/xjt2/fPtfU1OQqKyvduHHj3KxZs4w3j5YQAXLOuZdeesmNGTPGpaamuhkzZri6ujrrlQbc4sWLXU5OjktNTXVf//rX3eLFi11DQ4P1WnH3wQcfOEmX3JYsWeKcu/hR7KefftplZ2c7v9/v5syZ4+rr622XjoPLnYezZ8+6uXPnulGjRrmUlBQ3duxYt3z58iH3P2m9/f1Lcps2bYoc8/nnn7vvf//77mtf+5q7/vrr3cKFC11LS4vd0nFwpfNw9OhRN2vWLJeRkeH8fr+bMGGCe/zxx10oFLJd/Cv44xgAACYG/XtAAIChiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAw8f8AADsJyH8qeG8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainLoader)\n",
    "# images, labels = next(dataiter)\n",
    "images, label = next(dataiter)\n",
    "\n",
    "# show images\n",
    "plt.imshow(images[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convStack = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 1, 3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "            torch.nn.Conv2d(1, 1, 3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(25, 25),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(25, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.convStack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAI(dataLoader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    trainLoss=0\n",
    "    for  X, y in dataLoader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        trainLoss +=loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    return trainLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valAI(dataLoader, model, loss_fn,):\n",
    "    model.eval()\n",
    "    valLoss=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataLoader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            valLoss+=loss.item()\n",
    "    \n",
    "    return valLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt: -1 - val loss: 20.8473961353302 - train loss: 76.37488412857056\n",
      "cnt: 0 - val loss: 20.851682901382446 - train loss: 76.3619294166565\n",
      "cnt: 0 - val loss: 20.82669496536255 - train loss: 76.34645676612854\n",
      "cnt: 0 - val loss: 20.83600378036499 - train loss: 76.33375549316406\n",
      "cnt: 1 - val loss: 20.819438457489014 - train loss: 76.31933259963989\n",
      "cnt: 0 - val loss: 20.830579042434692 - train loss: 76.3043007850647\n",
      "cnt: 1 - val loss: 20.81882333755493 - train loss: 76.29307556152344\n",
      "cnt: 0 - val loss: 20.80320143699646 - train loss: 76.28252267837524\n",
      "cnt: 0 - val loss: 20.813332557678223 - train loss: 76.26720547676086\n",
      "cnt: 1 - val loss: 20.796043634414673 - train loss: 76.25598478317261\n",
      "cnt: 0 - val loss: 20.81167697906494 - train loss: 76.24727845191956\n",
      "cnt: 1 - val loss: 20.809003591537476 - train loss: 76.23648142814636\n",
      "cnt: 2 - val loss: 20.79623794555664 - train loss: 76.22483420372009\n",
      "cnt: 3 - val loss: 20.805533170700073 - train loss: 76.21270060539246\n",
      "cnt: 4 - val loss: 20.797950506210327 - train loss: 76.20627570152283\n",
      "cnt: 5 - val loss: 20.80362844467163 - train loss: 76.1959764957428\n",
      "cnt: 6 - val loss: 20.791958570480347 - train loss: 76.1860785484314\n",
      "cnt: 0 - val loss: 20.785303592681885 - train loss: 76.17778038978577\n",
      "cnt: 0 - val loss: 20.78517723083496 - train loss: 76.16826581954956\n",
      "cnt: 0 - val loss: 20.78644061088562 - train loss: 76.16000294685364\n",
      "cnt: 1 - val loss: 20.785743474960327 - train loss: 76.14996337890625\n",
      "cnt: 2 - val loss: 20.78002405166626 - train loss: 76.14195203781128\n",
      "cnt: 0 - val loss: 20.77565622329712 - train loss: 76.13389849662781\n",
      "cnt: 0 - val loss: 20.78202509880066 - train loss: 76.12602806091309\n",
      "cnt: 1 - val loss: 20.775830507278442 - train loss: 76.11737537384033\n",
      "cnt: 2 - val loss: 20.767727375030518 - train loss: 76.10896635055542\n",
      "cnt: 0 - val loss: 20.780914545059204 - train loss: 76.1004569530487\n",
      "cnt: 1 - val loss: 20.761866807937622 - train loss: 76.0936930179596\n",
      "cnt: 0 - val loss: 20.76273226737976 - train loss: 76.08672761917114\n",
      "cnt: 1 - val loss: 20.759838104248047 - train loss: 76.07747650146484\n",
      "cnt: 0 - val loss: 20.772623777389526 - train loss: 76.07080864906311\n",
      "cnt: 1 - val loss: 20.750382900238037 - train loss: 76.06401777267456\n",
      "cnt: 0 - val loss: 20.75677466392517 - train loss: 76.05659866333008\n",
      "cnt: 1 - val loss: 20.7468364238739 - train loss: 76.04851365089417\n",
      "cnt: 0 - val loss: 20.74551033973694 - train loss: 76.04019021987915\n",
      "cnt: 0 - val loss: 20.751805305480957 - train loss: 76.03385591506958\n",
      "cnt: 1 - val loss: 20.751403331756592 - train loss: 76.02672910690308\n",
      "cnt: 2 - val loss: 20.754268169403076 - train loss: 76.01772832870483\n",
      "cnt: 3 - val loss: 20.740381002426147 - train loss: 76.00903844833374\n",
      "cnt: 0 - val loss: 20.746055364608765 - train loss: 76.00176644325256\n",
      "cnt: 1 - val loss: 20.733858108520508 - train loss: 75.9936261177063\n",
      "cnt: 0 - val loss: 20.734744787216187 - train loss: 75.98580026626587\n",
      "cnt: 1 - val loss: 20.729669094085693 - train loss: 75.97816109657288\n",
      "cnt: 0 - val loss: 20.725936889648438 - train loss: 75.9693546295166\n",
      "cnt: 0 - val loss: 20.72966980934143 - train loss: 75.96140575408936\n",
      "cnt: 1 - val loss: 20.728981733322144 - train loss: 75.95269203186035\n",
      "cnt: 2 - val loss: 20.715559720993042 - train loss: 75.94556403160095\n",
      "cnt: 0 - val loss: 20.725104331970215 - train loss: 75.93922066688538\n",
      "cnt: 1 - val loss: 20.722504377365112 - train loss: 75.93104910850525\n",
      "cnt: 2 - val loss: 20.71850347518921 - train loss: 75.92435050010681\n",
      "cnt: 3 - val loss: 20.718592405319214 - train loss: 75.91715407371521\n",
      "cnt: 4 - val loss: 20.717347621917725 - train loss: 75.91152310371399\n",
      "cnt: 5 - val loss: 20.714189052581787 - train loss: 75.90443515777588\n",
      "cnt: 0 - val loss: 20.714380264282227 - train loss: 75.89813566207886\n",
      "cnt: 1 - val loss: 20.709693670272827 - train loss: 75.89310264587402\n",
      "cnt: 0 - val loss: 20.702269554138184 - train loss: 75.8859760761261\n",
      "cnt: 0 - val loss: 20.697829246520996 - train loss: 75.88017892837524\n",
      "cnt: 0 - val loss: 20.704095125198364 - train loss: 75.87501835823059\n",
      "cnt: 1 - val loss: 20.697080373764038 - train loss: 75.86869359016418\n",
      "cnt: 0 - val loss: 20.698798656463623 - train loss: 75.8624815940857\n",
      "cnt: 1 - val loss: 20.69861602783203 - train loss: 75.8580322265625\n",
      "cnt: 2 - val loss: 20.69567060470581 - train loss: 75.85172438621521\n",
      "cnt: 0 - val loss: 20.695849895477295 - train loss: 75.84684038162231\n",
      "cnt: 1 - val loss: 20.69027543067932 - train loss: 75.83930897712708\n",
      "cnt: 0 - val loss: 20.694323778152466 - train loss: 75.83454537391663\n",
      "cnt: 1 - val loss: 20.69120192527771 - train loss: 75.82928681373596\n",
      "cnt: 2 - val loss: 20.680341958999634 - train loss: 75.82398104667664\n",
      "cnt: 0 - val loss: 20.688425540924072 - train loss: 75.81836271286011\n",
      "cnt: 1 - val loss: 20.683464527130127 - train loss: 75.81227016448975\n",
      "cnt: 2 - val loss: 20.686089754104614 - train loss: 75.80645775794983\n",
      "cnt: 3 - val loss: 20.68770456314087 - train loss: 75.80121874809265\n",
      "cnt: 4 - val loss: 20.67933440208435 - train loss: 75.79465699195862\n",
      "cnt: 0 - val loss: 20.68114924430847 - train loss: 75.78902387619019\n",
      "cnt: 1 - val loss: 20.678004503250122 - train loss: 75.7830765247345\n",
      "cnt: 0 - val loss: 20.674330234527588 - train loss: 75.77764987945557\n",
      "cnt: 0 - val loss: 20.675865650177002 - train loss: 75.77178692817688\n",
      "cnt: 1 - val loss: 20.66843318939209 - train loss: 75.76538968086243\n",
      "cnt: 0 - val loss: 20.668936729431152 - train loss: 75.76045799255371\n",
      "cnt: 1 - val loss: 20.67291283607483 - train loss: 75.7544436454773\n",
      "cnt: 2 - val loss: 20.667206048965454 - train loss: 75.748779296875\n",
      "cnt: 0 - val loss: 20.66005802154541 - train loss: 75.74137902259827\n",
      "cnt: 0 - val loss: 20.667256116867065 - train loss: 75.7359528541565\n",
      "cnt: 1 - val loss: 20.664119005203247 - train loss: 75.72974634170532\n",
      "cnt: 2 - val loss: 20.659502506256104 - train loss: 75.72377943992615\n",
      "cnt: 0 - val loss: 20.655130863189697 - train loss: 75.71680474281311\n",
      "cnt: 0 - val loss: 20.65717053413391 - train loss: 75.71011114120483\n",
      "cnt: 1 - val loss: 20.654149055480957 - train loss: 75.70464158058167\n",
      "cnt: 0 - val loss: 20.65417194366455 - train loss: 75.69770765304565\n",
      "cnt: 1 - val loss: 20.647849559783936 - train loss: 75.69104719161987\n",
      "cnt: 0 - val loss: 20.646454572677612 - train loss: 75.68455004692078\n",
      "cnt: 0 - val loss: 20.648303031921387 - train loss: 75.67746829986572\n",
      "cnt: 1 - val loss: 20.64627957344055 - train loss: 75.67090678215027\n",
      "cnt: 0 - val loss: 20.643367528915405 - train loss: 75.66297197341919\n",
      "cnt: 0 - val loss: 20.641079425811768 - train loss: 75.65653443336487\n",
      "cnt: 0 - val loss: 20.639842748641968 - train loss: 75.64945840835571\n",
      "cnt: 0 - val loss: 20.637532472610474 - train loss: 75.64142274856567\n",
      "cnt: 0 - val loss: 20.63239073753357 - train loss: 75.63434505462646\n",
      "cnt: 0 - val loss: 20.638336181640625 - train loss: 75.62715792655945\n",
      "cnt: 1 - val loss: 20.626851797103882 - train loss: 75.61846685409546\n",
      "cnt: 0 - val loss: 20.628186464309692 - train loss: 75.61185812950134\n",
      "cnt: 1 - val loss: 20.631774425506592 - train loss: 75.60358786582947\n",
      "cnt: 2 - val loss: 20.623449325561523 - train loss: 75.59517788887024\n",
      "cnt: 0 - val loss: 20.621633291244507 - train loss: 75.58655166625977\n",
      "cnt: 0 - val loss: 20.619552612304688 - train loss: 75.5783839225769\n",
      "cnt: 0 - val loss: 20.613890171051025 - train loss: 75.57072281837463\n",
      "cnt: 0 - val loss: 20.616358041763306 - train loss: 75.56195497512817\n",
      "cnt: 1 - val loss: 20.6107075214386 - train loss: 75.55334806442261\n",
      "cnt: 0 - val loss: 20.60598134994507 - train loss: 75.54459166526794\n",
      "cnt: 0 - val loss: 20.608652114868164 - train loss: 75.53444743156433\n",
      "cnt: 1 - val loss: 20.601125717163086 - train loss: 75.525874376297\n",
      "cnt: 0 - val loss: 20.603626012802124 - train loss: 75.51531624794006\n",
      "cnt: 1 - val loss: 20.59667992591858 - train loss: 75.50530338287354\n",
      "cnt: 0 - val loss: 20.594217777252197 - train loss: 75.49591016769409\n",
      "cnt: 0 - val loss: 20.593934059143066 - train loss: 75.48604989051819\n",
      "cnt: 0 - val loss: 20.58959197998047 - train loss: 75.47583198547363\n",
      "cnt: 0 - val loss: 20.590818405151367 - train loss: 75.46577429771423\n",
      "cnt: 1 - val loss: 20.582403421401978 - train loss: 75.45445489883423\n",
      "cnt: 0 - val loss: 20.581038236618042 - train loss: 75.44413566589355\n",
      "cnt: 0 - val loss: 20.575583696365356 - train loss: 75.4337055683136\n",
      "cnt: 0 - val loss: 20.57371187210083 - train loss: 75.4215714931488\n",
      "cnt: 0 - val loss: 20.573423385620117 - train loss: 75.40993309020996\n",
      "cnt: 0 - val loss: 20.56922698020935 - train loss: 75.39808130264282\n",
      "cnt: 0 - val loss: 20.5633487701416 - train loss: 75.38633370399475\n",
      "cnt: 0 - val loss: 20.561935901641846 - train loss: 75.3740644454956\n",
      "cnt: 0 - val loss: 20.55780792236328 - train loss: 75.3626081943512\n",
      "cnt: 0 - val loss: 20.555081367492676 - train loss: 75.34951829910278\n",
      "cnt: 0 - val loss: 20.553658485412598 - train loss: 75.33566379547119\n",
      "cnt: 0 - val loss: 20.548276901245117 - train loss: 75.32279706001282\n",
      "cnt: 0 - val loss: 20.550198078155518 - train loss: 75.30966281890869\n",
      "cnt: 1 - val loss: 20.539108514785767 - train loss: 75.29419565200806\n",
      "cnt: 0 - val loss: 20.53073787689209 - train loss: 75.27943730354309\n",
      "cnt: 0 - val loss: 20.525919198989868 - train loss: 75.26609897613525\n",
      "cnt: 0 - val loss: 20.53481411933899 - train loss: 75.2503514289856\n",
      "cnt: 1 - val loss: 20.527573347091675 - train loss: 75.23556303977966\n",
      "cnt: 2 - val loss: 20.514430046081543 - train loss: 75.2187910079956\n",
      "cnt: 0 - val loss: 20.51534390449524 - train loss: 75.2025237083435\n",
      "cnt: 1 - val loss: 20.51295280456543 - train loss: 75.18598055839539\n",
      "cnt: 0 - val loss: 20.504104614257812 - train loss: 75.17025661468506\n",
      "cnt: 0 - val loss: 20.501821994781494 - train loss: 75.15180110931396\n",
      "cnt: 0 - val loss: 20.496050119400024 - train loss: 75.13468742370605\n",
      "cnt: 0 - val loss: 20.492269277572632 - train loss: 75.11699986457825\n",
      "cnt: 0 - val loss: 20.488735675811768 - train loss: 75.09813594818115\n",
      "cnt: 0 - val loss: 20.47924780845642 - train loss: 75.07928204536438\n",
      "cnt: 0 - val loss: 20.478647232055664 - train loss: 75.05877590179443\n",
      "cnt: 0 - val loss: 20.465725898742676 - train loss: 75.03839135169983\n",
      "cnt: 0 - val loss: 20.46357035636902 - train loss: 75.01838898658752\n",
      "cnt: 0 - val loss: 20.464521646499634 - train loss: 74.99800181388855\n",
      "cnt: 1 - val loss: 20.450323581695557 - train loss: 74.97544932365417\n",
      "cnt: 0 - val loss: 20.441696643829346 - train loss: 74.95367002487183\n",
      "cnt: 0 - val loss: 20.446040153503418 - train loss: 74.93062376976013\n",
      "cnt: 1 - val loss: 20.43065857887268 - train loss: 74.90751004219055\n",
      "cnt: 0 - val loss: 20.422703981399536 - train loss: 74.88351106643677\n",
      "cnt: 0 - val loss: 20.4159414768219 - train loss: 74.85847854614258\n",
      "cnt: 0 - val loss: 20.410377740859985 - train loss: 74.83349275588989\n",
      "cnt: 0 - val loss: 20.409621715545654 - train loss: 74.80680322647095\n",
      "cnt: 0 - val loss: 20.409725666046143 - train loss: 74.7808768749237\n",
      "cnt: 1 - val loss: 20.38923954963684 - train loss: 74.75345873832703\n",
      "cnt: 0 - val loss: 20.390533924102783 - train loss: 74.72569274902344\n",
      "cnt: 1 - val loss: 20.37180733680725 - train loss: 74.69655179977417\n",
      "cnt: 0 - val loss: 20.362908124923706 - train loss: 74.66556358337402\n",
      "cnt: 0 - val loss: 20.35393190383911 - train loss: 74.63524961471558\n",
      "cnt: 0 - val loss: 20.35183095932007 - train loss: 74.60283327102661\n",
      "cnt: 0 - val loss: 20.34211754798889 - train loss: 74.57064461708069\n",
      "cnt: 0 - val loss: 20.326343774795532 - train loss: 74.53765916824341\n",
      "cnt: 0 - val loss: 20.317712545394897 - train loss: 74.50256681442261\n",
      "cnt: 0 - val loss: 20.311241149902344 - train loss: 74.46719217300415\n",
      "cnt: 0 - val loss: 20.30920100212097 - train loss: 74.43164300918579\n",
      "cnt: 0 - val loss: 20.291145086288452 - train loss: 74.39297342300415\n",
      "cnt: 0 - val loss: 20.280754804611206 - train loss: 74.3545343875885\n",
      "cnt: 0 - val loss: 20.27071261405945 - train loss: 74.31449031829834\n",
      "cnt: 0 - val loss: 20.255638360977173 - train loss: 74.273672580719\n",
      "cnt: 0 - val loss: 20.243799686431885 - train loss: 74.23120617866516\n",
      "cnt: 0 - val loss: 20.242279291152954 - train loss: 74.1866466999054\n",
      "cnt: 0 - val loss: 20.220316410064697 - train loss: 74.14127278327942\n",
      "cnt: 0 - val loss: 20.19960904121399 - train loss: 74.09501504898071\n",
      "cnt: 0 - val loss: 20.196805000305176 - train loss: 74.046706199646\n",
      "cnt: 0 - val loss: 20.179099798202515 - train loss: 73.99551844596863\n",
      "cnt: 0 - val loss: 20.17320966720581 - train loss: 73.94620752334595\n",
      "cnt: 0 - val loss: 20.15977144241333 - train loss: 73.89212942123413\n",
      "cnt: 0 - val loss: 20.13548755645752 - train loss: 73.83708024024963\n",
      "cnt: 0 - val loss: 20.12135338783264 - train loss: 73.78034090995789\n",
      "cnt: 0 - val loss: 20.097716569900513 - train loss: 73.72400259971619\n",
      "cnt: 0 - val loss: 20.09792733192444 - train loss: 73.66203022003174\n",
      "cnt: 1 - val loss: 20.086888074874878 - train loss: 73.60096955299377\n",
      "cnt: 0 - val loss: 20.063186645507812 - train loss: 73.53598070144653\n",
      "cnt: 0 - val loss: 20.024755001068115 - train loss: 73.46942353248596\n",
      "cnt: 0 - val loss: 20.01956582069397 - train loss: 73.40012264251709\n",
      "cnt: 0 - val loss: 19.99178194999695 - train loss: 73.32760262489319\n",
      "cnt: 0 - val loss: 19.971503734588623 - train loss: 73.25299859046936\n",
      "cnt: 0 - val loss: 19.948553323745728 - train loss: 73.17478847503662\n",
      "cnt: 0 - val loss: 19.945380687713623 - train loss: 73.09715580940247\n",
      "cnt: 0 - val loss: 19.93534517288208 - train loss: 73.01412105560303\n",
      "cnt: 0 - val loss: 19.88956594467163 - train loss: 72.9276659488678\n",
      "cnt: 0 - val loss: 19.85590958595276 - train loss: 72.83855700492859\n",
      "cnt: 0 - val loss: 19.828091621398926 - train loss: 72.74685382843018\n",
      "cnt: 0 - val loss: 19.81896686553955 - train loss: 72.65133166313171\n",
      "cnt: 0 - val loss: 19.797065496444702 - train loss: 72.55346417427063\n",
      "cnt: 0 - val loss: 19.744771003723145 - train loss: 72.4505820274353\n",
      "cnt: 0 - val loss: 19.736571073532104 - train loss: 72.34073400497437\n",
      "cnt: 0 - val loss: 19.68839120864868 - train loss: 72.23194146156311\n",
      "cnt: 0 - val loss: 19.670536756515503 - train loss: 72.11715722084045\n",
      "cnt: 0 - val loss: 19.64655566215515 - train loss: 71.99798369407654\n",
      "cnt: 0 - val loss: 19.5976779460907 - train loss: 71.87214064598083\n",
      "cnt: 0 - val loss: 19.557993412017822 - train loss: 71.74649262428284\n",
      "cnt: 0 - val loss: 19.525787353515625 - train loss: 71.61133456230164\n",
      "cnt: 0 - val loss: 19.47496199607849 - train loss: 71.46987009048462\n",
      "cnt: 0 - val loss: 19.427936792373657 - train loss: 71.3275375366211\n",
      "cnt: 0 - val loss: 19.38672637939453 - train loss: 71.17724227905273\n",
      "cnt: 0 - val loss: 19.37361454963684 - train loss: 71.01897096633911\n",
      "cnt: 0 - val loss: 19.3232638835907 - train loss: 70.85784697532654\n",
      "cnt: 0 - val loss: 19.269806385040283 - train loss: 70.69183683395386\n",
      "cnt: 0 - val loss: 19.21288537979126 - train loss: 70.51402163505554\n",
      "cnt: 0 - val loss: 19.14661931991577 - train loss: 70.33622145652771\n",
      "cnt: 0 - val loss: 19.120068550109863 - train loss: 70.14600419998169\n",
      "cnt: 0 - val loss: 19.0599844455719 - train loss: 69.94672107696533\n",
      "cnt: 0 - val loss: 19.022316455841064 - train loss: 69.74695491790771\n",
      "cnt: 0 - val loss: 18.96977138519287 - train loss: 69.5349771976471\n",
      "cnt: 0 - val loss: 18.88189148902893 - train loss: 69.31060218811035\n",
      "cnt: 0 - val loss: 18.846503019332886 - train loss: 69.08183336257935\n",
      "cnt: 0 - val loss: 18.744158029556274 - train loss: 68.84142351150513\n",
      "cnt: 0 - val loss: 18.674397945404053 - train loss: 68.59814238548279\n",
      "cnt: 0 - val loss: 18.62660241127014 - train loss: 68.34531688690186\n",
      "cnt: 0 - val loss: 18.516637325286865 - train loss: 68.07400584220886\n",
      "cnt: 0 - val loss: 18.445437908172607 - train loss: 67.8021092414856\n",
      "cnt: 0 - val loss: 18.377849578857422 - train loss: 67.51218438148499\n",
      "cnt: 0 - val loss: 18.326452255249023 - train loss: 67.21706652641296\n",
      "cnt: 0 - val loss: 18.19925332069397 - train loss: 66.91064429283142\n",
      "cnt: 0 - val loss: 18.112064719200134 - train loss: 66.59427905082703\n",
      "cnt: 0 - val loss: 18.043705344200134 - train loss: 66.26353847980499\n",
      "cnt: 0 - val loss: 17.929688811302185 - train loss: 65.9210913181305\n",
      "cnt: 0 - val loss: 17.864951848983765 - train loss: 65.5654354095459\n",
      "cnt: 0 - val loss: 17.738956689834595 - train loss: 65.20357358455658\n",
      "cnt: 0 - val loss: 17.607229232788086 - train loss: 64.8251541852951\n",
      "cnt: 0 - val loss: 17.536198496818542 - train loss: 64.43255758285522\n",
      "cnt: 0 - val loss: 17.44007647037506 - train loss: 64.02714705467224\n",
      "cnt: 0 - val loss: 17.32834041118622 - train loss: 63.609805941581726\n",
      "cnt: 0 - val loss: 17.161681056022644 - train loss: 63.17731213569641\n",
      "cnt: 0 - val loss: 17.070974230766296 - train loss: 62.724279284477234\n",
      "cnt: 0 - val loss: 16.984196543693542 - train loss: 62.25484919548035\n",
      "cnt: 0 - val loss: 16.80384349822998 - train loss: 61.77530062198639\n",
      "cnt: 0 - val loss: 16.619579672813416 - train loss: 61.28531289100647\n",
      "cnt: 0 - val loss: 16.526894211769104 - train loss: 60.76951324939728\n",
      "cnt: 0 - val loss: 16.338077068328857 - train loss: 60.255154728889465\n",
      "cnt: 0 - val loss: 16.23379623889923 - train loss: 59.72732603549957\n",
      "cnt: 0 - val loss: 16.116810083389282 - train loss: 59.18477547168732\n",
      "cnt: 0 - val loss: 15.928921222686768 - train loss: 58.638866782188416\n",
      "cnt: 0 - val loss: 15.785202503204346 - train loss: 58.08914291858673\n",
      "cnt: 0 - val loss: 15.581806898117065 - train loss: 57.52676546573639\n",
      "cnt: 0 - val loss: 15.465887904167175 - train loss: 56.954676270484924\n",
      "cnt: 0 - val loss: 15.345210433006287 - train loss: 56.370946526527405\n",
      "cnt: 0 - val loss: 15.14286994934082 - train loss: 55.77810287475586\n",
      "cnt: 0 - val loss: 14.987736463546753 - train loss: 55.171228528022766\n",
      "cnt: 0 - val loss: 14.74311637878418 - train loss: 54.570762515068054\n",
      "cnt: 0 - val loss: 14.652604341506958 - train loss: 53.95028305053711\n",
      "cnt: 0 - val loss: 14.459180355072021 - train loss: 53.3253413438797\n",
      "cnt: 0 - val loss: 14.28344476222992 - train loss: 52.69476282596588\n",
      "cnt: 0 - val loss: 14.118541955947876 - train loss: 52.06230366230011\n",
      "cnt: 0 - val loss: 13.863479256629944 - train loss: 51.42703866958618\n",
      "cnt: 0 - val loss: 13.676558017730713 - train loss: 50.7863233089447\n",
      "cnt: 0 - val loss: 13.587636947631836 - train loss: 50.13334035873413\n",
      "cnt: 0 - val loss: 13.373727917671204 - train loss: 49.48690438270569\n",
      "cnt: 0 - val loss: 13.243229031562805 - train loss: 48.83485448360443\n",
      "cnt: 0 - val loss: 13.020695805549622 - train loss: 48.17459225654602\n",
      "cnt: 0 - val loss: 12.903268694877625 - train loss: 47.523056745529175\n",
      "cnt: 0 - val loss: 12.689146876335144 - train loss: 46.862250089645386\n",
      "cnt: 0 - val loss: 12.545138239860535 - train loss: 46.206567883491516\n",
      "cnt: 0 - val loss: 12.322972297668457 - train loss: 45.5636922121048\n",
      "cnt: 0 - val loss: 12.207351207733154 - train loss: 44.90661919116974\n",
      "cnt: 0 - val loss: 11.985853433609009 - train loss: 44.27013957500458\n",
      "cnt: 0 - val loss: 11.841650366783142 - train loss: 43.62598705291748\n",
      "cnt: 0 - val loss: 11.561874032020569 - train loss: 42.98772060871124\n",
      "cnt: 0 - val loss: 11.418300986289978 - train loss: 42.351601123809814\n",
      "cnt: 0 - val loss: 11.312812805175781 - train loss: 41.72825062274933\n",
      "cnt: 0 - val loss: 11.135839700698853 - train loss: 41.10498559474945\n",
      "cnt: 0 - val loss: 10.919647812843323 - train loss: 40.50143218040466\n",
      "cnt: 0 - val loss: 10.737573504447937 - train loss: 39.898369908332825\n",
      "cnt: 0 - val loss: 10.645367622375488 - train loss: 39.291571736335754\n",
      "cnt: 0 - val loss: 10.437256336212158 - train loss: 38.71317756175995\n",
      "cnt: 0 - val loss: 10.309873342514038 - train loss: 38.13960516452789\n",
      "cnt: 0 - val loss: 10.105905294418335 - train loss: 37.573235750198364\n",
      "cnt: 0 - val loss: 10.053491234779358 - train loss: 37.01641142368317\n",
      "cnt: 0 - val loss: 9.868374466896057 - train loss: 36.4694002866745\n",
      "cnt: 0 - val loss: 9.64870548248291 - train loss: 35.92714715003967\n",
      "cnt: 0 - val loss: 9.600368738174438 - train loss: 35.40646028518677\n",
      "cnt: 0 - val loss: 9.443082809448242 - train loss: 34.905725836753845\n",
      "cnt: 0 - val loss: 9.321317434310913 - train loss: 34.393873512744904\n",
      "cnt: 0 - val loss: 9.214458584785461 - train loss: 33.91067558526993\n",
      "cnt: 0 - val loss: 9.066960096359253 - train loss: 33.42335569858551\n",
      "cnt: 0 - val loss: 8.942181766033173 - train loss: 32.972774267196655\n",
      "cnt: 0 - val loss: 8.847822546958923 - train loss: 32.51423239707947\n",
      "cnt: 0 - val loss: 8.69460016489029 - train loss: 32.068914353847504\n",
      "cnt: 0 - val loss: 8.461416721343994 - train loss: 31.648853182792664\n",
      "cnt: 0 - val loss: 8.476838111877441 - train loss: 31.23241776227951\n",
      "cnt: 1 - val loss: 8.377125263214111 - train loss: 30.82861566543579\n",
      "cnt: 0 - val loss: 8.195745587348938 - train loss: 30.433375537395477\n",
      "cnt: 0 - val loss: 8.194068014621735 - train loss: 30.053578913211823\n",
      "cnt: 0 - val loss: 7.990126311779022 - train loss: 29.67220175266266\n",
      "cnt: 0 - val loss: 7.923542261123657 - train loss: 29.32571691274643\n",
      "cnt: 0 - val loss: 7.830437242984772 - train loss: 28.968723475933075\n",
      "cnt: 0 - val loss: 7.757401525974274 - train loss: 28.635205686092377\n",
      "cnt: 0 - val loss: 7.725922167301178 - train loss: 28.30758637189865\n",
      "cnt: 0 - val loss: 7.567596793174744 - train loss: 27.986397981643677\n",
      "cnt: 0 - val loss: 7.599188923835754 - train loss: 27.68075406551361\n",
      "cnt: 1 - val loss: 7.435091137886047 - train loss: 27.38614785671234\n",
      "cnt: 0 - val loss: 7.348444640636444 - train loss: 27.108424961566925\n",
      "cnt: 0 - val loss: 7.196844696998596 - train loss: 26.8203467130661\n",
      "cnt: 0 - val loss: 7.24192750453949 - train loss: 26.560681223869324\n",
      "cnt: 1 - val loss: 7.1779820919036865 - train loss: 26.29013282060623\n",
      "cnt: 0 - val loss: 7.115613758563995 - train loss: 26.048611879348755\n",
      "cnt: 0 - val loss: 7.050230264663696 - train loss: 25.806842148303986\n",
      "cnt: 0 - val loss: 6.847989797592163 - train loss: 25.574175715446472\n",
      "cnt: 0 - val loss: 6.931397497653961 - train loss: 25.338995039463043\n",
      "cnt: 1 - val loss: 6.862158179283142 - train loss: 25.124238908290863\n",
      "cnt: 2 - val loss: 6.7728477120399475 - train loss: 24.913348615169525\n",
      "cnt: 0 - val loss: 6.7587921023368835 - train loss: 24.709738731384277\n",
      "cnt: 0 - val loss: 6.725787937641144 - train loss: 24.495399951934814\n",
      "cnt: 0 - val loss: 6.528051972389221 - train loss: 24.291468858718872\n",
      "cnt: 0 - val loss: 6.586637318134308 - train loss: 24.11673992872238\n",
      "cnt: 1 - val loss: 6.589293539524078 - train loss: 23.91368931531906\n",
      "cnt: 2 - val loss: 6.582667529582977 - train loss: 23.729847848415375\n",
      "cnt: 3 - val loss: 6.415054202079773 - train loss: 23.561094880104065\n",
      "cnt: 0 - val loss: 6.410666823387146 - train loss: 23.39376735687256\n",
      "cnt: 0 - val loss: 6.311847686767578 - train loss: 23.230215311050415\n",
      "cnt: 0 - val loss: 6.2808937430381775 - train loss: 23.07904052734375\n",
      "cnt: 0 - val loss: 6.323311030864716 - train loss: 22.917098462581635\n",
      "cnt: 1 - val loss: 6.19401079416275 - train loss: 22.774231493473053\n",
      "cnt: 0 - val loss: 6.131478786468506 - train loss: 22.632303953170776\n",
      "cnt: 0 - val loss: 6.279129266738892 - train loss: 22.489163398742676\n",
      "cnt: 1 - val loss: 6.105743765830994 - train loss: 22.34590780735016\n",
      "cnt: 0 - val loss: 6.108873724937439 - train loss: 22.202142894268036\n",
      "cnt: 1 - val loss: 6.047816336154938 - train loss: 22.082005381584167\n",
      "cnt: 0 - val loss: 5.947064578533173 - train loss: 21.94782429933548\n",
      "cnt: 0 - val loss: 6.008177757263184 - train loss: 21.82095843553543\n",
      "cnt: 1 - val loss: 5.952214479446411 - train loss: 21.707932233810425\n",
      "cnt: 2 - val loss: 5.964834690093994 - train loss: 21.59118139743805\n",
      "cnt: 3 - val loss: 5.924433648586273 - train loss: 21.463494539260864\n",
      "cnt: 0 - val loss: 5.816307902336121 - train loss: 21.353387534618378\n",
      "cnt: 0 - val loss: 5.782107949256897 - train loss: 21.24143749475479\n",
      "cnt: 0 - val loss: 5.7351120710372925 - train loss: 21.1325181722641\n",
      "cnt: 0 - val loss: 5.736486732959747 - train loss: 21.035628736019135\n",
      "cnt: 1 - val loss: 5.702842712402344 - train loss: 20.937851667404175\n",
      "cnt: 0 - val loss: 5.671791076660156 - train loss: 20.8283714056015\n",
      "cnt: 0 - val loss: 5.7682342529296875 - train loss: 20.735782980918884\n",
      "cnt: 1 - val loss: 5.746162295341492 - train loss: 20.629030644893646\n",
      "cnt: 2 - val loss: 5.60638964176178 - train loss: 20.546606719493866\n",
      "cnt: 0 - val loss: 5.590474426746368 - train loss: 20.45003777742386\n",
      "cnt: 0 - val loss: 5.630428075790405 - train loss: 20.35788518190384\n",
      "cnt: 1 - val loss: 5.54256272315979 - train loss: 20.270477950572968\n",
      "cnt: 0 - val loss: 5.575060307979584 - train loss: 20.175440073013306\n",
      "cnt: 1 - val loss: 5.431221783161163 - train loss: 20.107080698013306\n",
      "cnt: 0 - val loss: 5.498281300067902 - train loss: 20.02425968647003\n",
      "cnt: 1 - val loss: 5.449002504348755 - train loss: 19.955153107643127\n",
      "cnt: 2 - val loss: 5.454246878623962 - train loss: 19.855371057987213\n",
      "cnt: 3 - val loss: 5.460138201713562 - train loss: 19.764988839626312\n",
      "cnt: 4 - val loss: 5.504537343978882 - train loss: 19.691472113132477\n",
      "cnt: 5 - val loss: 5.446556031703949 - train loss: 19.62979280948639\n",
      "cnt: 6 - val loss: 5.378330588340759 - train loss: 19.549595713615417\n",
      "cnt: 0 - val loss: 5.314678370952606 - train loss: 19.49015486240387\n",
      "cnt: 0 - val loss: 5.318220555782318 - train loss: 19.408263444900513\n",
      "cnt: 1 - val loss: 5.302740037441254 - train loss: 19.345637917518616\n",
      "cnt: 0 - val loss: 5.282482624053955 - train loss: 19.2716366648674\n",
      "cnt: 0 - val loss: 5.344787776470184 - train loss: 19.210617303848267\n",
      "cnt: 1 - val loss: 5.341104865074158 - train loss: 19.157776355743408\n",
      "cnt: 2 - val loss: 5.187495678663254 - train loss: 19.072352290153503\n",
      "cnt: 0 - val loss: 5.297045290470123 - train loss: 19.00695300102234\n",
      "cnt: 1 - val loss: 5.2280455231666565 - train loss: 18.953285932540894\n",
      "cnt: 2 - val loss: 5.126270651817322 - train loss: 18.90600085258484\n",
      "cnt: 0 - val loss: 5.205390751361847 - train loss: 18.8319131731987\n",
      "cnt: 1 - val loss: 5.137561023235321 - train loss: 18.775193572044373\n",
      "cnt: 2 - val loss: 5.177705109119415 - train loss: 18.71007549762726\n",
      "cnt: 3 - val loss: 5.179200768470764 - train loss: 18.662789046764374\n",
      "cnt: 4 - val loss: 5.168283462524414 - train loss: 18.607621610164642\n",
      "cnt: 5 - val loss: 5.177929043769836 - train loss: 18.550403654575348\n",
      "cnt: 6 - val loss: 5.098615288734436 - train loss: 18.506186932325363\n",
      "cnt: 0 - val loss: 5.1362298130989075 - train loss: 18.432940304279327\n",
      "cnt: 1 - val loss: 5.132382750511169 - train loss: 18.393262922763824\n",
      "cnt: 2 - val loss: 5.059137046337128 - train loss: 18.339491993188858\n",
      "cnt: 0 - val loss: 5.039157032966614 - train loss: 18.293062925338745\n",
      "cnt: 0 - val loss: 5.082268297672272 - train loss: 18.24957123398781\n",
      "cnt: 1 - val loss: 4.971761703491211 - train loss: 18.195580661296844\n",
      "cnt: 0 - val loss: 5.023281753063202 - train loss: 18.13479083776474\n",
      "cnt: 1 - val loss: 5.064570069313049 - train loss: 18.112761199474335\n",
      "cnt: 2 - val loss: 5.100219905376434 - train loss: 18.04877108335495\n",
      "cnt: 3 - val loss: 4.9524489641189575 - train loss: 17.995021760463715\n",
      "cnt: 0 - val loss: 4.962027907371521 - train loss: 17.962755382061005\n",
      "cnt: 1 - val loss: 4.923863351345062 - train loss: 17.926650822162628\n",
      "cnt: 0 - val loss: 4.89988374710083 - train loss: 17.866031795740128\n",
      "cnt: 0 - val loss: 4.932781279087067 - train loss: 17.828150033950806\n",
      "cnt: 1 - val loss: 4.8838230073452 - train loss: 17.779233932495117\n",
      "cnt: 0 - val loss: 4.85923108458519 - train loss: 17.73710435628891\n",
      "cnt: 0 - val loss: 4.917950987815857 - train loss: 17.70907160639763\n",
      "cnt: 1 - val loss: 4.842451632022858 - train loss: 17.665055841207504\n",
      "cnt: 0 - val loss: 5.022554814815521 - train loss: 17.620621651411057\n",
      "cnt: 1 - val loss: 4.8584094643592834 - train loss: 17.578301161527634\n",
      "cnt: 2 - val loss: 4.886965274810791 - train loss: 17.539938062429428\n",
      "cnt: 3 - val loss: 4.872817754745483 - train loss: 17.513662844896317\n",
      "cnt: 4 - val loss: 4.805782288312912 - train loss: 17.47662404179573\n",
      "cnt: 0 - val loss: 4.7546985149383545 - train loss: 17.421062737703323\n",
      "cnt: 0 - val loss: 4.899440318346024 - train loss: 17.387525528669357\n",
      "cnt: 1 - val loss: 4.840732276439667 - train loss: 17.352341026067734\n",
      "cnt: 2 - val loss: 4.827319443225861 - train loss: 17.318365275859833\n",
      "cnt: 3 - val loss: 4.73045939207077 - train loss: 17.28251913189888\n",
      "cnt: 0 - val loss: 4.838154315948486 - train loss: 17.25743806362152\n",
      "cnt: 1 - val loss: 4.862296879291534 - train loss: 17.212197691202164\n",
      "cnt: 2 - val loss: 4.77523410320282 - train loss: 17.186156034469604\n",
      "cnt: 3 - val loss: 4.81865781545639 - train loss: 17.151648074388504\n",
      "cnt: 4 - val loss: 4.671548902988434 - train loss: 17.115143805742264\n",
      "cnt: 0 - val loss: 4.750091314315796 - train loss: 17.081421494483948\n",
      "cnt: 1 - val loss: 4.737587481737137 - train loss: 17.04983538389206\n",
      "cnt: 2 - val loss: 4.695560574531555 - train loss: 17.02815192937851\n",
      "cnt: 3 - val loss: 4.658816546201706 - train loss: 16.978644460439682\n",
      "cnt: 0 - val loss: 4.721674859523773 - train loss: 16.949352651834488\n",
      "cnt: 1 - val loss: 4.746784090995789 - train loss: 16.91821649670601\n",
      "cnt: 2 - val loss: 4.816780596971512 - train loss: 16.89225396513939\n",
      "cnt: 3 - val loss: 4.635788977146149 - train loss: 16.852701395750046\n",
      "cnt: 0 - val loss: 4.681968837976456 - train loss: 16.82832822203636\n",
      "cnt: 1 - val loss: 4.703222870826721 - train loss: 16.79921942949295\n",
      "cnt: 2 - val loss: 4.590915411710739 - train loss: 16.76607310771942\n",
      "cnt: 0 - val loss: 4.627131074666977 - train loss: 16.74350354075432\n",
      "cnt: 1 - val loss: 4.672430068254471 - train loss: 16.72282037138939\n",
      "cnt: 2 - val loss: 4.629347413778305 - train loss: 16.683256030082703\n",
      "cnt: 3 - val loss: 4.613118767738342 - train loss: 16.65516984462738\n",
      "cnt: 4 - val loss: 4.608025848865509 - train loss: 16.619153708219528\n",
      "cnt: 5 - val loss: 4.699681103229523 - train loss: 16.587095856666565\n",
      "cnt: 6 - val loss: 4.584422677755356 - train loss: 16.57321509718895\n",
      "cnt: 0 - val loss: 4.552007704973221 - train loss: 16.538206189870834\n",
      "cnt: 0 - val loss: 4.580380320549011 - train loss: 16.515329778194427\n",
      "cnt: 1 - val loss: 4.666638255119324 - train loss: 16.482518196105957\n",
      "cnt: 2 - val loss: 4.574340373277664 - train loss: 16.470584958791733\n",
      "cnt: 3 - val loss: 4.534680783748627 - train loss: 16.42787978053093\n",
      "cnt: 0 - val loss: 4.501744091510773 - train loss: 16.416457772254944\n",
      "cnt: 0 - val loss: 4.549546658992767 - train loss: 16.4004148542881\n",
      "cnt: 1 - val loss: 4.551775693893433 - train loss: 16.35993978381157\n",
      "cnt: 2 - val loss: 4.578374624252319 - train loss: 16.33008098602295\n",
      "cnt: 3 - val loss: 4.524500727653503 - train loss: 16.31954899430275\n",
      "cnt: 4 - val loss: 4.526968419551849 - train loss: 16.293325006961823\n",
      "cnt: 5 - val loss: 4.458002686500549 - train loss: 16.265356719493866\n",
      "cnt: 0 - val loss: 4.436176210641861 - train loss: 16.247769236564636\n",
      "cnt: 0 - val loss: 4.542001783847809 - train loss: 16.21657583117485\n",
      "cnt: 1 - val loss: 4.658781141042709 - train loss: 16.193676620721817\n",
      "cnt: 2 - val loss: 4.413323074579239 - train loss: 16.16057860851288\n",
      "cnt: 0 - val loss: 4.491164594888687 - train loss: 16.14136639237404\n",
      "cnt: 1 - val loss: 4.571403115987778 - train loss: 16.130897760391235\n",
      "cnt: 2 - val loss: 4.5765420794487 - train loss: 16.09786957502365\n",
      "cnt: 3 - val loss: 4.463371694087982 - train loss: 16.061321586370468\n",
      "cnt: 4 - val loss: 4.455897957086563 - train loss: 16.047321498394012\n",
      "cnt: 5 - val loss: 4.452233612537384 - train loss: 16.04154598712921\n",
      "cnt: 6 - val loss: 4.573630660772324 - train loss: 15.996110945940018\n",
      "cnt: 7 - val loss: 4.360181570053101 - train loss: 15.981490015983582\n",
      "cnt: 0 - val loss: 4.5669147074222565 - train loss: 15.976418107748032\n",
      "cnt: 1 - val loss: 4.468994051218033 - train loss: 15.934173911809921\n",
      "cnt: 2 - val loss: 4.497286915779114 - train loss: 15.923383980989456\n",
      "cnt: 3 - val loss: 4.4563087821006775 - train loss: 15.90043357014656\n",
      "cnt: 4 - val loss: 4.430014789104462 - train loss: 15.875151872634888\n",
      "cnt: 5 - val loss: 4.363869994878769 - train loss: 15.86265367269516\n",
      "cnt: 6 - val loss: 4.498028814792633 - train loss: 15.847340434789658\n",
      "cnt: 7 - val loss: 4.473191022872925 - train loss: 15.82569169998169\n",
      "cnt: 8 - val loss: 4.330012619495392 - train loss: 15.799443870782852\n",
      "cnt: 0 - val loss: 4.352255791425705 - train loss: 15.791797965765\n",
      "cnt: 1 - val loss: 4.461315542459488 - train loss: 15.7542465031147\n",
      "cnt: 2 - val loss: 4.386092364788055 - train loss: 15.731368869543076\n",
      "cnt: 3 - val loss: 4.427706927061081 - train loss: 15.716411530971527\n",
      "cnt: 4 - val loss: 4.348385900259018 - train loss: 15.694309413433075\n",
      "cnt: 5 - val loss: 4.2875242829322815 - train loss: 15.673315346240997\n",
      "cnt: 0 - val loss: 4.276652574539185 - train loss: 15.675194501876831\n",
      "cnt: 0 - val loss: 4.406885772943497 - train loss: 15.636630594730377\n",
      "cnt: 1 - val loss: 4.291421711444855 - train loss: 15.621317207813263\n",
      "cnt: 2 - val loss: 4.457263171672821 - train loss: 15.602016925811768\n",
      "cnt: 3 - val loss: 4.332092672586441 - train loss: 15.569218397140503\n",
      "cnt: 4 - val loss: 4.275820374488831 - train loss: 15.559038072824478\n",
      "cnt: 0 - val loss: 4.42390775680542 - train loss: 15.551955759525299\n",
      "cnt: 1 - val loss: 4.322707861661911 - train loss: 15.523856520652771\n",
      "cnt: 2 - val loss: 4.336601287126541 - train loss: 15.505379498004913\n",
      "cnt: 3 - val loss: 4.354391843080521 - train loss: 15.488424301147461\n",
      "cnt: 4 - val loss: 4.232645094394684 - train loss: 15.471824139356613\n",
      "cnt: 0 - val loss: 4.327796697616577 - train loss: 15.448592752218246\n",
      "cnt: 1 - val loss: 4.4203616082668304 - train loss: 15.43336072564125\n",
      "cnt: 2 - val loss: 4.255386531352997 - train loss: 15.415820389986038\n",
      "cnt: 3 - val loss: 4.305009752511978 - train loss: 15.41023001074791\n",
      "cnt: 4 - val loss: 4.250501185655594 - train loss: 15.376574456691742\n",
      "cnt: 5 - val loss: 4.27352300286293 - train loss: 15.352632731199265\n",
      "cnt: 6 - val loss: 4.281173795461655 - train loss: 15.338569909334183\n",
      "cnt: 7 - val loss: 4.28748431801796 - train loss: 15.313808143138885\n",
      "cnt: 8 - val loss: 4.2316286861896515 - train loss: 15.30778831243515\n",
      "cnt: 0 - val loss: 4.303742229938507 - train loss: 15.28580293059349\n",
      "cnt: 1 - val loss: 4.204069972038269 - train loss: 15.275585860013962\n",
      "cnt: 0 - val loss: 4.269033461809158 - train loss: 15.250310182571411\n",
      "cnt: 1 - val loss: 4.28751477599144 - train loss: 15.233470022678375\n",
      "cnt: 2 - val loss: 4.198749899864197 - train loss: 15.224890261888504\n",
      "cnt: 0 - val loss: 4.18911749124527 - train loss: 15.20131853222847\n",
      "cnt: 0 - val loss: 4.251764476299286 - train loss: 15.18142756819725\n",
      "cnt: 1 - val loss: 4.231265068054199 - train loss: 15.16102620959282\n",
      "cnt: 2 - val loss: 4.273683667182922 - train loss: 15.15017756819725\n",
      "cnt: 3 - val loss: 4.346312075853348 - train loss: 15.139999836683273\n",
      "cnt: 4 - val loss: 4.24233141541481 - train loss: 15.129431009292603\n",
      "cnt: 5 - val loss: 4.332312047481537 - train loss: 15.104966193437576\n",
      "cnt: 6 - val loss: 4.2200183272361755 - train loss: 15.088351845741272\n",
      "cnt: 7 - val loss: 4.330839455127716 - train loss: 15.06068953871727\n",
      "cnt: 8 - val loss: 4.156193643808365 - train loss: 15.050549328327179\n",
      "cnt: 0 - val loss: 4.22212752699852 - train loss: 15.043567061424255\n",
      "cnt: 1 - val loss: 4.2166324853897095 - train loss: 15.019557774066925\n",
      "cnt: 2 - val loss: 4.186915725469589 - train loss: 15.00361055135727\n",
      "cnt: 3 - val loss: 4.288808524608612 - train loss: 14.999452710151672\n",
      "cnt: 4 - val loss: 4.139325052499771 - train loss: 14.978440701961517\n",
      "cnt: 0 - val loss: 4.149978190660477 - train loss: 14.964133560657501\n",
      "cnt: 1 - val loss: 4.155122876167297 - train loss: 14.94201186299324\n",
      "cnt: 2 - val loss: 4.1298588514328 - train loss: 14.933229327201843\n",
      "cnt: 0 - val loss: 4.164476722478867 - train loss: 14.910810947418213\n",
      "cnt: 1 - val loss: 4.166263967752457 - train loss: 14.911683708429337\n",
      "cnt: 2 - val loss: 4.165370613336563 - train loss: 14.8737753033638\n",
      "cnt: 3 - val loss: 4.234767019748688 - train loss: 14.865535974502563\n",
      "cnt: 4 - val loss: 4.0937656462192535 - train loss: 14.855225175619125\n",
      "cnt: 0 - val loss: 4.132062077522278 - train loss: 14.824498176574707\n",
      "cnt: 1 - val loss: 4.106353759765625 - train loss: 14.83093649148941\n",
      "cnt: 2 - val loss: 4.08910596370697 - train loss: 14.813932597637177\n",
      "cnt: 0 - val loss: 4.196622997522354 - train loss: 14.786595791578293\n",
      "cnt: 1 - val loss: 4.064005583524704 - train loss: 14.763548642396927\n",
      "cnt: 0 - val loss: 4.132846415042877 - train loss: 14.763337105512619\n",
      "cnt: 1 - val loss: 4.1531069576740265 - train loss: 14.753597110509872\n",
      "cnt: 2 - val loss: 4.093039453029633 - train loss: 14.751302361488342\n",
      "cnt: 3 - val loss: 4.20212134718895 - train loss: 14.724594622850418\n",
      "cnt: 4 - val loss: 4.005004853010178 - train loss: 14.70878154039383\n",
      "cnt: 0 - val loss: 4.193088620901108 - train loss: 14.685986280441284\n",
      "cnt: 1 - val loss: 4.142365485429764 - train loss: 14.66907063126564\n",
      "cnt: 2 - val loss: 4.083328276872635 - train loss: 14.665944129228592\n",
      "cnt: 3 - val loss: 4.058880299329758 - train loss: 14.657419353723526\n",
      "cnt: 4 - val loss: 4.114535272121429 - train loss: 14.631850928068161\n",
      "cnt: 5 - val loss: 4.035304814577103 - train loss: 14.61389821767807\n",
      "cnt: 6 - val loss: 4.120386481285095 - train loss: 14.60740801692009\n",
      "cnt: 7 - val loss: 4.031516075134277 - train loss: 14.587902426719666\n",
      "cnt: 8 - val loss: 4.134232848882675 - train loss: 14.566149234771729\n",
      "cnt: 9 - val loss: 4.036305546760559 - train loss: 14.559894561767578\n",
      "cnt: 10 - val loss: 3.9658036828041077 - train loss: 14.551751792430878\n",
      "cnt: 0 - val loss: 4.062136024236679 - train loss: 14.536019295454025\n",
      "cnt: 1 - val loss: 4.0507011115550995 - train loss: 14.522302955389023\n",
      "cnt: 2 - val loss: 4.068274408578873 - train loss: 14.507656157016754\n",
      "cnt: 3 - val loss: 3.9671230018138885 - train loss: 14.486495435237885\n",
      "cnt: 4 - val loss: 4.065666615962982 - train loss: 14.477999955415726\n",
      "cnt: 5 - val loss: 4.0088677406311035 - train loss: 14.468551129102707\n",
      "cnt: 6 - val loss: 4.024394780397415 - train loss: 14.448700040578842\n",
      "cnt: 7 - val loss: 4.102850168943405 - train loss: 14.434138894081116\n",
      "cnt: 8 - val loss: 4.015994489192963 - train loss: 14.427626132965088\n",
      "cnt: 9 - val loss: 4.033120363950729 - train loss: 14.402209758758545\n",
      "cnt: 10 - val loss: 3.9989554286003113 - train loss: 14.395120948553085\n",
      "cnt: 11 - val loss: 4.032443583011627 - train loss: 14.403074443340302\n",
      "cnt: 12 - val loss: 4.006710320711136 - train loss: 14.374281167984009\n",
      "cnt: 13 - val loss: 3.964939147233963 - train loss: 14.359990388154984\n",
      "cnt: 0 - val loss: 3.995416224002838 - train loss: 14.346501260995865\n",
      "cnt: 1 - val loss: 4.036228120326996 - train loss: 14.345103025436401\n",
      "cnt: 2 - val loss: 3.94596004486084 - train loss: 14.314934998750687\n",
      "cnt: 0 - val loss: 3.927825689315796 - train loss: 14.299965977668762\n",
      "cnt: 0 - val loss: 4.072533279657364 - train loss: 14.292552709579468\n",
      "cnt: 1 - val loss: 4.002997815608978 - train loss: 14.287141293287277\n",
      "cnt: 2 - val loss: 4.034569263458252 - train loss: 14.267497807741165\n",
      "cnt: 3 - val loss: 4.090276122093201 - train loss: 14.251757234334946\n",
      "cnt: 4 - val loss: 4.0372366309165955 - train loss: 14.259108006954193\n",
      "cnt: 5 - val loss: 4.042371928691864 - train loss: 14.2342629134655\n",
      "cnt: 6 - val loss: 3.904713958501816 - train loss: 14.214421212673187\n",
      "cnt: 0 - val loss: 3.928394466638565 - train loss: 14.21132841706276\n",
      "cnt: 1 - val loss: 4.028008759021759 - train loss: 14.188415050506592\n",
      "cnt: 2 - val loss: 3.9905636608600616 - train loss: 14.189343988895416\n",
      "cnt: 3 - val loss: 3.9607811868190765 - train loss: 14.160418212413788\n",
      "cnt: 4 - val loss: 3.861867368221283 - train loss: 14.158390432596207\n",
      "cnt: 0 - val loss: 3.9990140199661255 - train loss: 14.14259448647499\n",
      "cnt: 1 - val loss: 3.8648425340652466 - train loss: 14.124046295881271\n",
      "cnt: 2 - val loss: 3.953638195991516 - train loss: 14.114885687828064\n",
      "cnt: 3 - val loss: 4.034942597150803 - train loss: 14.112843543291092\n",
      "cnt: 4 - val loss: 3.9414720237255096 - train loss: 14.093844443559647\n",
      "cnt: 5 - val loss: 3.931205004453659 - train loss: 14.074678599834442\n",
      "cnt: 6 - val loss: 3.8938081562519073 - train loss: 14.05839627981186\n",
      "cnt: 7 - val loss: 3.8872999250888824 - train loss: 14.053808689117432\n",
      "cnt: 8 - val loss: 4.016780972480774 - train loss: 14.040013074874878\n",
      "cnt: 9 - val loss: 3.8601267337799072 - train loss: 14.02606937289238\n",
      "cnt: 0 - val loss: 3.907351464033127 - train loss: 14.018588453531265\n",
      "cnt: 1 - val loss: 3.9784475564956665 - train loss: 13.984316289424896\n",
      "cnt: 2 - val loss: 3.923308163881302 - train loss: 13.994976848363876\n",
      "cnt: 3 - val loss: 3.8685519695281982 - train loss: 13.970066666603088\n",
      "cnt: 4 - val loss: 3.9976969957351685 - train loss: 13.957024991512299\n",
      "cnt: 5 - val loss: 3.9868226647377014 - train loss: 13.945441514253616\n",
      "cnt: 6 - val loss: 3.951586037874222 - train loss: 13.940831303596497\n",
      "cnt: 7 - val loss: 3.845699191093445 - train loss: 13.940255433321\n",
      "cnt: 0 - val loss: 3.913788825273514 - train loss: 13.91627624630928\n",
      "cnt: 1 - val loss: 3.8549951016902924 - train loss: 13.898654639720917\n",
      "cnt: 2 - val loss: 3.858852297067642 - train loss: 13.895482003688812\n",
      "cnt: 3 - val loss: 3.96402707695961 - train loss: 13.879720538854599\n",
      "cnt: 4 - val loss: 3.871860533952713 - train loss: 13.876929819583893\n",
      "cnt: 5 - val loss: 3.86337348818779 - train loss: 13.857102483510971\n",
      "cnt: 6 - val loss: 3.8666136264801025 - train loss: 13.849601835012436\n",
      "cnt: 7 - val loss: 3.8605163395404816 - train loss: 13.836804300546646\n",
      "cnt: 8 - val loss: 3.8317538499832153 - train loss: 13.817998737096786\n",
      "cnt: 0 - val loss: 3.8082577884197235 - train loss: 13.80556920170784\n",
      "cnt: 0 - val loss: 3.9547324180603027 - train loss: 13.805713176727295\n",
      "cnt: 1 - val loss: 3.8437426686286926 - train loss: 13.79177325963974\n",
      "cnt: 2 - val loss: 3.8697521686553955 - train loss: 13.76716873049736\n",
      "cnt: 3 - val loss: 3.9243251979351044 - train loss: 13.765759527683258\n",
      "cnt: 4 - val loss: 3.83733469247818 - train loss: 13.74025171995163\n",
      "cnt: 5 - val loss: 3.838829070329666 - train loss: 13.73439684510231\n",
      "cnt: 6 - val loss: 3.863213986158371 - train loss: 13.735708236694336\n",
      "cnt: 7 - val loss: 3.877562016248703 - train loss: 13.71933102607727\n",
      "cnt: 8 - val loss: 3.7892132103443146 - train loss: 13.708524107933044\n",
      "cnt: 0 - val loss: 3.7901068031787872 - train loss: 13.697452545166016\n",
      "cnt: 1 - val loss: 3.8001248240470886 - train loss: 13.691134005784988\n",
      "cnt: 2 - val loss: 3.923784911632538 - train loss: 13.660100221633911\n",
      "cnt: 3 - val loss: 3.8863130509853363 - train loss: 13.665050059556961\n",
      "cnt: 4 - val loss: 3.807621330022812 - train loss: 13.658800929784775\n",
      "cnt: 5 - val loss: 3.7949106097221375 - train loss: 13.640150874853134\n",
      "cnt: 6 - val loss: 3.7839259803295135 - train loss: 13.636619120836258\n",
      "cnt: 0 - val loss: 3.7970676124095917 - train loss: 13.618236601352692\n",
      "cnt: 1 - val loss: 3.7872765362262726 - train loss: 13.595126390457153\n",
      "cnt: 2 - val loss: 3.7770801782608032 - train loss: 13.603020995855331\n",
      "cnt: 0 - val loss: 3.9217827320098877 - train loss: 13.595302194356918\n",
      "cnt: 1 - val loss: 3.7737514674663544 - train loss: 13.576095193624496\n",
      "cnt: 0 - val loss: 3.737976998090744 - train loss: 13.575122147798538\n",
      "cnt: 0 - val loss: 3.9374197125434875 - train loss: 13.561220109462738\n",
      "cnt: 1 - val loss: 3.752321094274521 - train loss: 13.551124066114426\n",
      "cnt: 2 - val loss: 3.788766473531723 - train loss: 13.532956779003143\n",
      "cnt: 3 - val loss: 3.7367301285266876 - train loss: 13.517120897769928\n",
      "cnt: 0 - val loss: 3.884823977947235 - train loss: 13.513918966054916\n",
      "cnt: 1 - val loss: 3.8396809697151184 - train loss: 13.50875174999237\n",
      "cnt: 2 - val loss: 3.7744868397712708 - train loss: 13.505353510379791\n",
      "cnt: 3 - val loss: 3.698079079389572 - train loss: 13.496263891458511\n",
      "cnt: 0 - val loss: 3.789050042629242 - train loss: 13.482250481843948\n",
      "cnt: 1 - val loss: 3.7354338467121124 - train loss: 13.459809422492981\n",
      "cnt: 2 - val loss: 3.7352408170700073 - train loss: 13.46090817451477\n",
      "cnt: 3 - val loss: 3.8134202361106873 - train loss: 13.454583883285522\n",
      "cnt: 4 - val loss: 3.740525782108307 - train loss: 13.435348778963089\n",
      "cnt: 5 - val loss: 3.724985271692276 - train loss: 13.424561828374863\n",
      "cnt: 6 - val loss: 3.8128465712070465 - train loss: 13.40782955288887\n",
      "cnt: 7 - val loss: 3.7083783745765686 - train loss: 13.406740456819534\n",
      "cnt: 8 - val loss: 3.7959004938602448 - train loss: 13.387621223926544\n",
      "cnt: 9 - val loss: 3.7948824763298035 - train loss: 13.37756296992302\n",
      "cnt: 10 - val loss: 3.720178574323654 - train loss: 13.383672654628754\n",
      "cnt: 11 - val loss: 3.7070791125297546 - train loss: 13.36869502067566\n",
      "cnt: 12 - val loss: 3.871491312980652 - train loss: 13.366093635559082\n",
      "cnt: 13 - val loss: 3.742444187402725 - train loss: 13.34416925907135\n",
      "cnt: 14 - val loss: 3.794444054365158 - train loss: 13.333039939403534\n",
      "cnt: 15 - val loss: 3.715639978647232 - train loss: 13.325697481632233\n",
      "cnt: 16 - val loss: 3.705092281103134 - train loss: 13.305218666791916\n",
      "cnt: 17 - val loss: 3.736077159643173 - train loss: 13.310361087322235\n",
      "cnt: 18 - val loss: 3.6865543127059937 - train loss: 13.294691145420074\n",
      "cnt: 0 - val loss: 3.7114715576171875 - train loss: 13.293182402849197\n",
      "cnt: 1 - val loss: 3.7165593206882477 - train loss: 13.290636956691742\n",
      "cnt: 2 - val loss: 3.7197285294532776 - train loss: 13.268009066581726\n",
      "cnt: 3 - val loss: 3.7193164825439453 - train loss: 13.261900454759598\n",
      "cnt: 4 - val loss: 3.660787910223007 - train loss: 13.255175739526749\n",
      "cnt: 0 - val loss: 3.696124404668808 - train loss: 13.242237120866776\n",
      "cnt: 1 - val loss: 3.7038314044475555 - train loss: 13.231470882892609\n",
      "cnt: 2 - val loss: 3.731304109096527 - train loss: 13.228116065263748\n",
      "cnt: 3 - val loss: 3.7637664079666138 - train loss: 13.218948811292648\n",
      "cnt: 4 - val loss: 3.754025310277939 - train loss: 13.203615814447403\n",
      "cnt: 5 - val loss: 3.717484951019287 - train loss: 13.200672268867493\n",
      "cnt: 6 - val loss: 3.629668176174164 - train loss: 13.17454394698143\n",
      "cnt: 0 - val loss: 3.6397995054721832 - train loss: 13.178420275449753\n",
      "cnt: 1 - val loss: 3.6987219154834747 - train loss: 13.17405852675438\n",
      "cnt: 2 - val loss: 3.726711928844452 - train loss: 13.156986445188522\n",
      "cnt: 3 - val loss: 3.6605996191501617 - train loss: 13.155578166246414\n",
      "cnt: 4 - val loss: 3.6562174558639526 - train loss: 13.147082298994064\n",
      "cnt: 5 - val loss: 3.7966600954532623 - train loss: 13.120191127061844\n",
      "cnt: 6 - val loss: 3.6869159936904907 - train loss: 13.123066753149033\n",
      "cnt: 7 - val loss: 3.6800132393836975 - train loss: 13.109267354011536\n",
      "cnt: 8 - val loss: 3.6259348392486572 - train loss: 13.105612725019455\n",
      "cnt: 0 - val loss: 3.724602848291397 - train loss: 13.09687465429306\n",
      "cnt: 1 - val loss: 3.6017967760562897 - train loss: 13.082761824131012\n",
      "cnt: 0 - val loss: 3.639266461133957 - train loss: 13.07508733868599\n",
      "cnt: 1 - val loss: 3.5822793543338776 - train loss: 13.068569958209991\n",
      "cnt: 0 - val loss: 3.6582475006580353 - train loss: 13.069788455963135\n",
      "cnt: 1 - val loss: 3.593002051115036 - train loss: 13.051308244466782\n",
      "cnt: 2 - val loss: 3.6688172221183777 - train loss: 13.051336973905563\n",
      "cnt: 3 - val loss: 3.581691801548004 - train loss: 13.030427306890488\n",
      "cnt: 0 - val loss: 3.5797510147094727 - train loss: 13.022464215755463\n",
      "cnt: 0 - val loss: 3.5802472829818726 - train loss: 13.009064942598343\n",
      "cnt: 1 - val loss: 3.6264165341854095 - train loss: 13.008849114179611\n",
      "cnt: 2 - val loss: 3.6439490020275116 - train loss: 13.011105448007584\n",
      "cnt: 3 - val loss: 3.641251027584076 - train loss: 12.989672422409058\n",
      "cnt: 4 - val loss: 3.6143282651901245 - train loss: 12.990639299154282\n",
      "cnt: 5 - val loss: 3.56412872672081 - train loss: 12.977545619010925\n",
      "cnt: 0 - val loss: 3.561801552772522 - train loss: 12.957374811172485\n",
      "cnt: 0 - val loss: 3.5636546909809113 - train loss: 12.951598793268204\n",
      "cnt: 1 - val loss: 3.6584655046463013 - train loss: 12.943472862243652\n",
      "cnt: 2 - val loss: 3.6877286434173584 - train loss: 12.95326241850853\n",
      "cnt: 3 - val loss: 3.6359833776950836 - train loss: 12.932815164327621\n",
      "cnt: 4 - val loss: 3.5940676033496857 - train loss: 12.922545790672302\n",
      "cnt: 5 - val loss: 3.735379070043564 - train loss: 12.907039284706116\n",
      "cnt: 6 - val loss: 3.6754122972488403 - train loss: 12.91475522518158\n",
      "cnt: 7 - val loss: 3.5878645181655884 - train loss: 12.90156564116478\n",
      "cnt: 8 - val loss: 3.617875635623932 - train loss: 12.884425461292267\n",
      "cnt: 9 - val loss: 3.5797327160835266 - train loss: 12.881507277488708\n",
      "cnt: 10 - val loss: 3.6335289776325226 - train loss: 12.879780262708664\n",
      "cnt: 11 - val loss: 3.5924829244613647 - train loss: 12.877112835645676\n",
      "cnt: 12 - val loss: 3.6408019959926605 - train loss: 12.862750858068466\n",
      "cnt: 13 - val loss: 3.586412191390991 - train loss: 12.84257596731186\n",
      "cnt: 14 - val loss: 3.591976135969162 - train loss: 12.848477303981781\n",
      "cnt: 15 - val loss: 3.7006545066833496 - train loss: 12.841042965650558\n",
      "cnt: 16 - val loss: 3.5783438980579376 - train loss: 12.826500803232193\n",
      "cnt: 17 - val loss: 3.565892219543457 - train loss: 12.833165228366852\n",
      "cnt: 18 - val loss: 3.813176602125168 - train loss: 12.809934675693512\n",
      "cnt: 19 - val loss: 3.5640245378017426 - train loss: 12.81684535741806\n",
      "cnt: 20 - val loss: 3.580563932657242 - train loss: 12.79696011543274\n",
      "cnt: 21 - val loss: 3.5633623898029327 - train loss: 12.79801219701767\n",
      "cnt: 22 - val loss: 3.5443862974643707 - train loss: 12.785196393728256\n",
      "cnt: 0 - val loss: 3.550089716911316 - train loss: 12.784981697797775\n",
      "cnt: 1 - val loss: 3.4859219193458557 - train loss: 12.764370650053024\n",
      "cnt: 0 - val loss: 3.582133114337921 - train loss: 12.763360649347305\n",
      "cnt: 1 - val loss: 3.5879401564598083 - train loss: 12.745757699012756\n",
      "cnt: 2 - val loss: 3.5343735218048096 - train loss: 12.749726563692093\n",
      "cnt: 3 - val loss: 3.5971831381320953 - train loss: 12.739540219306946\n",
      "cnt: 4 - val loss: 3.649606019258499 - train loss: 12.735446184873581\n",
      "cnt: 5 - val loss: 3.6807931661605835 - train loss: 12.72609069943428\n",
      "cnt: 6 - val loss: 3.5217280089855194 - train loss: 12.71801307797432\n",
      "cnt: 7 - val loss: 3.5782836079597473 - train loss: 12.704241901636124\n",
      "cnt: 8 - val loss: 3.5181318521499634 - train loss: 12.708827435970306\n",
      "cnt: 9 - val loss: 3.533166766166687 - train loss: 12.69351390004158\n",
      "cnt: 10 - val loss: 3.5629492700099945 - train loss: 12.69980525970459\n",
      "cnt: 11 - val loss: 3.5929236114025116 - train loss: 12.680350989103317\n",
      "cnt: 12 - val loss: 3.5905039310455322 - train loss: 12.673038393259048\n",
      "cnt: 13 - val loss: 3.5838717818260193 - train loss: 12.670797944068909\n",
      "cnt: 14 - val loss: 3.5589021146297455 - train loss: 12.664667427539825\n",
      "cnt: 15 - val loss: 3.5570394694805145 - train loss: 12.65238881111145\n",
      "cnt: 16 - val loss: 3.396617218852043 - train loss: 12.649585574865341\n",
      "cnt: 0 - val loss: 3.556276172399521 - train loss: 12.649317264556885\n",
      "cnt: 1 - val loss: 3.514364868402481 - train loss: 12.638681262731552\n",
      "cnt: 2 - val loss: 3.491538017988205 - train loss: 12.624170809984207\n",
      "cnt: 3 - val loss: 3.5392946004867554 - train loss: 12.620837271213531\n",
      "cnt: 4 - val loss: 3.4821209013462067 - train loss: 12.606641352176666\n",
      "cnt: 5 - val loss: 3.4965311884880066 - train loss: 12.597931683063507\n",
      "cnt: 6 - val loss: 3.6839534640312195 - train loss: 12.59106969833374\n",
      "cnt: 7 - val loss: 3.525067001581192 - train loss: 12.587037801742554\n",
      "cnt: 8 - val loss: 3.484997659921646 - train loss: 12.573700815439224\n",
      "cnt: 9 - val loss: 3.465338855981827 - train loss: 12.580395519733429\n",
      "cnt: 10 - val loss: 3.4873043298721313 - train loss: 12.56294259428978\n",
      "cnt: 11 - val loss: 3.480563312768936 - train loss: 12.573176145553589\n",
      "cnt: 12 - val loss: 3.574991226196289 - train loss: 12.555678129196167\n",
      "cnt: 13 - val loss: 3.5663750767707825 - train loss: 12.551255822181702\n",
      "cnt: 14 - val loss: 3.4575356245040894 - train loss: 12.549272388219833\n",
      "cnt: 15 - val loss: 3.430694818496704 - train loss: 12.529821634292603\n",
      "cnt: 16 - val loss: 3.5565200448036194 - train loss: 12.53329086303711\n",
      "cnt: 17 - val loss: 3.456348091363907 - train loss: 12.516231447458267\n",
      "cnt: 18 - val loss: 3.498112827539444 - train loss: 12.510341584682465\n",
      "cnt: 19 - val loss: 3.532141000032425 - train loss: 12.506000250577927\n",
      "cnt: 20 - val loss: 3.4217300415039062 - train loss: 12.509277135133743\n",
      "cnt: 21 - val loss: 3.4851819574832916 - train loss: 12.49991312623024\n",
      "cnt: 22 - val loss: 3.4922851622104645 - train loss: 12.497578918933868\n",
      "cnt: 23 - val loss: 3.426318973302841 - train loss: 12.472673922777176\n",
      "cnt: 24 - val loss: 3.499832332134247 - train loss: 12.470276653766632\n",
      "cnt: 25 - val loss: 3.4111341536045074 - train loss: 12.463557183742523\n",
      "cnt: 26 - val loss: 3.4792079627513885 - train loss: 12.458196729421616\n",
      "cnt: 27 - val loss: 3.4274575114250183 - train loss: 12.45715218782425\n",
      "cnt: 28 - val loss: 3.4868738055229187 - train loss: 12.453024238348007\n",
      "cnt: 29 - val loss: 3.4352211356163025 - train loss: 12.443628519773483\n",
      "cnt: 30 - val loss: 3.4244092404842377 - train loss: 12.438532620668411\n",
      "cnt: 31 - val loss: 3.4515017569065094 - train loss: 12.436868816614151\n",
      "cnt: 32 - val loss: 3.4585581719875336 - train loss: 12.430556923151016\n",
      "cnt: 33 - val loss: 3.4034297466278076 - train loss: 12.419654071331024\n",
      "cnt: 34 - val loss: 3.524114489555359 - train loss: 12.407267451286316\n",
      "cnt: 35 - val loss: 3.4119275510311127 - train loss: 12.401728421449661\n",
      "cnt: 36 - val loss: 3.5684064626693726 - train loss: 12.396204173564911\n",
      "cnt: 37 - val loss: 3.4737877547740936 - train loss: 12.390468746423721\n",
      "cnt: 38 - val loss: 3.4267308712005615 - train loss: 12.394093215465546\n",
      "cnt: 39 - val loss: 3.487027883529663 - train loss: 12.385378867387772\n",
      "cnt: 40 - val loss: 3.492229640483856 - train loss: 12.389416605234146\n",
      "cnt: 41 - val loss: 3.477358102798462 - train loss: 12.363338619470596\n",
      "cnt: 42 - val loss: 3.5670651495456696 - train loss: 12.366614818572998\n",
      "cnt: 43 - val loss: 3.448411673307419 - train loss: 12.358807116746902\n",
      "cnt: 44 - val loss: 3.4104848504066467 - train loss: 12.346313625574112\n",
      "cnt: 45 - val loss: 3.558127462863922 - train loss: 12.338561862707138\n",
      "cnt: 46 - val loss: 3.5255859792232513 - train loss: 12.32531252503395\n",
      "cnt: 47 - val loss: 3.434611141681671 - train loss: 12.333630532026291\n",
      "cnt: 48 - val loss: 3.331629529595375 - train loss: 12.324058502912521\n",
      "cnt: 0 - val loss: 3.4251860678195953 - train loss: 12.316412061452866\n",
      "cnt: 1 - val loss: 3.4203474521636963 - train loss: 12.303143799304962\n",
      "cnt: 2 - val loss: 3.465954065322876 - train loss: 12.299331933259964\n",
      "cnt: 3 - val loss: 3.4746523797512054 - train loss: 12.295145958662033\n",
      "cnt: 4 - val loss: 3.394855558872223 - train loss: 12.292962551116943\n",
      "cnt: 5 - val loss: 3.3653523921966553 - train loss: 12.289072811603546\n",
      "cnt: 6 - val loss: 3.420384645462036 - train loss: 12.279502511024475\n",
      "cnt: 7 - val loss: 3.4692441523075104 - train loss: 12.265467703342438\n",
      "cnt: 8 - val loss: 3.50631582736969 - train loss: 12.25808709859848\n",
      "cnt: 9 - val loss: 3.350926488637924 - train loss: 12.25676143169403\n",
      "cnt: 10 - val loss: 3.4233309030532837 - train loss: 12.262131005525589\n",
      "cnt: 11 - val loss: 3.3461743891239166 - train loss: 12.24223741889\n",
      "cnt: 12 - val loss: 3.4340413212776184 - train loss: 12.247971057891846\n",
      "cnt: 13 - val loss: 3.426548719406128 - train loss: 12.23868328332901\n",
      "cnt: 14 - val loss: 3.3815905153751373 - train loss: 12.22975206375122\n",
      "cnt: 15 - val loss: 3.4188657701015472 - train loss: 12.228280633687973\n",
      "cnt: 16 - val loss: 3.385698616504669 - train loss: 12.215154439210892\n",
      "cnt: 17 - val loss: 3.4721401631832123 - train loss: 12.21775135397911\n",
      "cnt: 18 - val loss: 3.455210506916046 - train loss: 12.202789187431335\n",
      "cnt: 19 - val loss: 3.420842856168747 - train loss: 12.20783594250679\n",
      "cnt: 20 - val loss: 3.462486743927002 - train loss: 12.206444412469864\n",
      "cnt: 21 - val loss: 3.414448082447052 - train loss: 12.17821004986763\n",
      "cnt: 22 - val loss: 3.3985645174980164 - train loss: 12.18663102388382\n",
      "cnt: 23 - val loss: 3.423189014196396 - train loss: 12.18541333079338\n",
      "cnt: 24 - val loss: 3.4314466416835785 - train loss: 12.172670602798462\n",
      "cnt: 25 - val loss: 3.4515497386455536 - train loss: 12.172475069761276\n",
      "cnt: 26 - val loss: 3.4238339960575104 - train loss: 12.156756162643433\n",
      "cnt: 27 - val loss: 3.361332058906555 - train loss: 12.152802914381027\n",
      "cnt: 28 - val loss: 3.372576117515564 - train loss: 12.167030155658722\n",
      "cnt: 29 - val loss: 3.3431442081928253 - train loss: 12.140920162200928\n",
      "cnt: 30 - val loss: 3.491008698940277 - train loss: 12.147797644138336\n",
      "cnt: 31 - val loss: 3.355292171239853 - train loss: 12.137072116136551\n",
      "cnt: 32 - val loss: 3.4735933542251587 - train loss: 12.131996721029282\n",
      "cnt: 33 - val loss: 3.390017122030258 - train loss: 12.140071213245392\n",
      "cnt: 34 - val loss: 3.426358997821808 - train loss: 12.118817955255508\n",
      "cnt: 35 - val loss: 3.406722843647003 - train loss: 12.10472160577774\n",
      "cnt: 36 - val loss: 3.463098794221878 - train loss: 12.11524522304535\n",
      "cnt: 37 - val loss: 3.511549025774002 - train loss: 12.103400498628616\n",
      "cnt: 38 - val loss: 3.5180200338363647 - train loss: 12.100254774093628\n",
      "cnt: 39 - val loss: 3.374446928501129 - train loss: 12.076054006814957\n",
      "cnt: 40 - val loss: 3.3045067191123962 - train loss: 12.086025059223175\n",
      "cnt: 0 - val loss: 3.362330138683319 - train loss: 12.084232449531555\n",
      "cnt: 1 - val loss: 3.332446902990341 - train loss: 12.076505929231644\n",
      "cnt: 2 - val loss: 3.4143128097057343 - train loss: 12.070892661809921\n",
      "cnt: 3 - val loss: 3.3833820521831512 - train loss: 12.070172220468521\n",
      "cnt: 4 - val loss: 3.336915910243988 - train loss: 12.055682182312012\n",
      "cnt: 5 - val loss: 3.35732838511467 - train loss: 12.05335283279419\n",
      "cnt: 6 - val loss: 3.4684678316116333 - train loss: 12.045933604240417\n",
      "cnt: 7 - val loss: 3.414023607969284 - train loss: 12.049888730049133\n",
      "cnt: 8 - val loss: 3.3128271102905273 - train loss: 12.03513789176941\n",
      "cnt: 9 - val loss: 3.30067577958107 - train loss: 12.029882669448853\n",
      "cnt: 0 - val loss: 3.3700386583805084 - train loss: 12.03285637497902\n",
      "cnt: 1 - val loss: 3.3417804837226868 - train loss: 12.011854588985443\n",
      "cnt: 2 - val loss: 3.383026421070099 - train loss: 12.011029243469238\n",
      "cnt: 3 - val loss: 3.335793286561966 - train loss: 12.006366461515427\n",
      "cnt: 4 - val loss: 3.3184724748134613 - train loss: 12.009358078241348\n",
      "cnt: 5 - val loss: 3.3343193531036377 - train loss: 12.020029932260513\n",
      "cnt: 6 - val loss: 3.309141367673874 - train loss: 11.991390198469162\n",
      "cnt: 7 - val loss: 3.420767158269882 - train loss: 12.004284292459488\n",
      "cnt: 8 - val loss: 3.355840116739273 - train loss: 11.994870781898499\n",
      "cnt: 9 - val loss: 3.3760760128498077 - train loss: 11.975898444652557\n",
      "cnt: 10 - val loss: 3.2987582087516785 - train loss: 11.980335265398026\n",
      "cnt: 0 - val loss: 3.269536793231964 - train loss: 11.976980328559875\n",
      "cnt: 0 - val loss: 3.375019282102585 - train loss: 11.96388727426529\n",
      "cnt: 1 - val loss: 3.308556705713272 - train loss: 11.954716354608536\n",
      "cnt: 2 - val loss: 3.3981305062770844 - train loss: 11.956357270479202\n",
      "cnt: 3 - val loss: 3.3332785069942474 - train loss: 11.940157443284988\n",
      "cnt: 4 - val loss: 3.318548768758774 - train loss: 11.945825755596161\n",
      "cnt: 5 - val loss: 3.4682019650936127 - train loss: 11.946974962949753\n",
      "cnt: 6 - val loss: 3.359542816877365 - train loss: 11.938773095607758\n",
      "cnt: 7 - val loss: 3.381736546754837 - train loss: 11.922793209552765\n",
      "cnt: 8 - val loss: 3.3320544064044952 - train loss: 11.919142812490463\n",
      "cnt: 9 - val loss: 3.2862987220287323 - train loss: 11.918779462575912\n",
      "cnt: 10 - val loss: 3.3541098833084106 - train loss: 11.905525118112564\n",
      "cnt: 11 - val loss: 3.3548279106616974 - train loss: 11.913474887609482\n",
      "cnt: 12 - val loss: 3.318098694086075 - train loss: 11.906436145305634\n",
      "cnt: 13 - val loss: 3.296328693628311 - train loss: 11.893820106983185\n",
      "cnt: 14 - val loss: 3.414539396762848 - train loss: 11.903284281492233\n",
      "cnt: 15 - val loss: 3.3155344128608704 - train loss: 11.900460749864578\n",
      "cnt: 16 - val loss: 3.3171535432338715 - train loss: 11.884919196367264\n",
      "cnt: 17 - val loss: 3.316663146018982 - train loss: 11.885946333408356\n",
      "cnt: 18 - val loss: 3.34182471036911 - train loss: 11.876455157995224\n",
      "cnt: 19 - val loss: 3.3772334456443787 - train loss: 11.87189769744873\n",
      "cnt: 20 - val loss: 3.3262974321842194 - train loss: 11.857605397701263\n",
      "cnt: 21 - val loss: 3.3029637336730957 - train loss: 11.852633714675903\n",
      "cnt: 22 - val loss: 3.2647606134414673 - train loss: 11.85302197933197\n",
      "cnt: 0 - val loss: 3.337244361639023 - train loss: 11.85389992594719\n",
      "cnt: 1 - val loss: 3.2609240114688873 - train loss: 11.858566761016846\n",
      "cnt: 0 - val loss: 3.350022405385971 - train loss: 11.825419247150421\n",
      "cnt: 1 - val loss: 3.320066213607788 - train loss: 11.836642861366272\n",
      "cnt: 2 - val loss: 3.34672012925148 - train loss: 11.828749656677246\n",
      "cnt: 3 - val loss: 3.274018347263336 - train loss: 11.813978791236877\n",
      "cnt: 4 - val loss: 3.4105081260204315 - train loss: 11.814627081155777\n",
      "cnt: 5 - val loss: 3.3031198382377625 - train loss: 11.829311788082123\n",
      "cnt: 6 - val loss: 3.289682298898697 - train loss: 11.827290624380112\n",
      "cnt: 7 - val loss: 3.288611739873886 - train loss: 11.804998129606247\n",
      "cnt: 8 - val loss: 3.3335370123386383 - train loss: 11.80485725402832\n",
      "cnt: 9 - val loss: 3.1738263815641403 - train loss: 11.796285092830658\n",
      "cnt: 0 - val loss: 3.2501772344112396 - train loss: 11.785778105258942\n",
      "cnt: 1 - val loss: 3.2418012022972107 - train loss: 11.777740806341171\n",
      "cnt: 2 - val loss: 3.297536551952362 - train loss: 11.77940371632576\n",
      "cnt: 3 - val loss: 3.356596976518631 - train loss: 11.772342503070831\n",
      "cnt: 4 - val loss: 3.448393166065216 - train loss: 11.779853850603104\n",
      "cnt: 5 - val loss: 3.2260454297065735 - train loss: 11.76682710647583\n",
      "cnt: 6 - val loss: 3.41994246840477 - train loss: 11.759710937738419\n",
      "cnt: 7 - val loss: 3.2871407568454742 - train loss: 11.760925143957138\n",
      "cnt: 8 - val loss: 3.325668543577194 - train loss: 11.750730633735657\n",
      "cnt: 9 - val loss: 3.2951833605766296 - train loss: 11.752890288829803\n",
      "cnt: 10 - val loss: 3.2446972131729126 - train loss: 11.748303592205048\n",
      "cnt: 11 - val loss: 3.345987915992737 - train loss: 11.740547806024551\n",
      "cnt: 12 - val loss: 3.2900389432907104 - train loss: 11.738909184932709\n",
      "cnt: 13 - val loss: 3.258868008852005 - train loss: 11.72906231880188\n",
      "cnt: 14 - val loss: 3.2715366780757904 - train loss: 11.727736860513687\n",
      "cnt: 15 - val loss: 3.2505495846271515 - train loss: 11.729350209236145\n",
      "cnt: 16 - val loss: 3.340202033519745 - train loss: 11.720947861671448\n",
      "cnt: 17 - val loss: 3.34197735786438 - train loss: 11.715842485427856\n",
      "cnt: 18 - val loss: 3.2166285514831543 - train loss: 11.707152396440506\n",
      "cnt: 19 - val loss: 3.342313379049301 - train loss: 11.70711550116539\n",
      "cnt: 20 - val loss: 3.2428780496120453 - train loss: 11.705194056034088\n",
      "cnt: 21 - val loss: 3.2439084649086 - train loss: 11.685139924287796\n",
      "cnt: 22 - val loss: 3.247833490371704 - train loss: 11.686487317085266\n",
      "cnt: 23 - val loss: 3.2832930088043213 - train loss: 11.683122217655182\n",
      "cnt: 24 - val loss: 3.235311359167099 - train loss: 11.682700484991074\n",
      "cnt: 25 - val loss: 3.3315606713294983 - train loss: 11.674961149692535\n",
      "cnt: 26 - val loss: 3.3533114194869995 - train loss: 11.675024688243866\n",
      "cnt: 27 - val loss: 3.296783596277237 - train loss: 11.673267990350723\n",
      "cnt: 28 - val loss: 3.2569316029548645 - train loss: 11.673530995845795\n",
      "cnt: 29 - val loss: 3.241503119468689 - train loss: 11.658792942762375\n",
      "cnt: 30 - val loss: 3.2969738841056824 - train loss: 11.655244380235672\n",
      "cnt: 31 - val loss: 3.240140199661255 - train loss: 11.650336295366287\n",
      "cnt: 32 - val loss: 3.2805781066417694 - train loss: 11.651003271341324\n",
      "cnt: 33 - val loss: 3.2394683361053467 - train loss: 11.642414063215256\n",
      "cnt: 34 - val loss: 3.2935449182987213 - train loss: 11.642595499753952\n",
      "cnt: 35 - val loss: 3.2935980558395386 - train loss: 11.643309861421585\n",
      "cnt: 36 - val loss: 3.2349056005477905 - train loss: 11.638264656066895\n",
      "cnt: 37 - val loss: 3.2749923169612885 - train loss: 11.616506814956665\n",
      "cnt: 38 - val loss: 3.2539934515953064 - train loss: 11.627148121595383\n",
      "cnt: 39 - val loss: 3.3349629938602448 - train loss: 11.607654362916946\n",
      "cnt: 40 - val loss: 3.2074244022369385 - train loss: 11.611649990081787\n",
      "cnt: 41 - val loss: 3.3118377029895782 - train loss: 11.597216874361038\n",
      "cnt: 42 - val loss: 3.2195747196674347 - train loss: 11.60615223646164\n",
      "cnt: 43 - val loss: 3.1745011508464813 - train loss: 11.595922708511353\n",
      "cnt: 44 - val loss: 3.172733187675476 - train loss: 11.596765667200089\n",
      "cnt: 0 - val loss: 3.2494043111801147 - train loss: 11.588175654411316\n",
      "cnt: 1 - val loss: 3.195998936891556 - train loss: 11.595266669988632\n",
      "cnt: 2 - val loss: 3.238780438899994 - train loss: 11.579084008932114\n",
      "cnt: 3 - val loss: 3.236287921667099 - train loss: 11.57429650425911\n",
      "cnt: 4 - val loss: 3.2938392162323 - train loss: 11.592289060354233\n",
      "cnt: 5 - val loss: 3.321430742740631 - train loss: 11.557783722877502\n",
      "cnt: 6 - val loss: 3.258953720331192 - train loss: 11.557308912277222\n",
      "cnt: 7 - val loss: 3.2092506289482117 - train loss: 11.551828801631927\n",
      "cnt: 8 - val loss: 3.2376865446567535 - train loss: 11.549379676580429\n",
      "cnt: 9 - val loss: 3.2932792007923126 - train loss: 11.550790548324585\n",
      "cnt: 10 - val loss: 3.223201245069504 - train loss: 11.550579249858856\n",
      "cnt: 11 - val loss: 3.2524518966674805 - train loss: 11.542824923992157\n",
      "cnt: 12 - val loss: 3.216171473264694 - train loss: 11.53936505317688\n",
      "cnt: 13 - val loss: 3.2142083942890167 - train loss: 11.533192694187164\n",
      "cnt: 14 - val loss: 3.2709963619709015 - train loss: 11.53059408068657\n",
      "cnt: 15 - val loss: 3.2051626443862915 - train loss: 11.525032669305801\n",
      "cnt: 16 - val loss: 3.2177200615406036 - train loss: 11.521021872758865\n",
      "cnt: 17 - val loss: 3.2963398694992065 - train loss: 11.519582986831665\n",
      "cnt: 18 - val loss: 3.2813168466091156 - train loss: 11.517652720212936\n",
      "cnt: 19 - val loss: 3.196964144706726 - train loss: 11.507508844137192\n",
      "cnt: 20 - val loss: 3.2760896384716034 - train loss: 11.500948429107666\n",
      "cnt: 21 - val loss: 3.3823069036006927 - train loss: 11.507191091775894\n",
      "cnt: 22 - val loss: 3.1849564015865326 - train loss: 11.507490575313568\n",
      "cnt: 23 - val loss: 3.238101303577423 - train loss: 11.488318383693695\n",
      "cnt: 24 - val loss: 3.279571384191513 - train loss: 11.499707132577896\n",
      "cnt: 25 - val loss: 3.244062662124634 - train loss: 11.48153492808342\n",
      "cnt: 26 - val loss: 3.1710787415504456 - train loss: 11.48063525557518\n",
      "cnt: 0 - val loss: 3.2568534910678864 - train loss: 11.474075227975845\n",
      "cnt: 1 - val loss: 3.2094717025756836 - train loss: 11.468805819749832\n",
      "cnt: 2 - val loss: 3.28512042760849 - train loss: 11.467746645212173\n",
      "cnt: 3 - val loss: 3.274943917989731 - train loss: 11.463440388441086\n",
      "cnt: 4 - val loss: 3.2088143825531006 - train loss: 11.462343126535416\n",
      "cnt: 5 - val loss: 3.214027017354965 - train loss: 11.4664925634861\n",
      "cnt: 6 - val loss: 3.275091141462326 - train loss: 11.45556589961052\n",
      "cnt: 7 - val loss: 3.1489263772964478 - train loss: 11.454764485359192\n",
      "cnt: 0 - val loss: 3.1098569333553314 - train loss: 11.445790439844131\n",
      "cnt: 0 - val loss: 3.212986022233963 - train loss: 11.4437495470047\n",
      "cnt: 1 - val loss: 3.25404754281044 - train loss: 11.429052591323853\n",
      "cnt: 2 - val loss: 3.2062731087207794 - train loss: 11.43992304801941\n",
      "cnt: 3 - val loss: 3.1861947774887085 - train loss: 11.41987332701683\n",
      "cnt: 4 - val loss: 3.1683157682418823 - train loss: 11.425076007843018\n",
      "cnt: 5 - val loss: 3.1955023109912872 - train loss: 11.421345591545105\n",
      "cnt: 6 - val loss: 3.0777831375598907 - train loss: 11.41735115647316\n",
      "cnt: 0 - val loss: 3.2412162125110626 - train loss: 11.418525338172913\n",
      "cnt: 1 - val loss: 3.172812193632126 - train loss: 11.412230432033539\n",
      "cnt: 2 - val loss: 3.201770603656769 - train loss: 11.403026521205902\n",
      "cnt: 3 - val loss: 3.141173094511032 - train loss: 11.389473974704742\n",
      "cnt: 4 - val loss: 3.258976310491562 - train loss: 11.403374940156937\n",
      "cnt: 5 - val loss: 3.2095599472522736 - train loss: 11.399234026670456\n",
      "cnt: 6 - val loss: 3.244965136051178 - train loss: 11.388004034757614\n",
      "cnt: 7 - val loss: 3.274788051843643 - train loss: 11.38467139005661\n",
      "cnt: 8 - val loss: 3.1219497621059418 - train loss: 11.380196869373322\n",
      "cnt: 9 - val loss: 3.1361822485923767 - train loss: 11.382647842168808\n",
      "cnt: 10 - val loss: 3.2624418437480927 - train loss: 11.3809355199337\n",
      "cnt: 11 - val loss: 3.127454400062561 - train loss: 11.38225844502449\n",
      "cnt: 12 - val loss: 3.2118591368198395 - train loss: 11.373541533946991\n",
      "cnt: 13 - val loss: 3.192201167345047 - train loss: 11.368857502937317\n",
      "cnt: 14 - val loss: 3.2822554111480713 - train loss: 11.359688848257065\n",
      "cnt: 15 - val loss: 3.2559655010700226 - train loss: 11.352121889591217\n",
      "cnt: 16 - val loss: 3.176428884267807 - train loss: 11.352087795734406\n",
      "cnt: 17 - val loss: 3.143699526786804 - train loss: 11.3541778922081\n",
      "cnt: 18 - val loss: 3.276966094970703 - train loss: 11.347369283437729\n",
      "cnt: 19 - val loss: 3.1675123274326324 - train loss: 11.351046770811081\n",
      "cnt: 20 - val loss: 3.1899574995040894 - train loss: 11.335261285305023\n",
      "cnt: 21 - val loss: 3.1576657593250275 - train loss: 11.326024532318115\n",
      "cnt: 22 - val loss: 3.1085941195487976 - train loss: 11.3368179500103\n",
      "cnt: 23 - val loss: 3.172139674425125 - train loss: 11.324836492538452\n",
      "cnt: 24 - val loss: 3.16921329498291 - train loss: 11.330876380205154\n",
      "cnt: 25 - val loss: 3.2453027069568634 - train loss: 11.313904881477356\n",
      "cnt: 26 - val loss: 3.217684954404831 - train loss: 11.317054241895676\n",
      "cnt: 27 - val loss: 3.2015227377414703 - train loss: 11.308218389749527\n",
      "cnt: 28 - val loss: 3.2777212858200073 - train loss: 11.298809796571732\n",
      "cnt: 29 - val loss: 3.2512229681015015 - train loss: 11.301070272922516\n",
      "cnt: 30 - val loss: 3.1672530472278595 - train loss: 11.296677887439728\n",
      "cnt: 31 - val loss: 3.0953086614608765 - train loss: 11.280689239501953\n",
      "cnt: 32 - val loss: 3.132498025894165 - train loss: 11.28347834944725\n",
      "cnt: 33 - val loss: 3.1726872622966766 - train loss: 11.2934290766716\n",
      "cnt: 34 - val loss: 3.1353239119052887 - train loss: 11.28527045249939\n",
      "cnt: 35 - val loss: 3.2436878979206085 - train loss: 11.283938497304916\n",
      "cnt: 36 - val loss: 3.2131484746932983 - train loss: 11.27241536974907\n",
      "cnt: 37 - val loss: 3.219131201505661 - train loss: 11.275727063417435\n",
      "cnt: 38 - val loss: 3.085163414478302 - train loss: 11.2800412774086\n",
      "cnt: 39 - val loss: 3.1758595407009125 - train loss: 11.280993282794952\n",
      "cnt: 40 - val loss: 3.23667711019516 - train loss: 11.255133152008057\n",
      "cnt: 41 - val loss: 3.1298610270023346 - train loss: 11.256390810012817\n",
      "cnt: 42 - val loss: 3.1254382133483887 - train loss: 11.260118544101715\n",
      "cnt: 43 - val loss: 3.2810378074645996 - train loss: 11.272158980369568\n",
      "cnt: 44 - val loss: 3.1707361340522766 - train loss: 11.234333157539368\n",
      "cnt: 45 - val loss: 3.1370357871055603 - train loss: 11.253856539726257\n",
      "cnt: 46 - val loss: 3.126233994960785 - train loss: 11.240302950143814\n",
      "cnt: 47 - val loss: 3.1136240661144257 - train loss: 11.233787655830383\n",
      "cnt: 48 - val loss: 3.2116831243038177 - train loss: 11.229789167642593\n",
      "cnt: 49 - val loss: 3.118213951587677 - train loss: 11.228343278169632\n",
      "cnt: 50 - val loss: 3.24911105632782 - train loss: 11.217398047447205\n",
      "cnt: 51 - val loss: 3.1719268560409546 - train loss: 11.223832368850708\n",
      "cnt: 52 - val loss: 3.190908968448639 - train loss: 11.206580728292465\n",
      "cnt: 53 - val loss: 3.178520917892456 - train loss: 11.212765127420425\n",
      "cnt: 54 - val loss: 3.113944709300995 - train loss: 11.210651725530624\n",
      "cnt: 55 - val loss: 3.213474839925766 - train loss: 11.212446242570877\n",
      "cnt: 56 - val loss: 3.0874742567539215 - train loss: 11.207965761423111\n",
      "cnt: 57 - val loss: 3.1526370644569397 - train loss: 11.20113730430603\n",
      "cnt: 58 - val loss: 3.273772805929184 - train loss: 11.196568757295609\n",
      "cnt: 59 - val loss: 3.0991553962230682 - train loss: 11.193990916013718\n",
      "cnt: 60 - val loss: 3.064448729157448 - train loss: 11.182714343070984\n",
      "cnt: 0 - val loss: 3.1773228645324707 - train loss: 11.191498041152954\n",
      "cnt: 1 - val loss: 3.1178605258464813 - train loss: 11.179355800151825\n",
      "cnt: 2 - val loss: 3.165440171957016 - train loss: 11.1842440366745\n",
      "cnt: 3 - val loss: 3.111111879348755 - train loss: 11.176850020885468\n",
      "cnt: 4 - val loss: 3.1754265427589417 - train loss: 11.162896245718002\n",
      "cnt: 5 - val loss: 3.0777466893196106 - train loss: 11.16822636127472\n",
      "cnt: 6 - val loss: 3.1112619936466217 - train loss: 11.16402354836464\n",
      "cnt: 7 - val loss: 3.175612151622772 - train loss: 11.16082838177681\n",
      "cnt: 8 - val loss: 3.155319631099701 - train loss: 11.157585710287094\n",
      "cnt: 9 - val loss: 3.1256366074085236 - train loss: 11.158591866493225\n",
      "cnt: 10 - val loss: 3.0657098293304443 - train loss: 11.149752974510193\n",
      "cnt: 11 - val loss: 3.1351709365844727 - train loss: 11.154540836811066\n",
      "cnt: 12 - val loss: 3.210294157266617 - train loss: 11.157543540000916\n",
      "cnt: 13 - val loss: 3.236966133117676 - train loss: 11.129232496023178\n",
      "cnt: 14 - val loss: 3.1744286119937897 - train loss: 11.140418916940689\n",
      "cnt: 15 - val loss: 3.1019595563411713 - train loss: 11.14420199394226\n",
      "cnt: 16 - val loss: 3.1594491004943848 - train loss: 11.128397822380066\n",
      "cnt: 17 - val loss: 3.1627379655838013 - train loss: 11.131660252809525\n",
      "cnt: 18 - val loss: 3.1264579594135284 - train loss: 11.132276058197021\n",
      "cnt: 19 - val loss: 3.1115018725395203 - train loss: 11.137936234474182\n",
      "cnt: 20 - val loss: 3.0680119693279266 - train loss: 11.115541130304337\n",
      "cnt: 21 - val loss: 3.12594798207283 - train loss: 11.118118792772293\n",
      "cnt: 22 - val loss: 3.058242917060852 - train loss: 11.120363026857376\n",
      "cnt: 0 - val loss: 3.0735883116722107 - train loss: 11.106977373361588\n",
      "cnt: 1 - val loss: 3.1501815021038055 - train loss: 11.110858172178268\n",
      "cnt: 2 - val loss: 3.2022253572940826 - train loss: 11.095572501420975\n",
      "cnt: 3 - val loss: 3.1947878897190094 - train loss: 11.094136476516724\n",
      "cnt: 4 - val loss: 3.2011370062828064 - train loss: 11.109031349420547\n",
      "cnt: 5 - val loss: 3.1230501532554626 - train loss: 11.08836880326271\n",
      "cnt: 6 - val loss: 3.0935606956481934 - train loss: 11.090646415948868\n",
      "cnt: 7 - val loss: 3.072717100381851 - train loss: 11.090690284967422\n",
      "cnt: 8 - val loss: 3.125101774930954 - train loss: 11.070796102285385\n",
      "cnt: 9 - val loss: 3.178582727909088 - train loss: 11.084790796041489\n",
      "cnt: 10 - val loss: 3.021897777915001 - train loss: 11.093133479356766\n",
      "cnt: 0 - val loss: 3.1326633989810944 - train loss: 11.084295153617859\n",
      "cnt: 1 - val loss: 3.1054419577121735 - train loss: 11.064661145210266\n",
      "cnt: 2 - val loss: 3.0500427186489105 - train loss: 11.058879792690277\n",
      "cnt: 3 - val loss: 3.0767260789871216 - train loss: 11.0600206553936\n",
      "cnt: 4 - val loss: 3.188474088907242 - train loss: 11.052105903625488\n",
      "cnt: 5 - val loss: 3.117889881134033 - train loss: 11.053402781486511\n",
      "cnt: 6 - val loss: 3.193543016910553 - train loss: 11.043854147195816\n",
      "cnt: 7 - val loss: 3.069402515888214 - train loss: 11.041595935821533\n",
      "cnt: 8 - val loss: 3.0326100885868073 - train loss: 11.053863286972046\n",
      "cnt: 9 - val loss: 3.138650506734848 - train loss: 11.048227459192276\n",
      "cnt: 10 - val loss: 3.0528756082057953 - train loss: 11.039499014616013\n",
      "cnt: 11 - val loss: 3.0554299652576447 - train loss: 11.03834879398346\n",
      "cnt: 12 - val loss: 3.072608232498169 - train loss: 11.031800210475922\n",
      "cnt: 13 - val loss: 3.0904254615306854 - train loss: 11.03094658255577\n",
      "cnt: 14 - val loss: 3.062774956226349 - train loss: 11.033999800682068\n",
      "cnt: 15 - val loss: 3.0994672775268555 - train loss: 11.021892338991165\n",
      "cnt: 16 - val loss: 3.036433756351471 - train loss: 11.025144666433334\n",
      "cnt: 17 - val loss: 3.107755810022354 - train loss: 11.019199430942535\n",
      "cnt: 18 - val loss: 3.042923152446747 - train loss: 11.01756927371025\n",
      "cnt: 19 - val loss: 3.0827674567699432 - train loss: 10.992232412099838\n",
      "cnt: 20 - val loss: 3.0485385954380035 - train loss: 11.014972001314163\n",
      "cnt: 21 - val loss: 3.177517533302307 - train loss: 11.00755625963211\n",
      "cnt: 22 - val loss: 3.1592275500297546 - train loss: 10.997539550065994\n",
      "cnt: 23 - val loss: 3.1607118248939514 - train loss: 10.9941446185112\n",
      "cnt: 24 - val loss: 3.1710808277130127 - train loss: 10.997208416461945\n",
      "cnt: 25 - val loss: 3.067553013563156 - train loss: 10.990964859724045\n",
      "cnt: 26 - val loss: 3.0504042208194733 - train loss: 10.986173778772354\n",
      "cnt: 27 - val loss: 3.0840974748134613 - train loss: 10.993572056293488\n",
      "cnt: 28 - val loss: 3.0960083305835724 - train loss: 10.983403831720352\n",
      "cnt: 29 - val loss: 3.0810810327529907 - train loss: 10.979171097278595\n",
      "cnt: 30 - val loss: 3.0713520646095276 - train loss: 10.973855793476105\n",
      "cnt: 31 - val loss: 3.1902425289154053 - train loss: 10.973743945360184\n",
      "cnt: 32 - val loss: 3.137595474720001 - train loss: 10.96978223323822\n",
      "cnt: 33 - val loss: 3.1029850244522095 - train loss: 10.96520009636879\n",
      "cnt: 34 - val loss: 3.0560407638549805 - train loss: 10.95819154381752\n",
      "cnt: 35 - val loss: 3.096621662378311 - train loss: 10.959681004285812\n",
      "cnt: 36 - val loss: 3.097137451171875 - train loss: 10.959386050701141\n",
      "cnt: 37 - val loss: 3.106524020433426 - train loss: 10.94815531373024\n",
      "cnt: 38 - val loss: 3.0860678255558014 - train loss: 10.943765103816986\n",
      "cnt: 39 - val loss: 3.0842211842536926 - train loss: 10.94877901673317\n",
      "cnt: 40 - val loss: 3.0092445611953735 - train loss: 10.942348599433899\n",
      "cnt: 0 - val loss: 3.0867931246757507 - train loss: 10.93962687253952\n",
      "cnt: 1 - val loss: 3.059581607580185 - train loss: 10.941304385662079\n",
      "cnt: 2 - val loss: 3.024330824613571 - train loss: 10.9439916908741\n",
      "cnt: 3 - val loss: 3.156204044818878 - train loss: 10.932092607021332\n",
      "cnt: 4 - val loss: 3.020558178424835 - train loss: 10.927646219730377\n",
      "cnt: 5 - val loss: 3.109594017267227 - train loss: 10.927538365125656\n",
      "cnt: 6 - val loss: 3.072796791791916 - train loss: 10.92922392487526\n",
      "cnt: 7 - val loss: 3.0007046163082123 - train loss: 10.918539553880692\n",
      "cnt: 0 - val loss: 3.076501786708832 - train loss: 10.908637911081314\n",
      "cnt: 1 - val loss: 3.0547296702861786 - train loss: 10.920620858669281\n",
      "cnt: 2 - val loss: 3.0100134313106537 - train loss: 10.900862574577332\n",
      "cnt: 3 - val loss: 3.118593454360962 - train loss: 10.906235098838806\n",
      "cnt: 4 - val loss: 2.9992836713790894 - train loss: 10.90471002459526\n",
      "cnt: 0 - val loss: 3.0034984946250916 - train loss: 10.917105346918106\n",
      "cnt: 1 - val loss: 3.073975533246994 - train loss: 10.892802834510803\n",
      "cnt: 2 - val loss: 3.0883491337299347 - train loss: 10.887921631336212\n",
      "cnt: 3 - val loss: 3.0301135778427124 - train loss: 10.898131757974625\n",
      "cnt: 4 - val loss: 2.999686121940613 - train loss: 10.878501147031784\n",
      "cnt: 5 - val loss: 3.0716505646705627 - train loss: 10.887175858020782\n",
      "cnt: 6 - val loss: 3.0551370680332184 - train loss: 10.882496118545532\n",
      "cnt: 7 - val loss: 3.035478562116623 - train loss: 10.879382967948914\n",
      "cnt: 8 - val loss: 3.061537206172943 - train loss: 10.879693686962128\n",
      "cnt: 9 - val loss: 3.0387735962867737 - train loss: 10.871804147958755\n",
      "cnt: 10 - val loss: 3.058607041835785 - train loss: 10.88460060954094\n",
      "cnt: 11 - val loss: 2.9831059277057648 - train loss: 10.862690150737762\n",
      "cnt: 0 - val loss: 3.0517253279685974 - train loss: 10.853371530771255\n",
      "cnt: 1 - val loss: 3.0484410226345062 - train loss: 10.869710832834244\n",
      "cnt: 2 - val loss: 2.989449143409729 - train loss: 10.860571891069412\n",
      "cnt: 3 - val loss: 3.1661547422409058 - train loss: 10.853693157434464\n",
      "cnt: 4 - val loss: 3.081722140312195 - train loss: 10.847121596336365\n",
      "cnt: 5 - val loss: 3.0447876155376434 - train loss: 10.8491570353508\n",
      "cnt: 6 - val loss: 2.959505245089531 - train loss: 10.849784880876541\n",
      "cnt: 0 - val loss: 3.021828830242157 - train loss: 10.841264069080353\n",
      "cnt: 1 - val loss: 3.0515066385269165 - train loss: 10.831519782543182\n",
      "cnt: 2 - val loss: 3.0098917484283447 - train loss: 10.84042739868164\n",
      "cnt: 3 - val loss: 2.9764431565999985 - train loss: 10.836695730686188\n",
      "cnt: 4 - val loss: 3.0222777128219604 - train loss: 10.834538072347641\n",
      "cnt: 5 - val loss: 3.0212210714817047 - train loss: 10.839526146650314\n",
      "cnt: 6 - val loss: 3.042125314474106 - train loss: 10.826857328414917\n",
      "cnt: 7 - val loss: 3.051755875349045 - train loss: 10.827534347772598\n",
      "cnt: 8 - val loss: 3.0346195697784424 - train loss: 10.802040576934814\n",
      "cnt: 9 - val loss: 3.067881256341934 - train loss: 10.815107345581055\n",
      "cnt: 10 - val loss: 3.019583225250244 - train loss: 10.816727668046951\n",
      "cnt: 11 - val loss: 2.9934606850147247 - train loss: 10.80290362238884\n",
      "cnt: 12 - val loss: 3.063455194234848 - train loss: 10.808128356933594\n",
      "cnt: 13 - val loss: 3.0828517377376556 - train loss: 10.801788926124573\n",
      "cnt: 14 - val loss: 3.019354462623596 - train loss: 10.805712819099426\n",
      "cnt: 15 - val loss: 3.112467736005783 - train loss: 10.812347382307053\n",
      "cnt: 16 - val loss: 3.181021362543106 - train loss: 10.801784694194794\n",
      "cnt: 17 - val loss: 2.9353959411382675 - train loss: 10.80236992239952\n",
      "cnt: 0 - val loss: 3.042739987373352 - train loss: 10.792082756757736\n",
      "cnt: 1 - val loss: 3.0327431559562683 - train loss: 10.791833221912384\n",
      "cnt: 2 - val loss: 3.054886132478714 - train loss: 10.784186154603958\n",
      "cnt: 3 - val loss: 3.0368596017360687 - train loss: 10.779913783073425\n",
      "cnt: 4 - val loss: 3.0197151601314545 - train loss: 10.776773661375046\n",
      "cnt: 5 - val loss: 2.9820873141288757 - train loss: 10.77894076704979\n",
      "cnt: 6 - val loss: 3.0412522554397583 - train loss: 10.763843208551407\n",
      "cnt: 7 - val loss: 2.9699994325637817 - train loss: 10.766130983829498\n",
      "cnt: 8 - val loss: 3.078303337097168 - train loss: 10.756671577692032\n",
      "cnt: 9 - val loss: 3.0147833228111267 - train loss: 10.772239983081818\n",
      "cnt: 10 - val loss: 3.00798499584198 - train loss: 10.754371494054794\n",
      "cnt: 11 - val loss: 2.9550438225269318 - train loss: 10.761834621429443\n",
      "cnt: 12 - val loss: 3.055313378572464 - train loss: 10.766165375709534\n",
      "cnt: 13 - val loss: 3.1125870048999786 - train loss: 10.749320477247238\n",
      "cnt: 14 - val loss: 3.064526319503784 - train loss: 10.75269240140915\n",
      "cnt: 15 - val loss: 3.0239910185337067 - train loss: 10.750320792198181\n",
      "cnt: 16 - val loss: 2.966879963874817 - train loss: 10.73403924703598\n",
      "cnt: 17 - val loss: 3.109857887029648 - train loss: 10.7463698387146\n",
      "cnt: 18 - val loss: 3.055572360754013 - train loss: 10.737526714801788\n",
      "cnt: 19 - val loss: 2.9838830530643463 - train loss: 10.729957163333893\n",
      "cnt: 20 - val loss: 3.0849481523036957 - train loss: 10.738496750593185\n",
      "cnt: 21 - val loss: 3.071504682302475 - train loss: 10.745486617088318\n",
      "cnt: 22 - val loss: 3.1168760657310486 - train loss: 10.7213733792305\n",
      "cnt: 23 - val loss: 3.022966206073761 - train loss: 10.718807429075241\n",
      "cnt: 24 - val loss: 3.056339830160141 - train loss: 10.723651349544525\n",
      "cnt: 25 - val loss: 3.0483082830905914 - train loss: 10.724991619586945\n",
      "cnt: 26 - val loss: 2.963960587978363 - train loss: 10.709167420864105\n",
      "cnt: 27 - val loss: 3.0674629509449005 - train loss: 10.704084575176239\n",
      "cnt: 28 - val loss: 3.016237646341324 - train loss: 10.701754033565521\n",
      "cnt: 29 - val loss: 3.1445886194705963 - train loss: 10.70734018087387\n",
      "cnt: 30 - val loss: 3.048265427350998 - train loss: 10.702445685863495\n",
      "cnt: 31 - val loss: 2.9957302808761597 - train loss: 10.703933209180832\n",
      "cnt: 32 - val loss: 3.0566110014915466 - train loss: 10.703700631856918\n",
      "cnt: 33 - val loss: 2.923248127102852 - train loss: 10.689875185489655\n",
      "cnt: 0 - val loss: 3.099067658185959 - train loss: 10.700950890779495\n",
      "cnt: 1 - val loss: 3.013910472393036 - train loss: 10.685129076242447\n",
      "cnt: 2 - val loss: 2.994181305170059 - train loss: 10.69487327337265\n",
      "cnt: 3 - val loss: 3.0566715598106384 - train loss: 10.676451176404953\n",
      "cnt: 4 - val loss: 3.130660116672516 - train loss: 10.67928346991539\n",
      "cnt: 5 - val loss: 2.998055726289749 - train loss: 10.675699472427368\n",
      "cnt: 6 - val loss: 2.980370968580246 - train loss: 10.681479096412659\n",
      "cnt: 7 - val loss: 2.980092763900757 - train loss: 10.67397752404213\n",
      "cnt: 8 - val loss: 3.014804929494858 - train loss: 10.66792780160904\n",
      "cnt: 9 - val loss: 3.030025750398636 - train loss: 10.66719326376915\n",
      "cnt: 10 - val loss: 2.939000979065895 - train loss: 10.674135610461235\n",
      "cnt: 11 - val loss: 3.0337660908699036 - train loss: 10.657894492149353\n",
      "cnt: 12 - val loss: 3.0150749385356903 - train loss: 10.665107399225235\n",
      "cnt: 13 - val loss: 3.0565792322158813 - train loss: 10.669704496860504\n",
      "cnt: 14 - val loss: 2.9234986007213593 - train loss: 10.642771363258362\n",
      "cnt: 15 - val loss: 2.9698027968406677 - train loss: 10.645009845495224\n",
      "cnt: 16 - val loss: 2.995080828666687 - train loss: 10.63793233036995\n",
      "cnt: 17 - val loss: 2.9748300313949585 - train loss: 10.644166588783264\n",
      "cnt: 18 - val loss: 3.0037594735622406 - train loss: 10.638208329677582\n",
      "cnt: 19 - val loss: 2.956233710050583 - train loss: 10.638207703828812\n",
      "cnt: 20 - val loss: 3.0046262443065643 - train loss: 10.63731062412262\n",
      "cnt: 21 - val loss: 2.9783377647399902 - train loss: 10.634256392717361\n",
      "cnt: 22 - val loss: 3.05334535241127 - train loss: 10.626701831817627\n",
      "cnt: 23 - val loss: 2.9890493154525757 - train loss: 10.627047926187515\n",
      "cnt: 24 - val loss: 3.0150939524173737 - train loss: 10.634758651256561\n",
      "cnt: 25 - val loss: 2.9471384584903717 - train loss: 10.628845453262329\n",
      "cnt: 26 - val loss: 2.946401357650757 - train loss: 10.615478515625\n",
      "cnt: 27 - val loss: 2.98147115111351 - train loss: 10.616045475006104\n",
      "cnt: 28 - val loss: 3.0677649080753326 - train loss: 10.610324382781982\n",
      "cnt: 29 - val loss: 3.0524213910102844 - train loss: 10.617124795913696\n",
      "cnt: 30 - val loss: 2.954905182123184 - train loss: 10.617652118206024\n",
      "cnt: 31 - val loss: 3.018706977367401 - train loss: 10.603857010602951\n",
      "cnt: 32 - val loss: 3.099402040243149 - train loss: 10.60999920964241\n",
      "cnt: 33 - val loss: 2.9940418899059296 - train loss: 10.61189353466034\n",
      "cnt: 34 - val loss: 2.9622171223163605 - train loss: 10.604039758443832\n",
      "cnt: 35 - val loss: 2.9293178021907806 - train loss: 10.603313237428665\n",
      "cnt: 36 - val loss: 2.939334958791733 - train loss: 10.597677171230316\n",
      "cnt: 37 - val loss: 2.932823121547699 - train loss: 10.584440499544144\n",
      "cnt: 38 - val loss: 2.97151517868042 - train loss: 10.58029979467392\n",
      "cnt: 39 - val loss: 3.0135836601257324 - train loss: 10.580671936273575\n",
      "cnt: 40 - val loss: 3.0096166729927063 - train loss: 10.587496995925903\n",
      "cnt: 41 - val loss: 3.0945067703723907 - train loss: 10.587802439928055\n",
      "cnt: 42 - val loss: 3.0038385689258575 - train loss: 10.567604333162308\n",
      "cnt: 43 - val loss: 2.9986647367477417 - train loss: 10.576694250106812\n",
      "cnt: 44 - val loss: 3.01924791932106 - train loss: 10.571703404188156\n",
      "cnt: 45 - val loss: 2.9471018612384796 - train loss: 10.576333820819855\n",
      "cnt: 46 - val loss: 2.9440463483333588 - train loss: 10.562231868505478\n",
      "cnt: 47 - val loss: 2.984662026166916 - train loss: 10.557182252407074\n",
      "cnt: 48 - val loss: 2.9595122039318085 - train loss: 10.572507113218307\n",
      "cnt: 49 - val loss: 3.024591475725174 - train loss: 10.563402652740479\n",
      "cnt: 50 - val loss: 2.993635982275009 - train loss: 10.562572181224823\n",
      "cnt: 51 - val loss: 3.0448959469795227 - train loss: 10.560553699731827\n",
      "cnt: 52 - val loss: 3.006226748228073 - train loss: 10.553417265415192\n",
      "cnt: 53 - val loss: 2.9462527334690094 - train loss: 10.553809523582458\n",
      "cnt: 54 - val loss: 2.95944145321846 - train loss: 10.551589518785477\n",
      "cnt: 55 - val loss: 3.0589621663093567 - train loss: 10.54684892296791\n",
      "cnt: 56 - val loss: 3.0114427506923676 - train loss: 10.541287004947662\n",
      "cnt: 57 - val loss: 3.008866399526596 - train loss: 10.532770812511444\n",
      "cnt: 58 - val loss: 3.0276776552200317 - train loss: 10.535512506961823\n",
      "cnt: 59 - val loss: 2.948452353477478 - train loss: 10.525281757116318\n",
      "cnt: 60 - val loss: 2.998196691274643 - train loss: 10.539939522743225\n",
      "cnt: 61 - val loss: 3.007155418395996 - train loss: 10.526858359575272\n",
      "cnt: 62 - val loss: 2.9414208233356476 - train loss: 10.533181309700012\n",
      "cnt: 63 - val loss: 3.038552552461624 - train loss: 10.524126783013344\n",
      "cnt: 64 - val loss: 2.960133820772171 - train loss: 10.524196654558182\n",
      "cnt: 65 - val loss: 2.9681252241134644 - train loss: 10.523009687662125\n",
      "cnt: 66 - val loss: 3.092560052871704 - train loss: 10.51977014541626\n",
      "cnt: 67 - val loss: 2.864579200744629 - train loss: 10.519166857004166\n",
      "cnt: 0 - val loss: 2.9660792350769043 - train loss: 10.510003566741943\n",
      "cnt: 1 - val loss: 3.026384860277176 - train loss: 10.510287582874298\n",
      "cnt: 2 - val loss: 2.939838618040085 - train loss: 10.507937520742416\n",
      "cnt: 3 - val loss: 2.9262704849243164 - train loss: 10.497887581586838\n",
      "cnt: 4 - val loss: 3.004404306411743 - train loss: 10.505859017372131\n",
      "cnt: 5 - val loss: 2.9013691842556 - train loss: 10.498602449893951\n",
      "cnt: 6 - val loss: 2.960668683052063 - train loss: 10.492695838212967\n",
      "cnt: 7 - val loss: 2.9429698288440704 - train loss: 10.495382696390152\n",
      "cnt: 8 - val loss: 2.971181184053421 - train loss: 10.492040544748306\n",
      "cnt: 9 - val loss: 3.0368001461029053 - train loss: 10.482254475355148\n",
      "cnt: 10 - val loss: 3.1734257638454437 - train loss: 10.477224051952362\n",
      "cnt: 11 - val loss: 2.895052194595337 - train loss: 10.497817516326904\n",
      "cnt: 12 - val loss: 2.946874052286148 - train loss: 10.480655014514923\n",
      "cnt: 13 - val loss: 3.039785087108612 - train loss: 10.477506548166275\n",
      "cnt: 14 - val loss: 2.9850941002368927 - train loss: 10.480087280273438\n",
      "cnt: 15 - val loss: 2.9511585235595703 - train loss: 10.477857112884521\n",
      "cnt: 16 - val loss: 2.951806217432022 - train loss: 10.465124249458313\n",
      "cnt: 17 - val loss: 2.9198991656303406 - train loss: 10.483666837215424\n",
      "cnt: 18 - val loss: 2.960572272539139 - train loss: 10.468070894479752\n",
      "cnt: 19 - val loss: 2.9310083091259003 - train loss: 10.461635261774063\n",
      "cnt: 20 - val loss: 2.9455119371414185 - train loss: 10.45539504289627\n",
      "cnt: 21 - val loss: 2.9975982010364532 - train loss: 10.458377569913864\n",
      "cnt: 22 - val loss: 2.9687642753124237 - train loss: 10.453771084547043\n",
      "cnt: 23 - val loss: 2.982615649700165 - train loss: 10.456605613231659\n",
      "cnt: 24 - val loss: 2.959174782037735 - train loss: 10.453092157840729\n",
      "cnt: 25 - val loss: 2.955402433872223 - train loss: 10.451047331094742\n",
      "cnt: 26 - val loss: 3.062590330839157 - train loss: 10.445370495319366\n",
      "cnt: 27 - val loss: 2.968301236629486 - train loss: 10.443197786808014\n",
      "cnt: 28 - val loss: 3.066157728433609 - train loss: 10.44944778084755\n",
      "cnt: 29 - val loss: 2.8789817839860916 - train loss: 10.443421989679337\n",
      "cnt: 30 - val loss: 2.957478314638138 - train loss: 10.431660205125809\n",
      "cnt: 31 - val loss: 2.9303425550460815 - train loss: 10.444645911455154\n",
      "cnt: 32 - val loss: 2.996539145708084 - train loss: 10.431025505065918\n",
      "cnt: 33 - val loss: 2.9109018743038177 - train loss: 10.428273886442184\n",
      "cnt: 34 - val loss: 2.9667612612247467 - train loss: 10.431416034698486\n",
      "cnt: 35 - val loss: 2.936226576566696 - train loss: 10.425308048725128\n",
      "cnt: 36 - val loss: 2.92151802778244 - train loss: 10.415593087673187\n",
      "cnt: 37 - val loss: 3.07803276181221 - train loss: 10.414087533950806\n",
      "cnt: 38 - val loss: 2.914256602525711 - train loss: 10.424657791852951\n",
      "cnt: 39 - val loss: 2.9409995675086975 - train loss: 10.400878101587296\n",
      "cnt: 40 - val loss: 2.9669006168842316 - train loss: 10.418999940156937\n",
      "cnt: 41 - val loss: 2.9352977871894836 - train loss: 10.416392236948013\n",
      "cnt: 42 - val loss: 3.047976553440094 - train loss: 10.4011589884758\n",
      "cnt: 43 - val loss: 2.948422133922577 - train loss: 10.397041380405426\n",
      "cnt: 44 - val loss: 2.980491518974304 - train loss: 10.416726529598236\n",
      "cnt: 45 - val loss: 3.00219389796257 - train loss: 10.409552901983261\n",
      "cnt: 46 - val loss: 2.994377315044403 - train loss: 10.404995799064636\n",
      "cnt: 47 - val loss: 2.9680588245391846 - train loss: 10.392807930707932\n",
      "cnt: 48 - val loss: 2.9662475287914276 - train loss: 10.39805057644844\n",
      "cnt: 49 - val loss: 3.0221076905727386 - train loss: 10.388269275426865\n",
      "cnt: 50 - val loss: 2.9136678874492645 - train loss: 10.38154149055481\n",
      "cnt: 51 - val loss: 2.9452760815620422 - train loss: 10.380032986402512\n",
      "cnt: 52 - val loss: 3.0428357124328613 - train loss: 10.392406314611435\n",
      "cnt: 53 - val loss: 2.9290254414081573 - train loss: 10.380086958408356\n",
      "cnt: 54 - val loss: 2.984618365764618 - train loss: 10.3776376247406\n",
      "cnt: 55 - val loss: 2.8907041549682617 - train loss: 10.370965629816055\n",
      "cnt: 56 - val loss: 2.931202918291092 - train loss: 10.358971387147903\n",
      "cnt: 57 - val loss: 3.0060254335403442 - train loss: 10.368740230798721\n",
      "cnt: 58 - val loss: 2.8340710401535034 - train loss: 10.36224353313446\n",
      "cnt: 0 - val loss: 2.949092835187912 - train loss: 10.355000704526901\n",
      "cnt: 1 - val loss: 2.8975932896137238 - train loss: 10.360273033380508\n",
      "cnt: 2 - val loss: 2.919184237718582 - train loss: 10.365227609872818\n",
      "cnt: 3 - val loss: 2.908996134996414 - train loss: 10.357849657535553\n",
      "cnt: 4 - val loss: 2.93349888920784 - train loss: 10.350918680429459\n",
      "cnt: 5 - val loss: 2.931407779455185 - train loss: 10.351196229457855\n",
      "cnt: 6 - val loss: 2.9926575422286987 - train loss: 10.348923444747925\n",
      "cnt: 7 - val loss: 2.8993669748306274 - train loss: 10.34768345952034\n",
      "cnt: 8 - val loss: 2.851874455809593 - train loss: 10.360359340906143\n",
      "cnt: 9 - val loss: 2.8734926879405975 - train loss: 10.34108081459999\n",
      "cnt: 10 - val loss: 2.9419172406196594 - train loss: 10.331744566559792\n",
      "cnt: 11 - val loss: 2.925328701734543 - train loss: 10.332345813512802\n",
      "cnt: 12 - val loss: 2.8890461027622223 - train loss: 10.344684362411499\n",
      "cnt: 13 - val loss: 2.904470205307007 - train loss: 10.341722965240479\n",
      "cnt: 14 - val loss: 2.918718844652176 - train loss: 10.340443328022957\n",
      "cnt: 15 - val loss: 3.0105147063732147 - train loss: 10.330969870090485\n",
      "cnt: 16 - val loss: 2.8800781965255737 - train loss: 10.336181104183197\n",
      "cnt: 17 - val loss: 2.8468824476003647 - train loss: 10.314440846443176\n",
      "cnt: 18 - val loss: 2.9385706186294556 - train loss: 10.320320546627045\n",
      "cnt: 19 - val loss: 2.997999608516693 - train loss: 10.320233911275864\n",
      "cnt: 20 - val loss: 2.8798531889915466 - train loss: 10.311634540557861\n",
      "cnt: 21 - val loss: 2.891094535589218 - train loss: 10.324109703302383\n",
      "cnt: 22 - val loss: 2.964835464954376 - train loss: 10.311429172754288\n",
      "cnt: 23 - val loss: 2.908097207546234 - train loss: 10.326917827129364\n",
      "cnt: 24 - val loss: 2.859210103750229 - train loss: 10.317856460809708\n",
      "cnt: 25 - val loss: 2.951107770204544 - train loss: 10.310972541570663\n",
      "cnt: 26 - val loss: 2.916266828775406 - train loss: 10.31241649389267\n",
      "cnt: 27 - val loss: 2.9498252272605896 - train loss: 10.301717013120651\n",
      "cnt: 28 - val loss: 2.976036459207535 - train loss: 10.290762066841125\n",
      "cnt: 29 - val loss: 2.9683705866336823 - train loss: 10.300975382328033\n",
      "cnt: 30 - val loss: 2.9128457903862 - train loss: 10.303262412548065\n",
      "cnt: 31 - val loss: 2.853066563606262 - train loss: 10.296505987644196\n",
      "cnt: 32 - val loss: 2.842903107404709 - train loss: 10.286787688732147\n",
      "cnt: 33 - val loss: 2.9016752541065216 - train loss: 10.294496729969978\n",
      "cnt: 34 - val loss: 2.9590136110782623 - train loss: 10.28420314192772\n",
      "cnt: 35 - val loss: 2.9608433842658997 - train loss: 10.293995171785355\n",
      "cnt: 36 - val loss: 2.894625574350357 - train loss: 10.292299583554268\n",
      "cnt: 37 - val loss: 2.943481743335724 - train loss: 10.284331858158112\n",
      "cnt: 38 - val loss: 2.930081158876419 - train loss: 10.276293337345123\n",
      "cnt: 39 - val loss: 2.879882365465164 - train loss: 10.268950164318085\n",
      "cnt: 40 - val loss: 2.8236450254917145 - train loss: 10.2679483294487\n",
      "cnt: 0 - val loss: 2.9546452164649963 - train loss: 10.281315416097641\n",
      "cnt: 1 - val loss: 2.895282208919525 - train loss: 10.262941390275955\n",
      "cnt: 2 - val loss: 3.0029868483543396 - train loss: 10.264239624142647\n",
      "cnt: 3 - val loss: 2.844281315803528 - train loss: 10.26597511768341\n",
      "cnt: 4 - val loss: 2.9303424954414368 - train loss: 10.264841616153717\n",
      "cnt: 5 - val loss: 2.94373682141304 - train loss: 10.264853090047836\n",
      "cnt: 6 - val loss: 2.8898955583572388 - train loss: 10.255402848124504\n",
      "cnt: 7 - val loss: 2.9430728405714035 - train loss: 10.249828070402145\n",
      "cnt: 8 - val loss: 3.007598966360092 - train loss: 10.25847977399826\n",
      "cnt: 9 - val loss: 2.8524321913719177 - train loss: 10.252247467637062\n",
      "cnt: 10 - val loss: 2.9745894968509674 - train loss: 10.254083663225174\n",
      "cnt: 11 - val loss: 2.9009591042995453 - train loss: 10.253233760595322\n",
      "cnt: 12 - val loss: 2.909929573535919 - train loss: 10.24799594283104\n",
      "cnt: 13 - val loss: 2.917766273021698 - train loss: 10.23069652915001\n",
      "cnt: 14 - val loss: 2.947694569826126 - train loss: 10.229788601398468\n",
      "cnt: 15 - val loss: 2.881498694419861 - train loss: 10.234613537788391\n",
      "cnt: 16 - val loss: 2.928889214992523 - train loss: 10.236947894096375\n",
      "cnt: 17 - val loss: 2.8840931951999664 - train loss: 10.236940562725067\n",
      "cnt: 18 - val loss: 2.9398009181022644 - train loss: 10.232824176549911\n",
      "cnt: 19 - val loss: 2.922213703393936 - train loss: 10.235871851444244\n",
      "cnt: 20 - val loss: 3.0125500857830048 - train loss: 10.230091094970703\n",
      "cnt: 21 - val loss: 2.9121379256248474 - train loss: 10.226351410150528\n",
      "cnt: 22 - val loss: 2.8811227679252625 - train loss: 10.225376278162003\n",
      "cnt: 23 - val loss: 2.9318370819091797 - train loss: 10.21942275762558\n",
      "cnt: 24 - val loss: 2.84046933054924 - train loss: 10.22656935453415\n",
      "cnt: 25 - val loss: 2.877676874399185 - train loss: 10.218117713928223\n",
      "cnt: 26 - val loss: 2.8622022569179535 - train loss: 10.215355008840561\n",
      "cnt: 27 - val loss: 2.96155321598053 - train loss: 10.21819692850113\n",
      "cnt: 28 - val loss: 3.0089045464992523 - train loss: 10.205455332994461\n",
      "cnt: 29 - val loss: 2.953648418188095 - train loss: 10.22593742609024\n",
      "cnt: 30 - val loss: 2.932990610599518 - train loss: 10.203643381595612\n",
      "cnt: 31 - val loss: 2.951034128665924 - train loss: 10.205678850412369\n",
      "cnt: 32 - val loss: 2.953325003385544 - train loss: 10.202911496162415\n",
      "cnt: 33 - val loss: 2.9337566792964935 - train loss: 10.196183115243912\n",
      "cnt: 34 - val loss: 2.885019063949585 - train loss: 10.197291254997253\n",
      "cnt: 35 - val loss: 2.866361916065216 - train loss: 10.198704302310944\n",
      "cnt: 36 - val loss: 2.89313668012619 - train loss: 10.190205380320549\n",
      "cnt: 37 - val loss: 2.92288339138031 - train loss: 10.18280103802681\n",
      "cnt: 38 - val loss: 2.9017073214054108 - train loss: 10.185390919446945\n",
      "cnt: 39 - val loss: 2.8589829802513123 - train loss: 10.179638147354126\n",
      "cnt: 40 - val loss: 2.845109850168228 - train loss: 10.178800463676453\n",
      "cnt: 41 - val loss: 2.862363785505295 - train loss: 10.185298204421997\n",
      "cnt: 42 - val loss: 3.0392249822616577 - train loss: 10.170836389064789\n",
      "cnt: 43 - val loss: 2.976200431585312 - train loss: 10.178308725357056\n",
      "cnt: 44 - val loss: 2.9068902730941772 - train loss: 10.172747984528542\n",
      "cnt: 45 - val loss: 2.845095217227936 - train loss: 10.171950310468674\n",
      "cnt: 46 - val loss: 2.8458905816078186 - train loss: 10.173311352729797\n",
      "cnt: 47 - val loss: 2.8626633286476135 - train loss: 10.162782549858093\n",
      "cnt: 48 - val loss: 2.8881693482398987 - train loss: 10.168300330638885\n",
      "cnt: 49 - val loss: 2.8007294088602066 - train loss: 10.15788248181343\n",
      "cnt: 0 - val loss: 2.8773000240325928 - train loss: 10.17372253537178\n",
      "cnt: 1 - val loss: 2.9884746968746185 - train loss: 10.158563062548637\n",
      "cnt: 2 - val loss: 2.849262923002243 - train loss: 10.16226014494896\n",
      "cnt: 3 - val loss: 2.9700961112976074 - train loss: 10.154390841722488\n",
      "cnt: 4 - val loss: 2.8079884946346283 - train loss: 10.153155535459518\n",
      "cnt: 5 - val loss: 2.8673857748508453 - train loss: 10.145289450883865\n",
      "cnt: 6 - val loss: 2.9395469427108765 - train loss: 10.159324705600739\n",
      "cnt: 7 - val loss: 2.9345144033432007 - train loss: 10.14926302433014\n",
      "cnt: 8 - val loss: 2.890771836042404 - train loss: 10.152317851781845\n",
      "cnt: 9 - val loss: 2.985711991786957 - train loss: 10.14165285229683\n",
      "cnt: 10 - val loss: 2.84381639957428 - train loss: 10.14780929684639\n",
      "cnt: 11 - val loss: 2.7735172659158707 - train loss: 10.138037323951721\n",
      "cnt: 0 - val loss: 2.8431100249290466 - train loss: 10.13125404715538\n",
      "cnt: 1 - val loss: 2.9433436393737793 - train loss: 10.144280791282654\n",
      "cnt: 2 - val loss: 2.8417130410671234 - train loss: 10.129759848117828\n",
      "cnt: 3 - val loss: 2.9157299399375916 - train loss: 10.137089133262634\n",
      "cnt: 4 - val loss: 2.857627749443054 - train loss: 10.129088282585144\n",
      "cnt: 5 - val loss: 2.852939009666443 - train loss: 10.116854965686798\n",
      "cnt: 6 - val loss: 2.874941259622574 - train loss: 10.122820883989334\n",
      "cnt: 7 - val loss: 2.8775302469730377 - train loss: 10.123488634824753\n",
      "cnt: 8 - val loss: 2.86700639128685 - train loss: 10.117579072713852\n",
      "cnt: 9 - val loss: 2.8103834092617035 - train loss: 10.118045270442963\n",
      "cnt: 10 - val loss: 2.7991220206022263 - train loss: 10.120600134134293\n",
      "cnt: 11 - val loss: 2.918583482503891 - train loss: 10.11274853348732\n",
      "cnt: 12 - val loss: 2.8412870168685913 - train loss: 10.121869087219238\n",
      "cnt: 13 - val loss: 2.916631758213043 - train loss: 10.105585604906082\n",
      "cnt: 14 - val loss: 2.897696405649185 - train loss: 10.110159039497375\n",
      "cnt: 15 - val loss: 2.9334851801395416 - train loss: 10.106300085783005\n",
      "cnt: 16 - val loss: 2.857964873313904 - train loss: 10.108193695545197\n",
      "cnt: 17 - val loss: 2.9223718345165253 - train loss: 10.096902281045914\n",
      "cnt: 18 - val loss: 2.9578366577625275 - train loss: 10.094554096460342\n",
      "cnt: 19 - val loss: 2.843811273574829 - train loss: 10.089990615844727\n",
      "cnt: 20 - val loss: 2.8260154128074646 - train loss: 10.092292696237564\n",
      "cnt: 21 - val loss: 2.796189606189728 - train loss: 10.09439954161644\n",
      "cnt: 22 - val loss: 2.880077213048935 - train loss: 10.099481731653214\n",
      "cnt: 23 - val loss: 2.8262395560741425 - train loss: 10.088437706232071\n",
      "cnt: 24 - val loss: 2.834936708211899 - train loss: 10.080356270074844\n",
      "cnt: 25 - val loss: 2.753377005457878 - train loss: 10.088545382022858\n",
      "cnt: 0 - val loss: 2.9495464861392975 - train loss: 10.08478981256485\n",
      "cnt: 1 - val loss: 2.8536187410354614 - train loss: 10.082471385598183\n",
      "cnt: 2 - val loss: 2.7573027908802032 - train loss: 10.07870501279831\n",
      "cnt: 3 - val loss: 2.884217292070389 - train loss: 10.071188509464264\n",
      "cnt: 4 - val loss: 2.8611161708831787 - train loss: 10.069875255227089\n",
      "cnt: 5 - val loss: 2.9302847385406494 - train loss: 10.067031860351562\n",
      "cnt: 6 - val loss: 2.8658887147903442 - train loss: 10.073290258646011\n",
      "cnt: 7 - val loss: 2.81348380446434 - train loss: 10.068958312273026\n",
      "cnt: 8 - val loss: 2.8969503045082092 - train loss: 10.065393835306168\n",
      "cnt: 9 - val loss: 2.8923911452293396 - train loss: 10.064494848251343\n",
      "cnt: 10 - val loss: 2.8667084872722626 - train loss: 10.071543276309967\n",
      "cnt: 11 - val loss: 2.8452295064926147 - train loss: 10.050460532307625\n",
      "cnt: 12 - val loss: 2.825156569480896 - train loss: 10.055820912122726\n",
      "cnt: 13 - val loss: 2.7230038791894913 - train loss: 10.058169916272163\n",
      "cnt: 0 - val loss: 2.911810725927353 - train loss: 10.053093075752258\n",
      "cnt: 1 - val loss: 2.780887320637703 - train loss: 10.048679381608963\n",
      "cnt: 2 - val loss: 2.796595722436905 - train loss: 10.056916981935501\n",
      "cnt: 3 - val loss: 2.795157343149185 - train loss: 10.046777099370956\n",
      "cnt: 4 - val loss: 2.8738502264022827 - train loss: 10.03679122030735\n",
      "cnt: 5 - val loss: 2.811671555042267 - train loss: 10.043083846569061\n",
      "cnt: 6 - val loss: 2.8502632677555084 - train loss: 10.045644775032997\n",
      "cnt: 7 - val loss: 2.905167132616043 - train loss: 10.041259407997131\n",
      "cnt: 8 - val loss: 2.771091878414154 - train loss: 10.046691685914993\n",
      "cnt: 9 - val loss: 2.834272086620331 - train loss: 10.036007344722748\n",
      "cnt: 10 - val loss: 2.779642418026924 - train loss: 10.026866286993027\n",
      "cnt: 11 - val loss: 2.8410451114177704 - train loss: 10.03107276558876\n",
      "cnt: 12 - val loss: 2.860919862985611 - train loss: 10.027628719806671\n",
      "cnt: 13 - val loss: 2.7873972356319427 - train loss: 10.029464185237885\n",
      "cnt: 14 - val loss: 2.9296033680438995 - train loss: 10.02182149887085\n",
      "cnt: 15 - val loss: 2.7865104973316193 - train loss: 10.024011313915253\n",
      "cnt: 16 - val loss: 2.8688206374645233 - train loss: 10.028442457318306\n",
      "cnt: 17 - val loss: 2.7631252110004425 - train loss: 10.016580164432526\n",
      "cnt: 18 - val loss: 2.788637936115265 - train loss: 10.01951901614666\n",
      "cnt: 19 - val loss: 2.781577095389366 - train loss: 10.007657051086426\n",
      "cnt: 20 - val loss: 2.9295572340488434 - train loss: 10.018943220376968\n",
      "cnt: 21 - val loss: 2.8835699260234833 - train loss: 10.017610102891922\n",
      "cnt: 22 - val loss: 2.762706905603409 - train loss: 10.006352722644806\n",
      "cnt: 23 - val loss: 2.8263326287269592 - train loss: 10.013180166482925\n",
      "cnt: 24 - val loss: 2.955886036157608 - train loss: 10.002381712198257\n",
      "cnt: 25 - val loss: 2.7408462166786194 - train loss: 10.014002799987793\n",
      "cnt: 26 - val loss: 2.827934741973877 - train loss: 10.008962780237198\n",
      "cnt: 27 - val loss: 2.866019695997238 - train loss: 10.002632588148117\n",
      "cnt: 28 - val loss: 2.842596411705017 - train loss: 9.99325442314148\n",
      "cnt: 29 - val loss: 2.801079958677292 - train loss: 10.0036089271307\n",
      "cnt: 30 - val loss: 2.8098151981830597 - train loss: 9.993059158325195\n",
      "cnt: 31 - val loss: 2.8080914914608 - train loss: 9.993649616837502\n",
      "cnt: 32 - val loss: 2.7807907462120056 - train loss: 9.999236851930618\n",
      "cnt: 33 - val loss: 2.7885244488716125 - train loss: 9.984727635979652\n",
      "cnt: 34 - val loss: 2.7763023376464844 - train loss: 9.988756269216537\n",
      "cnt: 35 - val loss: 2.820116877555847 - train loss: 9.98839783668518\n",
      "cnt: 36 - val loss: 2.812272548675537 - train loss: 9.989075630903244\n",
      "cnt: 37 - val loss: 2.8439426720142365 - train loss: 9.9815014898777\n",
      "cnt: 38 - val loss: 2.8478668928146362 - train loss: 9.984923154115677\n",
      "cnt: 39 - val loss: 2.888375759124756 - train loss: 9.974150344729424\n",
      "cnt: 40 - val loss: 2.770075261592865 - train loss: 9.973418980836868\n",
      "cnt: 41 - val loss: 2.8464521020650864 - train loss: 9.968212515115738\n",
      "cnt: 42 - val loss: 2.806130677461624 - train loss: 9.972962081432343\n",
      "cnt: 43 - val loss: 2.7667640447616577 - train loss: 9.975243270397186\n",
      "cnt: 44 - val loss: 2.788705736398697 - train loss: 9.956495255231857\n",
      "cnt: 45 - val loss: 2.804047554731369 - train loss: 9.965132027864456\n",
      "cnt: 46 - val loss: 2.7599280774593353 - train loss: 9.964588314294815\n",
      "cnt: 47 - val loss: 2.8365285396575928 - train loss: 9.955572545528412\n",
      "cnt: 48 - val loss: 2.7670020163059235 - train loss: 9.955528050661087\n",
      "cnt: 49 - val loss: 2.8231568932533264 - train loss: 9.9644895195961\n",
      "cnt: 50 - val loss: 2.891951620578766 - train loss: 9.95432224869728\n",
      "cnt: 51 - val loss: 2.8186548352241516 - train loss: 9.956913113594055\n",
      "cnt: 52 - val loss: 2.789191573858261 - train loss: 9.94718550145626\n",
      "cnt: 53 - val loss: 2.801449954509735 - train loss: 9.94716140627861\n",
      "cnt: 54 - val loss: 2.8213841915130615 - train loss: 9.944308787584305\n",
      "cnt: 55 - val loss: 2.781098961830139 - train loss: 9.943215727806091\n",
      "cnt: 56 - val loss: 2.818659633398056 - train loss: 9.944760531187057\n",
      "cnt: 57 - val loss: 2.74919755756855 - train loss: 9.935147762298584\n",
      "cnt: 58 - val loss: 2.816126763820648 - train loss: 9.942314594984055\n",
      "cnt: 59 - val loss: 2.8053081333637238 - train loss: 9.943987026810646\n",
      "cnt: 60 - val loss: 2.9120306074619293 - train loss: 9.93670517206192\n",
      "cnt: 61 - val loss: 2.7363542169332504 - train loss: 9.93811647593975\n",
      "cnt: 62 - val loss: 2.78160697221756 - train loss: 9.934525817632675\n",
      "cnt: 63 - val loss: 2.8519256114959717 - train loss: 9.928910464048386\n",
      "cnt: 64 - val loss: 2.7673316597938538 - train loss: 9.935290485620499\n",
      "cnt: 65 - val loss: 2.926645129919052 - train loss: 9.919313445687294\n",
      "cnt: 66 - val loss: 2.7746109068393707 - train loss: 9.92441189289093\n",
      "cnt: 67 - val loss: 2.8196451365947723 - train loss: 9.923316523432732\n",
      "cnt: 68 - val loss: 2.7450961470603943 - train loss: 9.924628615379333\n",
      "cnt: 69 - val loss: 2.8377862572669983 - train loss: 9.931798100471497\n",
      "cnt: 70 - val loss: 2.754259392619133 - train loss: 9.91836079955101\n",
      "cnt: 71 - val loss: 2.7838975191116333 - train loss: 9.914041757583618\n",
      "cnt: 72 - val loss: 2.77476766705513 - train loss: 9.916356235742569\n",
      "cnt: 73 - val loss: 2.791102558374405 - train loss: 9.909483700990677\n",
      "cnt: 74 - val loss: 2.806852549314499 - train loss: 9.916974976658821\n",
      "cnt: 75 - val loss: 2.821777045726776 - train loss: 9.900421798229218\n",
      "cnt: 76 - val loss: 2.727455362677574 - train loss: 9.902091413736343\n",
      "cnt: 77 - val loss: 2.825535625219345 - train loss: 9.909837394952774\n",
      "cnt: 78 - val loss: 2.7554304897785187 - train loss: 9.89775064587593\n",
      "cnt: 79 - val loss: 2.806751936674118 - train loss: 9.91390797495842\n",
      "cnt: 80 - val loss: 2.803939700126648 - train loss: 9.892172008752823\n",
      "cnt: 81 - val loss: 2.836654543876648 - train loss: 9.905381664633751\n",
      "cnt: 82 - val loss: 2.780345171689987 - train loss: 9.894101172685623\n",
      "cnt: 83 - val loss: 2.753349542617798 - train loss: 9.891360640525818\n",
      "cnt: 84 - val loss: 2.7640657126903534 - train loss: 9.89211156964302\n",
      "cnt: 85 - val loss: 2.7635233998298645 - train loss: 9.883642822504044\n",
      "cnt: 86 - val loss: 2.8278069645166397 - train loss: 9.888394206762314\n",
      "cnt: 87 - val loss: 2.7042035907506943 - train loss: 9.872856169939041\n",
      "cnt: 0 - val loss: 2.7678778171539307 - train loss: 9.891232386231422\n",
      "cnt: 1 - val loss: 2.7536889016628265 - train loss: 9.88575792312622\n",
      "cnt: 2 - val loss: 2.7360057830810547 - train loss: 9.880997866392136\n",
      "cnt: 3 - val loss: 2.7007007598876953 - train loss: 9.88176617026329\n",
      "cnt: 0 - val loss: 2.755260854959488 - train loss: 9.890121579170227\n",
      "cnt: 1 - val loss: 2.8544537872076035 - train loss: 9.887020617723465\n",
      "cnt: 2 - val loss: 2.808041125535965 - train loss: 9.884596645832062\n",
      "cnt: 3 - val loss: 2.7830654680728912 - train loss: 9.865660697221756\n",
      "cnt: 4 - val loss: 2.8713502287864685 - train loss: 9.865436151623726\n",
      "cnt: 5 - val loss: 2.8221276700496674 - train loss: 9.859949797391891\n",
      "cnt: 6 - val loss: 2.793731540441513 - train loss: 9.860112637281418\n",
      "cnt: 7 - val loss: 2.8004772514104843 - train loss: 9.869647145271301\n",
      "cnt: 8 - val loss: 2.798611283302307 - train loss: 9.853433787822723\n",
      "cnt: 9 - val loss: 2.748582363128662 - train loss: 9.857057929039001\n",
      "cnt: 10 - val loss: 2.727151021361351 - train loss: 9.8497713804245\n",
      "cnt: 11 - val loss: 2.773423045873642 - train loss: 9.86584797501564\n",
      "cnt: 12 - val loss: 2.8976522386074066 - train loss: 9.854057282209396\n",
      "cnt: 13 - val loss: 2.7141040563583374 - train loss: 9.856297761201859\n",
      "cnt: 14 - val loss: 2.755086362361908 - train loss: 9.84694117307663\n",
      "cnt: 15 - val loss: 2.768973708152771 - train loss: 9.841963231563568\n",
      "cnt: 16 - val loss: 2.8586063385009766 - train loss: 9.848658412694931\n",
      "cnt: 17 - val loss: 2.8390356600284576 - train loss: 9.84736005961895\n",
      "cnt: 18 - val loss: 2.769366830587387 - train loss: 9.838759958744049\n",
      "cnt: 19 - val loss: 2.816301852464676 - train loss: 9.853686779737473\n",
      "cnt: 20 - val loss: 2.8909700214862823 - train loss: 9.849760681390762\n",
      "cnt: 21 - val loss: 2.8130176067352295 - train loss: 9.843572050333023\n",
      "cnt: 22 - val loss: 2.7728483080863953 - train loss: 9.832409977912903\n",
      "cnt: 23 - val loss: 2.7674106061458588 - train loss: 9.831427544355392\n",
      "cnt: 24 - val loss: 2.814530998468399 - train loss: 9.825264900922775\n",
      "cnt: 25 - val loss: 2.7841098308563232 - train loss: 9.822272777557373\n",
      "cnt: 26 - val loss: 2.6944569796323776 - train loss: 9.834177643060684\n",
      "cnt: 0 - val loss: 2.8006823658943176 - train loss: 9.817751631140709\n",
      "cnt: 1 - val loss: 2.6834431290626526 - train loss: 9.832957997918129\n",
      "cnt: 0 - val loss: 2.6897947937250137 - train loss: 9.815987139940262\n",
      "cnt: 1 - val loss: 2.7686024010181427 - train loss: 9.825529515743256\n",
      "cnt: 2 - val loss: 2.7917505502700806 - train loss: 9.821715861558914\n",
      "cnt: 3 - val loss: 2.80622136592865 - train loss: 9.807892769575119\n",
      "cnt: 4 - val loss: 2.847345530986786 - train loss: 9.816253870725632\n",
      "cnt: 5 - val loss: 2.7377140522003174 - train loss: 9.808768153190613\n",
      "cnt: 6 - val loss: 2.7934593558311462 - train loss: 9.81382830440998\n",
      "cnt: 7 - val loss: 2.7534466087818146 - train loss: 9.806656897068024\n",
      "cnt: 8 - val loss: 2.752379536628723 - train loss: 9.801808342337608\n",
      "cnt: 9 - val loss: 2.7807117104530334 - train loss: 9.806276440620422\n",
      "cnt: 10 - val loss: 2.7387315034866333 - train loss: 9.80406504869461\n",
      "cnt: 11 - val loss: 2.834106296300888 - train loss: 9.809648752212524\n",
      "cnt: 12 - val loss: 2.73273429274559 - train loss: 9.80320006608963\n",
      "cnt: 13 - val loss: 2.809939384460449 - train loss: 9.798927649855614\n",
      "cnt: 14 - val loss: 2.8234002590179443 - train loss: 9.799100488424301\n",
      "cnt: 15 - val loss: 2.7156882882118225 - train loss: 9.794447809457779\n",
      "cnt: 16 - val loss: 2.759600520133972 - train loss: 9.793371230363846\n",
      "cnt: 17 - val loss: 2.776280641555786 - train loss: 9.794311180710793\n",
      "cnt: 18 - val loss: 2.743966042995453 - train loss: 9.789666056632996\n",
      "cnt: 19 - val loss: 2.6935763508081436 - train loss: 9.790623918175697\n",
      "cnt: 20 - val loss: 2.7541262209415436 - train loss: 9.790467470884323\n",
      "cnt: 21 - val loss: 2.788053572177887 - train loss: 9.778164267539978\n",
      "cnt: 22 - val loss: 2.7589282989501953 - train loss: 9.775297909975052\n",
      "cnt: 23 - val loss: 2.7308885604143143 - train loss: 9.78007385134697\n",
      "cnt: 24 - val loss: 2.743033677339554 - train loss: 9.783004358410835\n",
      "cnt: 25 - val loss: 2.7992324829101562 - train loss: 9.773721307516098\n",
      "cnt: 26 - val loss: 2.7206738591194153 - train loss: 9.782601416110992\n",
      "cnt: 27 - val loss: 2.84117192029953 - train loss: 9.762526884675026\n",
      "cnt: 28 - val loss: 2.764954298734665 - train loss: 9.770881414413452\n",
      "cnt: 29 - val loss: 2.7263150215148926 - train loss: 9.772056102752686\n",
      "cnt: 30 - val loss: 2.8001762479543686 - train loss: 9.770942032337189\n",
      "cnt: 31 - val loss: 2.724962592124939 - train loss: 9.76256899535656\n",
      "cnt: 32 - val loss: 2.6837622821331024 - train loss: 9.765302062034607\n",
      "cnt: 33 - val loss: 2.898574948310852 - train loss: 9.764499843120575\n",
      "cnt: 34 - val loss: 2.7900920808315277 - train loss: 9.757117301225662\n",
      "cnt: 35 - val loss: 2.785492777824402 - train loss: 9.7551688849926\n",
      "cnt: 36 - val loss: 2.756449967622757 - train loss: 9.762582242488861\n",
      "cnt: 37 - val loss: 2.7946468591690063 - train loss: 9.760578781366348\n",
      "cnt: 38 - val loss: 2.8306492269039154 - train loss: 9.756465643644333\n",
      "cnt: 39 - val loss: 2.7346552908420563 - train loss: 9.757999688386917\n",
      "cnt: 40 - val loss: 2.7580739855766296 - train loss: 9.753833562135696\n",
      "cnt: 41 - val loss: 2.717112958431244 - train loss: 9.754900634288788\n",
      "cnt: 42 - val loss: 2.920387387275696 - train loss: 9.754795506596565\n",
      "cnt: 43 - val loss: 2.734007567167282 - train loss: 9.753016293048859\n",
      "cnt: 44 - val loss: 2.8500019907951355 - train loss: 9.744994580745697\n",
      "cnt: 45 - val loss: 2.7123170495033264 - train loss: 9.751276165246964\n",
      "cnt: 46 - val loss: 2.72055584192276 - train loss: 9.73287683725357\n",
      "cnt: 47 - val loss: 2.8007660806179047 - train loss: 9.757243901491165\n",
      "cnt: 48 - val loss: 2.6459159702062607 - train loss: 9.742736399173737\n",
      "cnt: 0 - val loss: 2.760603681206703 - train loss: 9.749790340662003\n",
      "cnt: 1 - val loss: 2.6881880164146423 - train loss: 9.740264594554901\n",
      "cnt: 2 - val loss: 2.7840560376644135 - train loss: 9.730875223875046\n",
      "cnt: 3 - val loss: 2.8553078174591064 - train loss: 9.724300354719162\n",
      "cnt: 4 - val loss: 2.781019538640976 - train loss: 9.733917221426964\n",
      "cnt: 5 - val loss: 2.7722375094890594 - train loss: 9.73346921801567\n",
      "cnt: 6 - val loss: 2.767219305038452 - train loss: 9.720229715108871\n",
      "cnt: 7 - val loss: 2.8384197652339935 - train loss: 9.727710723876953\n",
      "cnt: 8 - val loss: 2.7142319083213806 - train loss: 9.720291703939438\n",
      "cnt: 9 - val loss: 2.687387689948082 - train loss: 9.727600634098053\n",
      "cnt: 10 - val loss: 2.780217230319977 - train loss: 9.72059442102909\n",
      "cnt: 11 - val loss: 2.703815281391144 - train loss: 9.712649881839752\n",
      "cnt: 12 - val loss: 2.677205592393875 - train loss: 9.718092292547226\n",
      "cnt: 13 - val loss: 2.784550577402115 - train loss: 9.716249287128448\n",
      "cnt: 14 - val loss: 2.7879571318626404 - train loss: 9.711040616035461\n",
      "cnt: 15 - val loss: 2.8002750873565674 - train loss: 9.712446480989456\n",
      "cnt: 16 - val loss: 2.703367993235588 - train loss: 9.707381278276443\n",
      "cnt: 17 - val loss: 2.7116332054138184 - train loss: 9.704632461071014\n",
      "cnt: 18 - val loss: 2.7526548504829407 - train loss: 9.705880880355835\n",
      "cnt: 19 - val loss: 2.6807039231061935 - train loss: 9.717076450586319\n",
      "cnt: 20 - val loss: 2.72896409034729 - train loss: 9.69104379415512\n",
      "cnt: 21 - val loss: 2.6975812315940857 - train loss: 9.689946189522743\n",
      "cnt: 22 - val loss: 2.7148695290088654 - train loss: 9.703428149223328\n",
      "cnt: 23 - val loss: 2.7396976947784424 - train loss: 9.686491027474403\n",
      "cnt: 24 - val loss: 2.732168734073639 - train loss: 9.69128793478012\n",
      "cnt: 25 - val loss: 2.824259400367737 - train loss: 9.688496381044388\n",
      "cnt: 26 - val loss: 2.674863412976265 - train loss: 9.695598900318146\n",
      "cnt: 27 - val loss: 2.7462608218193054 - train loss: 9.70255571603775\n",
      "cnt: 28 - val loss: 2.7530823945999146 - train loss: 9.694236189126968\n",
      "cnt: 29 - val loss: 2.782960444688797 - train loss: 9.690138772130013\n",
      "cnt: 30 - val loss: 2.7503247261047363 - train loss: 9.691381201148033\n",
      "cnt: 31 - val loss: 2.7745737731456757 - train loss: 9.686993569135666\n",
      "cnt: 32 - val loss: 2.684464305639267 - train loss: 9.687590420246124\n",
      "cnt: 33 - val loss: 2.770859569311142 - train loss: 9.671598121523857\n",
      "cnt: 34 - val loss: 2.717371493577957 - train loss: 9.673863068223\n",
      "cnt: 35 - val loss: 2.771378666162491 - train loss: 9.672490984201431\n",
      "cnt: 36 - val loss: 2.7602224349975586 - train loss: 9.68073682487011\n",
      "cnt: 37 - val loss: 2.712813436985016 - train loss: 9.674732208251953\n",
      "cnt: 38 - val loss: 2.695921301841736 - train loss: 9.68096935749054\n",
      "cnt: 39 - val loss: 2.705203205347061 - train loss: 9.66126173734665\n",
      "cnt: 40 - val loss: 2.717210680246353 - train loss: 9.672740757465363\n",
      "cnt: 41 - val loss: 2.7261419594287872 - train loss: 9.666217565536499\n",
      "cnt: 42 - val loss: 2.7818698287010193 - train loss: 9.660041242837906\n",
      "cnt: 43 - val loss: 2.7397634983062744 - train loss: 9.662332355976105\n",
      "cnt: 44 - val loss: 2.7955716848373413 - train loss: 9.65974673628807\n",
      "cnt: 45 - val loss: 2.738686054944992 - train loss: 9.666741460561752\n",
      "cnt: 46 - val loss: 2.6337631344795227 - train loss: 9.651549130678177\n",
      "cnt: 0 - val loss: 2.7552387714385986 - train loss: 9.65705318748951\n",
      "cnt: 1 - val loss: 2.690737158060074 - train loss: 9.654257118701935\n",
      "cnt: 2 - val loss: 2.74373060464859 - train loss: 9.666348293423653\n",
      "cnt: 3 - val loss: 2.9937524497509003 - train loss: 9.635909274220467\n",
      "cnt: 4 - val loss: 2.831045091152191 - train loss: 9.644563123583794\n",
      "cnt: 5 - val loss: 2.831631600856781 - train loss: 9.660183161497116\n",
      "cnt: 6 - val loss: 2.704183518886566 - train loss: 9.641752988100052\n",
      "cnt: 7 - val loss: 2.6977940499782562 - train loss: 9.64320158958435\n",
      "cnt: 8 - val loss: 2.719221204519272 - train loss: 9.636360719799995\n",
      "cnt: 9 - val loss: 2.732931911945343 - train loss: 9.635122746229172\n",
      "cnt: 10 - val loss: 2.7825942933559418 - train loss: 9.630636900663376\n",
      "cnt: 11 - val loss: 2.7402595579624176 - train loss: 9.644812062382698\n",
      "cnt: 12 - val loss: 2.7348082661628723 - train loss: 9.638857528567314\n",
      "cnt: 13 - val loss: 2.692370265722275 - train loss: 9.63971021771431\n",
      "cnt: 14 - val loss: 2.731832519173622 - train loss: 9.631052270531654\n",
      "cnt: 15 - val loss: 2.79262438416481 - train loss: 9.635775908827782\n",
      "cnt: 16 - val loss: 2.7135477662086487 - train loss: 9.638163536787033\n",
      "cnt: 17 - val loss: 2.8085916340351105 - train loss: 9.633570984005928\n",
      "cnt: 18 - val loss: 2.734448105096817 - train loss: 9.62974625825882\n",
      "cnt: 19 - val loss: 2.7283690571784973 - train loss: 9.618546485900879\n",
      "cnt: 20 - val loss: 2.718945473432541 - train loss: 9.618740737438202\n",
      "cnt: 21 - val loss: 2.6956241726875305 - train loss: 9.627692341804504\n",
      "cnt: 22 - val loss: 2.721351444721222 - train loss: 9.623336613178253\n",
      "cnt: 23 - val loss: 2.6857452988624573 - train loss: 9.61429700255394\n",
      "cnt: 24 - val loss: 2.7361497282981873 - train loss: 9.636876493692398\n",
      "cnt: 25 - val loss: 2.7238161861896515 - train loss: 9.61227685213089\n",
      "cnt: 26 - val loss: 2.7812454104423523 - train loss: 9.610938340425491\n",
      "cnt: 27 - val loss: 2.8014632165431976 - train loss: 9.61645856499672\n",
      "cnt: 28 - val loss: 2.84797939658165 - train loss: 9.609217196702957\n",
      "cnt: 29 - val loss: 2.7399426698684692 - train loss: 9.607695668935776\n",
      "cnt: 30 - val loss: 2.7040029615163803 - train loss: 9.602326318621635\n",
      "cnt: 31 - val loss: 2.765789955854416 - train loss: 9.620336532592773\n",
      "cnt: 32 - val loss: 2.637632578611374 - train loss: 9.618095874786377\n",
      "cnt: 33 - val loss: 2.7717057168483734 - train loss: 9.607891991734505\n",
      "cnt: 34 - val loss: 2.683159679174423 - train loss: 9.605606332421303\n",
      "cnt: 35 - val loss: 2.7741448879241943 - train loss: 9.595856845378876\n",
      "cnt: 36 - val loss: 2.738963007926941 - train loss: 9.601630568504333\n",
      "cnt: 37 - val loss: 2.8059738874435425 - train loss: 9.597922593355179\n",
      "cnt: 38 - val loss: 2.6866556107997894 - train loss: 9.58520433306694\n",
      "cnt: 39 - val loss: 2.669817343354225 - train loss: 9.596060752868652\n",
      "cnt: 40 - val loss: 2.872875988483429 - train loss: 9.59818696975708\n",
      "cnt: 41 - val loss: 2.690813958644867 - train loss: 9.586983233690262\n",
      "cnt: 42 - val loss: 2.6861129999160767 - train loss: 9.585154250264168\n",
      "cnt: 43 - val loss: 2.7152144610881805 - train loss: 9.586465075612068\n",
      "cnt: 44 - val loss: 2.642514184117317 - train loss: 9.576916873455048\n",
      "cnt: 45 - val loss: 2.654948353767395 - train loss: 9.58463191986084\n",
      "cnt: 46 - val loss: 2.7182073295116425 - train loss: 9.581981301307678\n",
      "cnt: 47 - val loss: 2.674934431910515 - train loss: 9.581545650959015\n",
      "cnt: 48 - val loss: 2.727800190448761 - train loss: 9.570315435528755\n",
      "cnt: 49 - val loss: 2.731817662715912 - train loss: 9.577859982848167\n",
      "cnt: 50 - val loss: 2.6863448917865753 - train loss: 9.58325082063675\n",
      "cnt: 51 - val loss: 2.7341213822364807 - train loss: 9.582046300172806\n",
      "cnt: 52 - val loss: 2.6131284832954407 - train loss: 9.572222575545311\n",
      "cnt: 0 - val loss: 2.6686335504055023 - train loss: 9.575597390532494\n",
      "cnt: 1 - val loss: 2.6917058527469635 - train loss: 9.572382092475891\n",
      "cnt: 2 - val loss: 2.7635318636894226 - train loss: 9.569673851132393\n",
      "cnt: 3 - val loss: 2.795421540737152 - train loss: 9.560903176665306\n",
      "cnt: 4 - val loss: 2.663214862346649 - train loss: 9.568498879671097\n",
      "cnt: 5 - val loss: 2.661487355828285 - train loss: 9.563614547252655\n",
      "cnt: 6 - val loss: 2.7223033905029297 - train loss: 9.563154757022858\n",
      "cnt: 7 - val loss: 2.6381421834230423 - train loss: 9.551548153162003\n",
      "cnt: 8 - val loss: 2.6952245235443115 - train loss: 9.556443482637405\n",
      "cnt: 9 - val loss: 2.7163813412189484 - train loss: 9.560475587844849\n",
      "cnt: 10 - val loss: 2.685955226421356 - train loss: 9.557752534747124\n",
      "cnt: 11 - val loss: 2.721252828836441 - train loss: 9.550105929374695\n",
      "cnt: 12 - val loss: 2.6525155156850815 - train loss: 9.556463688611984\n",
      "cnt: 13 - val loss: 2.6763535737991333 - train loss: 9.555650919675827\n",
      "cnt: 14 - val loss: 2.636664405465126 - train loss: 9.551169946789742\n",
      "cnt: 15 - val loss: 2.7703197300434113 - train loss: 9.543936640024185\n",
      "cnt: 16 - val loss: 2.7453355491161346 - train loss: 9.536923542618752\n",
      "cnt: 17 - val loss: 2.836091995239258 - train loss: 9.540102005004883\n",
      "cnt: 18 - val loss: 2.7259273529052734 - train loss: 9.546620473265648\n",
      "cnt: 19 - val loss: 2.713560938835144 - train loss: 9.538309589028358\n",
      "cnt: 20 - val loss: 2.685802400112152 - train loss: 9.538357734680176\n",
      "cnt: 21 - val loss: 2.736236661672592 - train loss: 9.541202068328857\n",
      "cnt: 22 - val loss: 2.7448647916316986 - train loss: 9.534961223602295\n",
      "cnt: 23 - val loss: 2.783985286951065 - train loss: 9.520363196730614\n",
      "cnt: 24 - val loss: 2.7669289112091064 - train loss: 9.531832337379456\n",
      "cnt: 25 - val loss: 2.7729420512914658 - train loss: 9.541658148169518\n",
      "cnt: 26 - val loss: 2.724143922328949 - train loss: 9.533502414822578\n",
      "cnt: 27 - val loss: 2.8153578341007233 - train loss: 9.528553426265717\n",
      "cnt: 28 - val loss: 2.7530400156974792 - train loss: 9.53183813393116\n",
      "cnt: 29 - val loss: 2.6542691737413406 - train loss: 9.516120865941048\n",
      "cnt: 30 - val loss: 2.7377514243125916 - train loss: 9.518809616565704\n",
      "cnt: 31 - val loss: 2.6811790466308594 - train loss: 9.517930299043655\n",
      "cnt: 32 - val loss: 2.6762239038944244 - train loss: 9.513991460204124\n",
      "cnt: 33 - val loss: 2.642284631729126 - train loss: 9.521528214216232\n",
      "cnt: 34 - val loss: 2.7538892030715942 - train loss: 9.5133598446846\n",
      "cnt: 35 - val loss: 2.6024152636528015 - train loss: 9.517993196845055\n",
      "cnt: 0 - val loss: 2.797196179628372 - train loss: 9.516371071338654\n",
      "cnt: 1 - val loss: 2.7040713727474213 - train loss: 9.51021209359169\n",
      "cnt: 2 - val loss: 2.8172243237495422 - train loss: 9.507688447833061\n",
      "cnt: 3 - val loss: 2.6665221452713013 - train loss: 9.50184565782547\n",
      "cnt: 4 - val loss: 2.7074078917503357 - train loss: 9.499343067407608\n",
      "cnt: 5 - val loss: 2.596271276473999 - train loss: 9.510224908590317\n",
      "cnt: 0 - val loss: 2.6534058451652527 - train loss: 9.502310574054718\n",
      "cnt: 1 - val loss: 2.711181879043579 - train loss: 9.492322593927383\n",
      "cnt: 2 - val loss: 2.6349120885133743 - train loss: 9.50442686676979\n",
      "cnt: 3 - val loss: 2.6963599622249603 - train loss: 9.483191221952438\n",
      "cnt: 4 - val loss: 2.6368319988250732 - train loss: 9.499388545751572\n",
      "cnt: 5 - val loss: 2.6173236072063446 - train loss: 9.496498927474022\n",
      "cnt: 6 - val loss: 2.6804490089416504 - train loss: 9.499998450279236\n",
      "cnt: 7 - val loss: 2.7192592918872833 - train loss: 9.502415642142296\n",
      "cnt: 8 - val loss: 2.781349331140518 - train loss: 9.489242509007454\n",
      "cnt: 9 - val loss: 2.6761159896850586 - train loss: 9.494612127542496\n",
      "cnt: 10 - val loss: 2.7108452320098877 - train loss: 9.479220375418663\n",
      "cnt: 11 - val loss: 2.6512847244739532 - train loss: 9.490540534257889\n",
      "cnt: 12 - val loss: 2.722138285636902 - train loss: 9.480845287442207\n",
      "cnt: 13 - val loss: 2.6672912538051605 - train loss: 9.489477559924126\n",
      "cnt: 14 - val loss: 2.7100962102413177 - train loss: 9.492594838142395\n",
      "cnt: 15 - val loss: 2.683976709842682 - train loss: 9.488654971122742\n",
      "cnt: 16 - val loss: 2.6367563754320145 - train loss: 9.48480412364006\n",
      "cnt: 17 - val loss: 2.6949471533298492 - train loss: 9.476219177246094\n",
      "cnt: 18 - val loss: 2.7107082903385162 - train loss: 9.480775520205498\n",
      "cnt: 19 - val loss: 2.67409285902977 - train loss: 9.476321935653687\n",
      "cnt: 20 - val loss: 2.7081816643476486 - train loss: 9.473673224449158\n",
      "cnt: 21 - val loss: 2.5703921020030975 - train loss: 9.475704208016396\n",
      "cnt: 0 - val loss: 2.6863113045692444 - train loss: 9.465105265378952\n",
      "cnt: 1 - val loss: 2.744204729795456 - train loss: 9.46169176697731\n",
      "cnt: 2 - val loss: 2.643241196870804 - train loss: 9.4718599319458\n",
      "cnt: 3 - val loss: 2.6095805913209915 - train loss: 9.466865837574005\n",
      "cnt: 4 - val loss: 2.7342270612716675 - train loss: 9.471125185489655\n",
      "cnt: 5 - val loss: 2.5997987389564514 - train loss: 9.46631945669651\n",
      "cnt: 6 - val loss: 2.6769247204065323 - train loss: 9.454791516065598\n",
      "cnt: 7 - val loss: 2.8143853694200516 - train loss: 9.452245712280273\n",
      "cnt: 8 - val loss: 2.6969645619392395 - train loss: 9.453416466712952\n",
      "cnt: 9 - val loss: 2.6820778250694275 - train loss: 9.447430938482285\n",
      "cnt: 10 - val loss: 2.6425070464611053 - train loss: 9.455552160739899\n",
      "cnt: 11 - val loss: 2.7174075841903687 - train loss: 9.46150952577591\n",
      "cnt: 12 - val loss: 2.704108238220215 - train loss: 9.451874673366547\n",
      "cnt: 13 - val loss: 2.5785739719867706 - train loss: 9.453582242131233\n",
      "cnt: 14 - val loss: 2.7123013585805893 - train loss: 9.439319878816605\n",
      "cnt: 15 - val loss: 2.705567866563797 - train loss: 9.453184634447098\n",
      "cnt: 16 - val loss: 2.6422135829925537 - train loss: 9.445176541805267\n",
      "cnt: 17 - val loss: 2.6782878935337067 - train loss: 9.446885764598846\n",
      "cnt: 18 - val loss: 2.6655506789684296 - train loss: 9.449739694595337\n",
      "cnt: 19 - val loss: 2.6446729600429535 - train loss: 9.455996811389923\n",
      "cnt: 20 - val loss: 2.71857912838459 - train loss: 9.447633877396584\n",
      "cnt: 21 - val loss: 2.70110747218132 - train loss: 9.426890552043915\n",
      "cnt: 22 - val loss: 2.63346466422081 - train loss: 9.435547441244125\n",
      "cnt: 23 - val loss: 2.682061493396759 - train loss: 9.434039950370789\n",
      "cnt: 24 - val loss: 2.6265516877174377 - train loss: 9.437287151813507\n",
      "cnt: 25 - val loss: 2.6825998574495316 - train loss: 9.433195263147354\n",
      "cnt: 26 - val loss: 2.71535125374794 - train loss: 9.426900774240494\n",
      "cnt: 27 - val loss: 2.684416025876999 - train loss: 9.434461042284966\n",
      "cnt: 28 - val loss: 2.6703674495220184 - train loss: 9.41815534234047\n",
      "cnt: 29 - val loss: 2.6680468916893005 - train loss: 9.426757782697678\n",
      "cnt: 30 - val loss: 2.659312456846237 - train loss: 9.421182110905647\n",
      "cnt: 31 - val loss: 2.704079359769821 - train loss: 9.415750801563263\n",
      "cnt: 32 - val loss: 2.724965989589691 - train loss: 9.418124198913574\n",
      "cnt: 33 - val loss: 2.6221930384635925 - train loss: 9.42566242814064\n",
      "cnt: 34 - val loss: 2.754970908164978 - train loss: 9.420428425073624\n",
      "cnt: 35 - val loss: 2.7333273589611053 - train loss: 9.41476072371006\n",
      "cnt: 36 - val loss: 2.6594160199165344 - train loss: 9.411840796470642\n",
      "cnt: 37 - val loss: 2.7119427621364594 - train loss: 9.413830995559692\n",
      "cnt: 38 - val loss: 2.7727768272161484 - train loss: 9.41433584690094\n",
      "cnt: 39 - val loss: 2.724431738257408 - train loss: 9.416012302041054\n",
      "cnt: 40 - val loss: 2.6201410740613937 - train loss: 9.399361073970795\n",
      "cnt: 41 - val loss: 2.721944972872734 - train loss: 9.406989738345146\n",
      "cnt: 42 - val loss: 2.662912666797638 - train loss: 9.414809882640839\n",
      "cnt: 43 - val loss: 2.7127809822559357 - train loss: 9.396642699837685\n",
      "cnt: 44 - val loss: 2.73076494038105 - train loss: 9.40606839954853\n",
      "cnt: 45 - val loss: 2.6404722034931183 - train loss: 9.389111280441284\n",
      "cnt: 46 - val loss: 2.5855772644281387 - train loss: 9.392922699451447\n",
      "cnt: 47 - val loss: 2.69383904337883 - train loss: 9.394768863916397\n",
      "cnt: 48 - val loss: 2.676537275314331 - train loss: 9.39816814661026\n",
      "cnt: 49 - val loss: 2.8457897305488586 - train loss: 9.387928918004036\n",
      "cnt: 50 - val loss: 2.616079181432724 - train loss: 9.411308139562607\n",
      "cnt: 51 - val loss: 2.591838762164116 - train loss: 9.401111677289009\n",
      "cnt: 52 - val loss: 2.6407520473003387 - train loss: 9.38057728111744\n",
      "cnt: 53 - val loss: 2.6574734151363373 - train loss: 9.384976401925087\n",
      "cnt: 54 - val loss: 2.7187415957450867 - train loss: 9.395390957593918\n",
      "cnt: 55 - val loss: 2.6437349021434784 - train loss: 9.383719339966774\n",
      "cnt: 56 - val loss: 2.672532081604004 - train loss: 9.389511078596115\n",
      "cnt: 57 - val loss: 2.6216188222169876 - train loss: 9.388294115662575\n",
      "cnt: 58 - val loss: 2.5896543860435486 - train loss: 9.369561240077019\n",
      "cnt: 59 - val loss: 2.6430416107177734 - train loss: 9.379911035299301\n",
      "cnt: 60 - val loss: 2.681011527776718 - train loss: 9.376880466938019\n",
      "cnt: 61 - val loss: 2.5957827866077423 - train loss: 9.374220237135887\n",
      "cnt: 62 - val loss: 2.6224673092365265 - train loss: 9.376021072268486\n",
      "cnt: 63 - val loss: 2.669284373521805 - train loss: 9.370560646057129\n",
      "cnt: 64 - val loss: 2.597587838768959 - train loss: 9.37282446026802\n",
      "cnt: 65 - val loss: 2.6773841381073 - train loss: 9.377223625779152\n",
      "cnt: 66 - val loss: 2.7268855571746826 - train loss: 9.369594261050224\n",
      "cnt: 67 - val loss: 2.707354485988617 - train loss: 9.37101224064827\n",
      "cnt: 68 - val loss: 2.5989305526018143 - train loss: 9.361540898680687\n",
      "cnt: 69 - val loss: 2.6603039354085922 - train loss: 9.36481961607933\n",
      "cnt: 70 - val loss: 2.6535731703042984 - train loss: 9.372336700558662\n",
      "cnt: 71 - val loss: 2.5649035274982452 - train loss: 9.371028393507004\n",
      "cnt: 0 - val loss: 2.596822500228882 - train loss: 9.363581001758575\n",
      "cnt: 1 - val loss: 2.578156039118767 - train loss: 9.35708637535572\n",
      "cnt: 2 - val loss: 2.5996869951486588 - train loss: 9.360903427004814\n",
      "cnt: 3 - val loss: 2.6411509811878204 - train loss: 9.359085962176323\n",
      "cnt: 4 - val loss: 2.6641165912151337 - train loss: 9.35230165719986\n",
      "cnt: 5 - val loss: 2.6143424808979034 - train loss: 9.346755012869835\n",
      "cnt: 6 - val loss: 2.6765004694461823 - train loss: 9.359093979001045\n",
      "cnt: 7 - val loss: 2.648396611213684 - train loss: 9.344519093632698\n",
      "cnt: 8 - val loss: 2.6417235732078552 - train loss: 9.351099133491516\n",
      "cnt: 9 - val loss: 2.6991588324308395 - train loss: 9.342692226171494\n",
      "cnt: 10 - val loss: 2.6780793368816376 - train loss: 9.346421897411346\n",
      "cnt: 11 - val loss: 2.577541783452034 - train loss: 9.343374878168106\n",
      "cnt: 12 - val loss: 2.597359150648117 - train loss: 9.357673615217209\n",
      "cnt: 13 - val loss: 2.5985593646764755 - train loss: 9.346447557210922\n",
      "cnt: 14 - val loss: 2.6599966287612915 - train loss: 9.33451634645462\n",
      "cnt: 15 - val loss: 2.6958332657814026 - train loss: 9.344666868448257\n",
      "cnt: 16 - val loss: 2.696167290210724 - train loss: 9.338623479008675\n",
      "cnt: 17 - val loss: 2.76069076359272 - train loss: 9.335937514901161\n",
      "cnt: 18 - val loss: 2.621780037879944 - train loss: 9.33429017663002\n",
      "cnt: 19 - val loss: 2.647812783718109 - train loss: 9.343743607401848\n",
      "cnt: 20 - val loss: 2.627776801586151 - train loss: 9.334575280547142\n",
      "cnt: 21 - val loss: 2.7007055282592773 - train loss: 9.321425303816795\n",
      "cnt: 22 - val loss: 2.60470412671566 - train loss: 9.327213391661644\n",
      "cnt: 23 - val loss: 2.593448579311371 - train loss: 9.323709473013878\n",
      "cnt: 24 - val loss: 2.605196461081505 - train loss: 9.329111948609352\n",
      "cnt: 25 - val loss: 2.691045641899109 - train loss: 9.329465344548225\n",
      "cnt: 26 - val loss: 2.659433513879776 - train loss: 9.311818316578865\n",
      "cnt: 27 - val loss: 2.636882722377777 - train loss: 9.319382667541504\n",
      "cnt: 28 - val loss: 2.622354879975319 - train loss: 9.317436128854752\n",
      "cnt: 29 - val loss: 2.641600117087364 - train loss: 9.31742811203003\n",
      "cnt: 30 - val loss: 2.627295508980751 - train loss: 9.31343911588192\n",
      "cnt: 31 - val loss: 2.6788832247257233 - train loss: 9.310049578547478\n",
      "cnt: 32 - val loss: 2.5499951541423798 - train loss: 9.320136323571205\n",
      "cnt: 0 - val loss: 2.58279949426651 - train loss: 9.316072821617126\n",
      "cnt: 1 - val loss: 2.5751880556344986 - train loss: 9.310938537120819\n",
      "cnt: 2 - val loss: 2.719254493713379 - train loss: 9.291980758309364\n",
      "cnt: 3 - val loss: 2.68936550617218 - train loss: 9.301049530506134\n",
      "cnt: 4 - val loss: 2.6315832436084747 - train loss: 9.307427108287811\n",
      "cnt: 5 - val loss: 2.604027420282364 - train loss: 9.300316497683525\n",
      "cnt: 6 - val loss: 2.6957008242607117 - train loss: 9.297006011009216\n",
      "cnt: 7 - val loss: 2.6169467866420746 - train loss: 9.298288822174072\n",
      "cnt: 8 - val loss: 2.5930482000112534 - train loss: 9.30281713604927\n",
      "cnt: 9 - val loss: 2.6229797899723053 - train loss: 9.295049607753754\n",
      "cnt: 10 - val loss: 2.6577301919460297 - train loss: 9.295811340212822\n",
      "cnt: 11 - val loss: 2.658576473593712 - train loss: 9.30872654914856\n",
      "cnt: 12 - val loss: 2.6223080158233643 - train loss: 9.286537170410156\n",
      "cnt: 13 - val loss: 2.6678714752197266 - train loss: 9.292706221342087\n",
      "cnt: 14 - val loss: 2.60454298555851 - train loss: 9.29204685986042\n",
      "cnt: 15 - val loss: 2.59562486410141 - train loss: 9.293360322713852\n",
      "cnt: 16 - val loss: 2.6879719346761703 - train loss: 9.281439945101738\n",
      "cnt: 17 - val loss: 2.6424237489700317 - train loss: 9.290401428937912\n",
      "cnt: 18 - val loss: 2.6637459993362427 - train loss: 9.293842688202858\n",
      "cnt: 19 - val loss: 2.5614689886569977 - train loss: 9.280908033251762\n",
      "cnt: 20 - val loss: 2.6343711614608765 - train loss: 9.294430658221245\n",
      "cnt: 21 - val loss: 2.6399649381637573 - train loss: 9.280915677547455\n",
      "cnt: 22 - val loss: 2.622602254152298 - train loss: 9.28466285765171\n",
      "cnt: 23 - val loss: 2.678338333964348 - train loss: 9.282588750123978\n",
      "cnt: 24 - val loss: 2.644565671682358 - train loss: 9.286499798297882\n",
      "cnt: 25 - val loss: 2.706116646528244 - train loss: 9.271325513720512\n",
      "cnt: 26 - val loss: 2.591934084892273 - train loss: 9.275578707456589\n",
      "cnt: 27 - val loss: 2.604505091905594 - train loss: 9.27891081571579\n",
      "cnt: 28 - val loss: 2.652606099843979 - train loss: 9.271760508418083\n",
      "cnt: 29 - val loss: 2.727832779288292 - train loss: 9.27386112511158\n",
      "cnt: 30 - val loss: 2.569817289710045 - train loss: 9.26496997475624\n",
      "cnt: 31 - val loss: 2.607848823070526 - train loss: 9.268950924277306\n",
      "cnt: 32 - val loss: 2.665925770998001 - train loss: 9.262233197689056\n",
      "cnt: 33 - val loss: 2.6127878427505493 - train loss: 9.27265913784504\n",
      "cnt: 34 - val loss: 2.5763323307037354 - train loss: 9.275252684950829\n",
      "cnt: 35 - val loss: 2.5933804512023926 - train loss: 9.266589686274529\n",
      "cnt: 36 - val loss: 2.625217944383621 - train loss: 9.265449941158295\n",
      "cnt: 37 - val loss: 2.572887822985649 - train loss: 9.248536989092827\n",
      "cnt: 38 - val loss: 2.6466393172740936 - train loss: 9.257008418440819\n",
      "cnt: 39 - val loss: 2.586370825767517 - train loss: 9.25993113219738\n",
      "cnt: 40 - val loss: 2.5160591453313828 - train loss: 9.260207071900368\n",
      "cnt: 0 - val loss: 2.6494307965040207 - train loss: 9.250709161162376\n",
      "cnt: 1 - val loss: 2.6132020354270935 - train loss: 9.248361378908157\n",
      "cnt: 2 - val loss: 2.570405825972557 - train loss: 9.252213090658188\n",
      "cnt: 3 - val loss: 2.667792558670044 - train loss: 9.249697297811508\n",
      "cnt: 4 - val loss: 2.7009694576263428 - train loss: 9.247180491685867\n",
      "cnt: 5 - val loss: 2.792284309864044 - train loss: 9.238173767924309\n",
      "cnt: 6 - val loss: 2.561587706208229 - train loss: 9.249521672725677\n",
      "cnt: 7 - val loss: 2.5371254980564117 - train loss: 9.23844401538372\n",
      "cnt: 8 - val loss: 2.5512856543064117 - train loss: 9.241692677140236\n",
      "cnt: 9 - val loss: 2.5296193957328796 - train loss: 9.259057432413101\n",
      "cnt: 10 - val loss: 2.6288326382637024 - train loss: 9.2425377368927\n",
      "cnt: 11 - val loss: 2.658694863319397 - train loss: 9.230015903711319\n",
      "cnt: 12 - val loss: 2.6045545041561127 - train loss: 9.237056568264961\n",
      "cnt: 13 - val loss: 2.655060738325119 - train loss: 9.22731463611126\n",
      "cnt: 14 - val loss: 2.581907704472542 - train loss: 9.235671490430832\n",
      "cnt: 15 - val loss: 2.564636766910553 - train loss: 9.229535713791847\n",
      "cnt: 16 - val loss: 2.584390163421631 - train loss: 9.238068163394928\n",
      "cnt: 17 - val loss: 2.612501233816147 - train loss: 9.23612965643406\n",
      "cnt: 18 - val loss: 2.7336421757936478 - train loss: 9.231427416205406\n",
      "cnt: 19 - val loss: 2.573575109243393 - train loss: 9.231302320957184\n",
      "cnt: 20 - val loss: 2.6894690990448 - train loss: 9.229249611496925\n",
      "cnt: 21 - val loss: 2.6176533699035645 - train loss: 9.231380268931389\n",
      "cnt: 22 - val loss: 2.5602766424417496 - train loss: 9.223124906420708\n",
      "cnt: 23 - val loss: 2.6124593913555145 - train loss: 9.227812454104424\n",
      "cnt: 24 - val loss: 2.624370887875557 - train loss: 9.216887280344963\n",
      "cnt: 25 - val loss: 2.668809413909912 - train loss: 9.231153890490532\n",
      "cnt: 26 - val loss: 2.5832329243421555 - train loss: 9.20615915954113\n",
      "cnt: 27 - val loss: 2.6317360401153564 - train loss: 9.219763815402985\n",
      "cnt: 28 - val loss: 2.56313955783844 - train loss: 9.218501031398773\n",
      "cnt: 29 - val loss: 2.5658506006002426 - train loss: 9.207103982567787\n",
      "cnt: 30 - val loss: 2.7050670981407166 - train loss: 9.206948235630989\n",
      "cnt: 31 - val loss: 2.5362001955509186 - train loss: 9.20667228102684\n",
      "cnt: 32 - val loss: 2.5734091699123383 - train loss: 9.210159718990326\n",
      "cnt: 33 - val loss: 2.643151968717575 - train loss: 9.200471833348274\n",
      "cnt: 34 - val loss: 2.6545624434947968 - train loss: 9.203849852085114\n",
      "cnt: 35 - val loss: 2.6965421736240387 - train loss: 9.211463034152985\n",
      "cnt: 36 - val loss: 2.5153152644634247 - train loss: 9.195946007966995\n",
      "cnt: 0 - val loss: 2.6434913873672485 - train loss: 9.200322896242142\n",
      "cnt: 1 - val loss: 2.5765008479356766 - train loss: 9.211447611451149\n",
      "cnt: 2 - val loss: 2.6939858347177505 - train loss: 9.192776590585709\n",
      "cnt: 3 - val loss: 2.6270524859428406 - train loss: 9.186967983841896\n",
      "cnt: 4 - val loss: 2.6347562670707703 - train loss: 9.204028651118279\n",
      "cnt: 5 - val loss: 2.5965614318847656 - train loss: 9.197134733200073\n",
      "cnt: 6 - val loss: 2.7074856758117676 - train loss: 9.183796927332878\n",
      "cnt: 7 - val loss: 2.6105779111385345 - train loss: 9.18844985961914\n",
      "cnt: 8 - val loss: 2.556333437561989 - train loss: 9.18464209139347\n",
      "cnt: 9 - val loss: 2.530168756842613 - train loss: 9.199732899665833\n",
      "cnt: 10 - val loss: 2.5696793645620346 - train loss: 9.186703220009804\n",
      "cnt: 11 - val loss: 2.6503942906856537 - train loss: 9.179698646068573\n",
      "cnt: 12 - val loss: 2.6142488718032837 - train loss: 9.177115768194199\n",
      "cnt: 13 - val loss: 2.6063594222068787 - train loss: 9.176021054387093\n",
      "cnt: 14 - val loss: 2.5519191324710846 - train loss: 9.188240647315979\n",
      "cnt: 15 - val loss: 2.6968131363391876 - train loss: 9.172997564077377\n",
      "cnt: 16 - val loss: 2.509504973888397 - train loss: 9.167408406734467\n",
      "cnt: 0 - val loss: 2.613954797387123 - train loss: 9.1697136759758\n",
      "cnt: 1 - val loss: 2.6182360351085663 - train loss: 9.185323163866997\n",
      "cnt: 2 - val loss: 2.6826970279216766 - train loss: 9.170138627290726\n",
      "cnt: 3 - val loss: 2.6274830996990204 - train loss: 9.176506757736206\n",
      "cnt: 4 - val loss: 2.6685912907123566 - train loss: 9.176247969269753\n",
      "cnt: 5 - val loss: 2.5910907983779907 - train loss: 9.169316202402115\n",
      "cnt: 6 - val loss: 2.573236107826233 - train loss: 9.16623605787754\n",
      "cnt: 7 - val loss: 2.56393726170063 - train loss: 9.163098305463791\n",
      "cnt: 8 - val loss: 2.529611587524414 - train loss: 9.169458910822868\n",
      "cnt: 9 - val loss: 2.5842061638832092 - train loss: 9.159240067005157\n",
      "cnt: 10 - val loss: 2.586195081472397 - train loss: 9.17009024322033\n",
      "cnt: 11 - val loss: 2.607255280017853 - train loss: 9.162003725767136\n",
      "cnt: 12 - val loss: 2.6510082483291626 - train loss: 9.16042473912239\n",
      "cnt: 13 - val loss: 2.591542065143585 - train loss: 9.159981057047844\n",
      "cnt: 14 - val loss: 2.5767768919467926 - train loss: 9.16362115740776\n",
      "cnt: 15 - val loss: 2.5382087379693985 - train loss: 9.157139733433723\n",
      "cnt: 16 - val loss: 2.5521708875894547 - train loss: 9.158923864364624\n",
      "cnt: 17 - val loss: 2.568939134478569 - train loss: 9.147570446133614\n",
      "cnt: 18 - val loss: 2.5162467062473297 - train loss: 9.156499907374382\n",
      "cnt: 19 - val loss: 2.6670762598514557 - train loss: 9.15003715455532\n",
      "cnt: 20 - val loss: 2.555975303053856 - train loss: 9.141375094652176\n",
      "cnt: 21 - val loss: 2.592886596918106 - train loss: 9.158012509346008\n",
      "cnt: 22 - val loss: 2.644746571779251 - train loss: 9.147343218326569\n",
      "cnt: 23 - val loss: 2.572825625538826 - train loss: 9.151415258646011\n",
      "cnt: 24 - val loss: 2.5843519121408463 - train loss: 9.143670737743378\n",
      "cnt: 25 - val loss: 2.5868033170700073 - train loss: 9.153632700443268\n",
      "cnt: 26 - val loss: 2.617081254720688 - train loss: 9.144583016633987\n",
      "cnt: 27 - val loss: 2.597243756055832 - train loss: 9.14191222190857\n",
      "cnt: 28 - val loss: 2.609235644340515 - train loss: 9.139206573367119\n",
      "cnt: 29 - val loss: 2.565293461084366 - train loss: 9.142331749200821\n",
      "cnt: 30 - val loss: 2.6422576904296875 - train loss: 9.145390063524246\n",
      "cnt: 31 - val loss: 2.5926325917243958 - train loss: 9.132425710558891\n",
      "cnt: 32 - val loss: 2.543045297265053 - train loss: 9.128018900752068\n",
      "cnt: 33 - val loss: 2.570855140686035 - train loss: 9.135316491127014\n",
      "cnt: 34 - val loss: 2.573415696620941 - train loss: 9.128095537424088\n",
      "cnt: 35 - val loss: 2.5878933370113373 - train loss: 9.132130146026611\n",
      "cnt: 36 - val loss: 2.6169271767139435 - train loss: 9.126929208636284\n",
      "cnt: 37 - val loss: 2.632380485534668 - train loss: 9.135243818163872\n",
      "cnt: 38 - val loss: 2.5813915878534317 - train loss: 9.127275481820107\n",
      "cnt: 39 - val loss: 2.610760420560837 - train loss: 9.130990341305733\n",
      "cnt: 40 - val loss: 2.5850888788700104 - train loss: 9.12426546216011\n",
      "cnt: 41 - val loss: 2.5509112626314163 - train loss: 9.121839597821236\n",
      "cnt: 42 - val loss: 2.64341938495636 - train loss: 9.130632504820824\n",
      "cnt: 43 - val loss: 2.596203625202179 - train loss: 9.130954310297966\n",
      "cnt: 44 - val loss: 2.5494012981653214 - train loss: 9.113031655550003\n",
      "cnt: 45 - val loss: 2.6563787162303925 - train loss: 9.11533772945404\n",
      "cnt: 46 - val loss: 2.519331380724907 - train loss: 9.127012968063354\n",
      "cnt: 47 - val loss: 2.6753687262535095 - train loss: 9.123928382992744\n",
      "cnt: 48 - val loss: 2.6459966897964478 - train loss: 9.114948883652687\n",
      "cnt: 49 - val loss: 2.569785863161087 - train loss: 9.107946574687958\n",
      "cnt: 50 - val loss: 2.6070104986429214 - train loss: 9.109389618039131\n",
      "cnt: 51 - val loss: 2.6603772193193436 - train loss: 9.124702170491219\n",
      "cnt: 52 - val loss: 2.5409545600414276 - train loss: 9.1087846159935\n",
      "cnt: 53 - val loss: 2.597625643014908 - train loss: 9.108973190188408\n",
      "cnt: 54 - val loss: 2.5859180241823196 - train loss: 9.10677483677864\n",
      "cnt: 55 - val loss: 2.5583710372447968 - train loss: 9.104142263531685\n",
      "cnt: 56 - val loss: 2.5628312826156616 - train loss: 9.100507393479347\n",
      "cnt: 57 - val loss: 2.5628405064344406 - train loss: 9.09936748445034\n",
      "cnt: 58 - val loss: 2.5632045567035675 - train loss: 9.107654228806496\n",
      "cnt: 59 - val loss: 2.5793053656816483 - train loss: 9.091036528348923\n",
      "cnt: 60 - val loss: 2.5883871018886566 - train loss: 9.096414014697075\n",
      "cnt: 61 - val loss: 2.563702881336212 - train loss: 9.09306912124157\n",
      "cnt: 62 - val loss: 2.605823040008545 - train loss: 9.097385555505753\n",
      "cnt: 63 - val loss: 2.6170390844345093 - train loss: 9.087653040885925\n",
      "cnt: 64 - val loss: 2.6192833483219147 - train loss: 9.09941391646862\n",
      "cnt: 65 - val loss: 2.6103831380605698 - train loss: 9.081528127193451\n",
      "cnt: 66 - val loss: 2.5448321402072906 - train loss: 9.086567535996437\n",
      "cnt: 67 - val loss: 2.5425167083740234 - train loss: 9.085420593619347\n",
      "cnt: 68 - val loss: 2.534029483795166 - train loss: 9.093385323882103\n",
      "cnt: 69 - val loss: 2.481403961777687 - train loss: 9.07768777012825\n",
      "cnt: 0 - val loss: 2.586056560277939 - train loss: 9.080560892820358\n",
      "cnt: 1 - val loss: 2.5804622769355774 - train loss: 9.077307060360909\n",
      "cnt: 2 - val loss: 2.5167375206947327 - train loss: 9.075040653347969\n",
      "cnt: 3 - val loss: 2.560203343629837 - train loss: 9.083304345607758\n",
      "cnt: 4 - val loss: 2.633433699607849 - train loss: 9.07485580444336\n",
      "cnt: 5 - val loss: 2.5473466515541077 - train loss: 9.07323843240738\n",
      "cnt: 6 - val loss: 2.5764224529266357 - train loss: 9.075227320194244\n",
      "cnt: 7 - val loss: 2.6566845178604126 - train loss: 9.075562745332718\n",
      "cnt: 8 - val loss: 2.528780236840248 - train loss: 9.090178534388542\n",
      "cnt: 9 - val loss: 2.587111532688141 - train loss: 9.068865239620209\n",
      "cnt: 10 - val loss: 2.579746186733246 - train loss: 9.071177512407303\n",
      "cnt: 11 - val loss: 2.5413504242897034 - train loss: 9.06887473165989\n",
      "cnt: 12 - val loss: 2.516728565096855 - train loss: 9.066699177026749\n",
      "cnt: 13 - val loss: 2.6256395876407623 - train loss: 9.06771431863308\n",
      "cnt: 14 - val loss: 2.563314601778984 - train loss: 9.070332780480385\n",
      "cnt: 15 - val loss: 2.5319970101118088 - train loss: 9.066858634352684\n",
      "cnt: 16 - val loss: 2.5647996366024017 - train loss: 9.0646553337574\n",
      "cnt: 17 - val loss: 2.5935117453336716 - train loss: 9.066832900047302\n",
      "cnt: 18 - val loss: 2.595819115638733 - train loss: 9.055792704224586\n",
      "cnt: 19 - val loss: 2.552288234233856 - train loss: 9.053487673401833\n",
      "cnt: 20 - val loss: 2.5982520282268524 - train loss: 9.063321933150291\n",
      "cnt: 21 - val loss: 2.601359784603119 - train loss: 9.052258014678955\n",
      "cnt: 22 - val loss: 2.569249242544174 - train loss: 9.043232932686806\n",
      "cnt: 23 - val loss: 2.5157195031642914 - train loss: 9.049301460385323\n",
      "cnt: 24 - val loss: 2.5390283912420273 - train loss: 9.05933864414692\n",
      "cnt: 25 - val loss: 2.5199105590581894 - train loss: 9.052334293723106\n",
      "cnt: 26 - val loss: 2.635575592517853 - train loss: 9.050350427627563\n",
      "cnt: 27 - val loss: 2.523517370223999 - train loss: 9.049615383148193\n",
      "cnt: 28 - val loss: 2.534397006034851 - train loss: 9.039953723549843\n",
      "cnt: 29 - val loss: 2.6183061599731445 - train loss: 9.042916491627693\n",
      "cnt: 30 - val loss: 2.5713606774806976 - train loss: 9.037687227129936\n",
      "cnt: 31 - val loss: 2.5249821096658707 - train loss: 9.04926010966301\n",
      "cnt: 32 - val loss: 2.520151123404503 - train loss: 9.048935055732727\n",
      "cnt: 33 - val loss: 2.5852494537830353 - train loss: 9.0401820987463\n",
      "cnt: 34 - val loss: 2.593340963125229 - train loss: 9.037744894623756\n",
      "cnt: 35 - val loss: 2.602279454469681 - train loss: 9.041199907660484\n",
      "cnt: 36 - val loss: 2.579534560441971 - train loss: 9.042585492134094\n",
      "cnt: 37 - val loss: 2.5269463062286377 - train loss: 9.03280682861805\n",
      "cnt: 38 - val loss: 2.5299523323774338 - train loss: 9.034616068005562\n",
      "cnt: 39 - val loss: 2.574832320213318 - train loss: 9.040443897247314\n",
      "cnt: 40 - val loss: 2.7373390793800354 - train loss: 9.035444900393486\n",
      "cnt: 41 - val loss: 2.5005133748054504 - train loss: 9.035907685756683\n",
      "cnt: 42 - val loss: 2.5620245337486267 - train loss: 9.039786338806152\n",
      "cnt: 43 - val loss: 2.585103690624237 - train loss: 9.025732710957527\n",
      "cnt: 44 - val loss: 2.5767591297626495 - train loss: 9.025958478450775\n",
      "cnt: 45 - val loss: 2.5978469103574753 - train loss: 9.016572386026382\n",
      "cnt: 46 - val loss: 2.562304139137268 - train loss: 9.025849595665932\n",
      "cnt: 47 - val loss: 2.5797761976718903 - train loss: 9.026326477527618\n",
      "cnt: 48 - val loss: 2.585768908262253 - train loss: 9.02118581533432\n",
      "cnt: 49 - val loss: 2.5681430399417877 - train loss: 9.024347737431526\n",
      "cnt: 50 - val loss: 2.6367346942424774 - train loss: 9.016225442290306\n",
      "cnt: 51 - val loss: 2.6032615900039673 - train loss: 9.019985675811768\n",
      "cnt: 52 - val loss: 2.51907117664814 - train loss: 9.027722224593163\n",
      "cnt: 53 - val loss: 2.5226639956235886 - train loss: 9.014160320162773\n",
      "cnt: 54 - val loss: 2.662082254886627 - train loss: 9.008479699492455\n",
      "cnt: 55 - val loss: 2.5932323038578033 - train loss: 9.009974628686905\n",
      "cnt: 56 - val loss: 2.5163037925958633 - train loss: 9.012288242578506\n",
      "cnt: 57 - val loss: 2.553554132580757 - train loss: 9.00617840886116\n",
      "cnt: 58 - val loss: 2.51085002720356 - train loss: 9.014233991503716\n",
      "cnt: 59 - val loss: 2.581600069999695 - train loss: 9.00175616145134\n",
      "cnt: 60 - val loss: 2.5654225945472717 - train loss: 9.00615482032299\n",
      "cnt: 61 - val loss: 2.5444948077201843 - train loss: 9.000482976436615\n",
      "cnt: 62 - val loss: 2.587482377886772 - train loss: 9.009953871369362\n",
      "cnt: 63 - val loss: 2.622963070869446 - train loss: 8.998431921005249\n",
      "cnt: 64 - val loss: 2.543216198682785 - train loss: 9.003346413373947\n",
      "cnt: 65 - val loss: 2.59372141957283 - train loss: 8.989228188991547\n",
      "cnt: 66 - val loss: 2.533259838819504 - train loss: 8.991439759731293\n",
      "cnt: 67 - val loss: 2.530704140663147 - train loss: 8.988367661833763\n",
      "cnt: 68 - val loss: 2.521466553211212 - train loss: 8.98506811261177\n",
      "cnt: 69 - val loss: 2.5314821153879166 - train loss: 8.996840581297874\n",
      "cnt: 70 - val loss: 2.5930104553699493 - train loss: 8.98237757384777\n",
      "cnt: 71 - val loss: 2.5694241523742676 - train loss: 8.99408657848835\n",
      "cnt: 72 - val loss: 2.6924811005592346 - train loss: 8.988470137119293\n",
      "cnt: 73 - val loss: 2.4931110441684723 - train loss: 8.994467079639435\n",
      "cnt: 74 - val loss: 2.487073391675949 - train loss: 8.981645658612251\n",
      "cnt: 75 - val loss: 2.617490589618683 - train loss: 8.98624262213707\n",
      "cnt: 76 - val loss: 2.6405471712350845 - train loss: 8.980807408690453\n",
      "cnt: 77 - val loss: 2.5174616426229477 - train loss: 8.976792812347412\n",
      "cnt: 78 - val loss: 2.5788148939609528 - train loss: 8.974007219076157\n",
      "cnt: 79 - val loss: 2.504155620932579 - train loss: 8.981492355465889\n",
      "cnt: 80 - val loss: 2.628045439720154 - train loss: 8.973474770784378\n",
      "cnt: 81 - val loss: 2.5488479286432266 - train loss: 8.98132511973381\n",
      "cnt: 82 - val loss: 2.5334829092025757 - train loss: 8.984457239508629\n",
      "cnt: 83 - val loss: 2.5430245399475098 - train loss: 8.966330751776695\n",
      "cnt: 84 - val loss: 2.5831144750118256 - train loss: 8.974529430270195\n",
      "cnt: 85 - val loss: 2.63644702732563 - train loss: 8.98323941230774\n",
      "cnt: 86 - val loss: 2.522362530231476 - train loss: 8.980841800570488\n",
      "cnt: 87 - val loss: 2.5792982429265976 - train loss: 8.969970554113388\n",
      "cnt: 88 - val loss: 2.5338277518749237 - train loss: 8.980439379811287\n",
      "cnt: 89 - val loss: 2.460601657629013 - train loss: 8.965550869703293\n",
      "cnt: 0 - val loss: 2.6390691101551056 - train loss: 8.970327571034431\n",
      "cnt: 1 - val loss: 2.5769351720809937 - train loss: 8.957838013768196\n",
      "cnt: 2 - val loss: 2.5492442846298218 - train loss: 8.971864342689514\n",
      "cnt: 3 - val loss: 2.538186326622963 - train loss: 8.959557265043259\n",
      "cnt: 4 - val loss: 2.536651626229286 - train loss: 8.954762816429138\n",
      "cnt: 5 - val loss: 2.51648773252964 - train loss: 8.971010372042656\n",
      "cnt: 6 - val loss: 2.500786989927292 - train loss: 8.963250547647476\n",
      "cnt: 7 - val loss: 2.6040957272052765 - train loss: 8.954677566885948\n",
      "cnt: 8 - val loss: 2.532900959253311 - train loss: 8.948535203933716\n",
      "cnt: 9 - val loss: 2.485507383942604 - train loss: 8.966454163193703\n",
      "cnt: 10 - val loss: 2.501938506960869 - train loss: 8.964094027876854\n",
      "cnt: 11 - val loss: 2.5684338212013245 - train loss: 8.954141020774841\n",
      "cnt: 12 - val loss: 2.465916007757187 - train loss: 8.952079832553864\n",
      "cnt: 13 - val loss: 2.514443591237068 - train loss: 8.95099951326847\n",
      "cnt: 14 - val loss: 2.512334778904915 - train loss: 8.953536674380302\n",
      "cnt: 15 - val loss: 2.5029168128967285 - train loss: 8.958014413714409\n",
      "cnt: 16 - val loss: 2.5407083928585052 - train loss: 8.94633974134922\n",
      "cnt: 17 - val loss: 2.5733819752931595 - train loss: 8.95209276676178\n",
      "cnt: 18 - val loss: 2.565690040588379 - train loss: 8.945708826184273\n",
      "cnt: 19 - val loss: 2.517632484436035 - train loss: 8.943300664424896\n",
      "cnt: 20 - val loss: 2.4931015223264694 - train loss: 8.935858681797981\n",
      "cnt: 21 - val loss: 2.59532967209816 - train loss: 8.933714851737022\n",
      "cnt: 22 - val loss: 2.5004741847515106 - train loss: 8.94139090180397\n",
      "cnt: 23 - val loss: 2.572388917207718 - train loss: 8.937999293208122\n",
      "cnt: 24 - val loss: 2.551548019051552 - train loss: 8.930950701236725\n",
      "cnt: 25 - val loss: 2.5017635226249695 - train loss: 8.944197937846184\n",
      "cnt: 26 - val loss: 2.4892022162675858 - train loss: 8.93448144197464\n",
      "cnt: 27 - val loss: 2.4856420308351517 - train loss: 8.9442790299654\n",
      "cnt: 28 - val loss: 2.552211508154869 - train loss: 8.934321120381355\n",
      "cnt: 29 - val loss: 2.5163398385047913 - train loss: 8.930790081620216\n",
      "cnt: 30 - val loss: 2.488971531391144 - train loss: 8.93483404815197\n",
      "cnt: 31 - val loss: 2.575000047683716 - train loss: 8.924506962299347\n",
      "cnt: 32 - val loss: 2.4821815341711044 - train loss: 8.93436224758625\n",
      "cnt: 33 - val loss: 2.4529775381088257 - train loss: 8.934544682502747\n",
      "cnt: 0 - val loss: 2.5196466743946075 - train loss: 8.925669893622398\n",
      "cnt: 1 - val loss: 2.513266444206238 - train loss: 8.92645375430584\n",
      "cnt: 2 - val loss: 2.506504014134407 - train loss: 8.92422990500927\n",
      "cnt: 3 - val loss: 2.5485651195049286 - train loss: 8.932579800486565\n",
      "cnt: 4 - val loss: 2.584512770175934 - train loss: 8.925874456763268\n",
      "cnt: 5 - val loss: 2.5096106082201004 - train loss: 8.924864679574966\n",
      "cnt: 6 - val loss: 2.5733045041561127 - train loss: 8.923325851559639\n",
      "cnt: 7 - val loss: 2.573747009038925 - train loss: 8.91348959505558\n",
      "cnt: 8 - val loss: 2.5454039573669434 - train loss: 8.913455441594124\n",
      "cnt: 9 - val loss: 2.5424551963806152 - train loss: 8.91913266479969\n",
      "cnt: 10 - val loss: 2.5623285472393036 - train loss: 8.910606294870377\n",
      "cnt: 11 - val loss: 2.5324220955371857 - train loss: 8.917182713747025\n",
      "cnt: 12 - val loss: 2.5254052579402924 - train loss: 8.91134937107563\n",
      "cnt: 13 - val loss: 2.5841393023729324 - train loss: 8.907582402229309\n",
      "cnt: 14 - val loss: 2.5597861856222153 - train loss: 8.906939044594765\n",
      "cnt: 15 - val loss: 2.530740723013878 - train loss: 8.905772030353546\n",
      "cnt: 16 - val loss: 2.557884857058525 - train loss: 8.907050609588623\n",
      "cnt: 17 - val loss: 2.4796682000160217 - train loss: 8.895023718476295\n",
      "cnt: 18 - val loss: 2.56882306933403 - train loss: 8.917869418859482\n",
      "cnt: 19 - val loss: 2.614854171872139 - train loss: 8.903671607375145\n",
      "cnt: 20 - val loss: 2.5527229011058807 - train loss: 8.903726324439049\n",
      "cnt: 21 - val loss: 2.592437982559204 - train loss: 8.902250722050667\n",
      "cnt: 22 - val loss: 2.46927373111248 - train loss: 8.9084682315588\n",
      "cnt: 23 - val loss: 2.5522840321063995 - train loss: 8.907739669084549\n",
      "cnt: 24 - val loss: 2.5436332523822784 - train loss: 8.892343625426292\n",
      "cnt: 25 - val loss: 2.555035799741745 - train loss: 8.893870875239372\n",
      "cnt: 26 - val loss: 2.4248026609420776 - train loss: 8.89331965148449\n",
      "cnt: 0 - val loss: 2.5257538557052612 - train loss: 8.890035882592201\n",
      "cnt: 1 - val loss: 2.518895983695984 - train loss: 8.896166697144508\n",
      "cnt: 2 - val loss: 2.5759878158569336 - train loss: 8.891099721193314\n",
      "cnt: 3 - val loss: 2.49192051589489 - train loss: 8.897106572985649\n",
      "cnt: 4 - val loss: 2.536636620759964 - train loss: 8.887378513813019\n",
      "cnt: 5 - val loss: 2.4951577931642532 - train loss: 8.89209894835949\n",
      "cnt: 6 - val loss: 2.563409775495529 - train loss: 8.882845684885979\n",
      "cnt: 7 - val loss: 2.5278168618679047 - train loss: 8.8937618881464\n",
      "cnt: 8 - val loss: 2.553414836525917 - train loss: 8.877822875976562\n",
      "cnt: 9 - val loss: 2.5077995359897614 - train loss: 8.88060811161995\n",
      "cnt: 10 - val loss: 2.5229516327381134 - train loss: 8.879180520772934\n",
      "cnt: 11 - val loss: 2.589944675564766 - train loss: 8.870921969413757\n",
      "cnt: 12 - val loss: 2.4751104563474655 - train loss: 8.87371475994587\n",
      "cnt: 13 - val loss: 2.506732165813446 - train loss: 8.886317774653435\n",
      "cnt: 14 - val loss: 2.615158975124359 - train loss: 8.880527257919312\n",
      "cnt: 15 - val loss: 2.4321779757738113 - train loss: 8.87674742937088\n",
      "cnt: 16 - val loss: 2.4253358095884323 - train loss: 8.870777070522308\n",
      "cnt: 17 - val loss: 2.528971031308174 - train loss: 8.871753230690956\n",
      "cnt: 18 - val loss: 2.520656794309616 - train loss: 8.867949828505516\n",
      "cnt: 19 - val loss: 2.5064744353294373 - train loss: 8.858605772256851\n",
      "cnt: 20 - val loss: 2.5031915456056595 - train loss: 8.873542547225952\n",
      "cnt: 21 - val loss: 2.5511316806077957 - train loss: 8.865899488329887\n",
      "cnt: 22 - val loss: 2.519564986228943 - train loss: 8.864898636937141\n",
      "cnt: 23 - val loss: 2.6028119027614594 - train loss: 8.86967322230339\n",
      "cnt: 24 - val loss: 2.5077937841415405 - train loss: 8.867896601557732\n",
      "cnt: 25 - val loss: 2.566162556409836 - train loss: 8.870203077793121\n",
      "cnt: 26 - val loss: 2.5128466486930847 - train loss: 8.85456220805645\n",
      "cnt: 27 - val loss: 2.58341284096241 - train loss: 8.862078577280045\n",
      "cnt: 28 - val loss: 2.5031678676605225 - train loss: 8.869224473834038\n",
      "cnt: 29 - val loss: 2.5396692752838135 - train loss: 8.855859160423279\n",
      "cnt: 30 - val loss: 2.5216267108917236 - train loss: 8.868782430887222\n",
      "cnt: 31 - val loss: 2.513037532567978 - train loss: 8.849468410015106\n",
      "cnt: 32 - val loss: 2.4573840647935867 - train loss: 8.858873546123505\n",
      "cnt: 33 - val loss: 2.462670922279358 - train loss: 8.859234735369682\n",
      "cnt: 34 - val loss: 2.545953631401062 - train loss: 8.850372448563576\n",
      "cnt: 35 - val loss: 2.4869611859321594 - train loss: 8.854092881083488\n",
      "cnt: 36 - val loss: 2.5673699229955673 - train loss: 8.8486919850111\n",
      "cnt: 37 - val loss: 2.5580639839172363 - train loss: 8.84789463877678\n",
      "cnt: 38 - val loss: 2.5117844939231873 - train loss: 8.849499583244324\n",
      "cnt: 39 - val loss: 2.4461298137903214 - train loss: 8.841351464390755\n",
      "cnt: 40 - val loss: 2.551054000854492 - train loss: 8.847653239965439\n",
      "cnt: 41 - val loss: 2.4677039235830307 - train loss: 8.848103880882263\n",
      "cnt: 42 - val loss: 2.5003415644168854 - train loss: 8.84360608458519\n",
      "cnt: 43 - val loss: 2.4772444665431976 - train loss: 8.835414931178093\n",
      "cnt: 44 - val loss: 2.460947483778 - train loss: 8.829931914806366\n",
      "cnt: 45 - val loss: 2.508375436067581 - train loss: 8.836718678474426\n",
      "cnt: 46 - val loss: 2.6040635257959366 - train loss: 8.836994603276253\n",
      "cnt: 47 - val loss: 2.5445212572813034 - train loss: 8.834560632705688\n",
      "cnt: 48 - val loss: 2.5264501571655273 - train loss: 8.838027283549309\n",
      "cnt: 49 - val loss: 2.4901154935359955 - train loss: 8.83395867049694\n",
      "cnt: 50 - val loss: 2.4197105169296265 - train loss: 8.823562920093536\n",
      "cnt: 0 - val loss: 2.450627401471138 - train loss: 8.827060997486115\n",
      "cnt: 1 - val loss: 2.5859286189079285 - train loss: 8.828691318631172\n",
      "cnt: 2 - val loss: 2.521881654858589 - train loss: 8.828110009431839\n",
      "cnt: 3 - val loss: 2.5609211921691895 - train loss: 8.826511353254318\n",
      "cnt: 4 - val loss: 2.4505585581064224 - train loss: 8.82326340675354\n",
      "cnt: 5 - val loss: 2.569538027048111 - train loss: 8.830918312072754\n",
      "cnt: 6 - val loss: 2.470726892352104 - train loss: 8.823621317744255\n",
      "cnt: 7 - val loss: 2.4944063127040863 - train loss: 8.829982563853264\n",
      "cnt: 8 - val loss: 2.6002419888973236 - train loss: 8.82920053601265\n",
      "cnt: 9 - val loss: 2.4676399528980255 - train loss: 8.821857318282127\n",
      "cnt: 10 - val loss: 2.472742438316345 - train loss: 8.835739374160767\n",
      "cnt: 11 - val loss: 2.4864721596240997 - train loss: 8.811668246984482\n",
      "cnt: 12 - val loss: 2.49245685338974 - train loss: 8.818264052271843\n",
      "cnt: 13 - val loss: 2.4585810005664825 - train loss: 8.829951614141464\n",
      "cnt: 14 - val loss: 2.486277997493744 - train loss: 8.816981866955757\n",
      "cnt: 15 - val loss: 2.4905543327331543 - train loss: 8.816487520933151\n",
      "cnt: 16 - val loss: 2.4441902488470078 - train loss: 8.807134583592415\n",
      "cnt: 17 - val loss: 2.4583479911088943 - train loss: 8.818374946713448\n",
      "cnt: 18 - val loss: 2.48382431268692 - train loss: 8.821558743715286\n",
      "cnt: 19 - val loss: 2.4616119414567947 - train loss: 8.819581732153893\n",
      "cnt: 20 - val loss: 2.5522309094667435 - train loss: 8.80443288385868\n",
      "cnt: 21 - val loss: 2.524895578622818 - train loss: 8.801287233829498\n",
      "cnt: 22 - val loss: 2.54273784160614 - train loss: 8.81601682305336\n",
      "cnt: 23 - val loss: 2.5184923112392426 - train loss: 8.803230866789818\n",
      "cnt: 24 - val loss: 2.4719411432743073 - train loss: 8.81034404039383\n",
      "cnt: 25 - val loss: 2.601408764719963 - train loss: 8.800884678959846\n",
      "cnt: 26 - val loss: 2.4949027001857758 - train loss: 8.811931028962135\n",
      "cnt: 27 - val loss: 2.501019075512886 - train loss: 8.800281301140785\n",
      "cnt: 28 - val loss: 2.596553400158882 - train loss: 8.800004690885544\n",
      "cnt: 29 - val loss: 2.4801141917705536 - train loss: 8.79520696401596\n",
      "cnt: 30 - val loss: 2.466064527630806 - train loss: 8.806630849838257\n",
      "cnt: 31 - val loss: 2.49214369058609 - train loss: 8.793395176529884\n",
      "cnt: 32 - val loss: 2.4320510029792786 - train loss: 8.796891257166862\n",
      "cnt: 33 - val loss: 2.4855205416679382 - train loss: 8.791670098900795\n",
      "cnt: 34 - val loss: 2.5364107191562653 - train loss: 8.787887692451477\n",
      "cnt: 35 - val loss: 2.4816120266914368 - train loss: 8.783058166503906\n",
      "cnt: 36 - val loss: 2.460438162088394 - train loss: 8.791743278503418\n",
      "cnt: 37 - val loss: 2.476997449994087 - train loss: 8.79370591044426\n",
      "cnt: 38 - val loss: 2.5760580599308014 - train loss: 8.788568332791328\n",
      "cnt: 39 - val loss: 2.541867643594742 - train loss: 8.785301610827446\n",
      "cnt: 40 - val loss: 2.5214441418647766 - train loss: 8.796264350414276\n",
      "cnt: 41 - val loss: 2.535496175289154 - train loss: 8.784931316971779\n",
      "cnt: 42 - val loss: 2.4491148591041565 - train loss: 8.783278197050095\n",
      "cnt: 43 - val loss: 2.5597842037677765 - train loss: 8.785126015543938\n",
      "cnt: 44 - val loss: 2.4642646461725235 - train loss: 8.79280273616314\n",
      "cnt: 45 - val loss: 2.526679828763008 - train loss: 8.783792153000832\n",
      "cnt: 46 - val loss: 2.49142062664032 - train loss: 8.782610416412354\n",
      "cnt: 47 - val loss: 2.4795182645320892 - train loss: 8.777308121323586\n",
      "cnt: 48 - val loss: 2.531963050365448 - train loss: 8.782705828547478\n",
      "cnt: 49 - val loss: 2.5151616632938385 - train loss: 8.780818045139313\n",
      "cnt: 50 - val loss: 2.5084905326366425 - train loss: 8.77069328725338\n",
      "cnt: 51 - val loss: 2.447682276368141 - train loss: 8.772112712264061\n",
      "cnt: 52 - val loss: 2.5062506794929504 - train loss: 8.779117047786713\n",
      "cnt: 53 - val loss: 2.493217408657074 - train loss: 8.774231478571892\n",
      "cnt: 54 - val loss: 2.490940183401108 - train loss: 8.779520258307457\n",
      "cnt: 55 - val loss: 2.6508544385433197 - train loss: 8.768245369195938\n",
      "cnt: 56 - val loss: 2.538690984249115 - train loss: 8.768775463104248\n",
      "cnt: 57 - val loss: 2.58774733543396 - train loss: 8.768425926566124\n",
      "cnt: 58 - val loss: 2.458008646965027 - train loss: 8.766931474208832\n",
      "cnt: 59 - val loss: 2.4411132633686066 - train loss: 8.768925935029984\n",
      "cnt: 60 - val loss: 2.4554770439863205 - train loss: 8.752562880516052\n",
      "cnt: 61 - val loss: 2.5024817287921906 - train loss: 8.757417410612106\n",
      "cnt: 62 - val loss: 2.6005404591560364 - train loss: 8.765604183077812\n",
      "cnt: 63 - val loss: 2.5437656342983246 - train loss: 8.757396638393402\n",
      "cnt: 64 - val loss: 2.479354277253151 - train loss: 8.756209298968315\n",
      "cnt: 65 - val loss: 2.411060646176338 - train loss: 8.739297270774841\n",
      "cnt: 0 - val loss: 2.5975937843322754 - train loss: 8.76787556707859\n",
      "cnt: 1 - val loss: 2.52597214281559 - train loss: 8.757510930299759\n",
      "cnt: 2 - val loss: 2.513315200805664 - train loss: 8.760294288396835\n",
      "cnt: 3 - val loss: 2.4887554347515106 - train loss: 8.753118917346\n",
      "cnt: 4 - val loss: 2.4784072935581207 - train loss: 8.753782823681831\n",
      "cnt: 5 - val loss: 2.4567618519067764 - train loss: 8.756255269050598\n",
      "cnt: 6 - val loss: 2.5522045493125916 - train loss: 8.764016643166542\n",
      "cnt: 7 - val loss: 2.5377578735351562 - train loss: 8.763741806149483\n",
      "cnt: 8 - val loss: 2.4614507406949997 - train loss: 8.749857246875763\n",
      "cnt: 9 - val loss: 2.395005851984024 - train loss: 8.748700946569443\n",
      "cnt: 0 - val loss: 2.4285710155963898 - train loss: 8.749869659543037\n",
      "cnt: 1 - val loss: 2.544190436601639 - train loss: 8.737519055604935\n",
      "cnt: 2 - val loss: 2.4896627217531204 - train loss: 8.735332131385803\n",
      "cnt: 3 - val loss: 2.560518115758896 - train loss: 8.736109748482704\n",
      "cnt: 4 - val loss: 2.5266887545585632 - train loss: 8.746299177408218\n",
      "cnt: 5 - val loss: 2.463149920105934 - train loss: 8.735660940408707\n",
      "cnt: 6 - val loss: 2.429256483912468 - train loss: 8.740943670272827\n",
      "cnt: 7 - val loss: 2.539555698633194 - train loss: 8.740539535880089\n",
      "cnt: 8 - val loss: 2.4875295162200928 - train loss: 8.736637145280838\n",
      "cnt: 9 - val loss: 2.6205438673496246 - train loss: 8.741444066166878\n",
      "cnt: 10 - val loss: 2.475022405385971 - train loss: 8.72968240082264\n",
      "cnt: 11 - val loss: 2.4936582446098328 - train loss: 8.733690619468689\n",
      "cnt: 12 - val loss: 2.500807076692581 - train loss: 8.734308257699013\n",
      "cnt: 13 - val loss: 2.4988455325365067 - train loss: 8.725412800908089\n",
      "cnt: 14 - val loss: 2.539171189069748 - train loss: 8.731290683150291\n",
      "cnt: 15 - val loss: 2.5206543803215027 - train loss: 8.729395225644112\n",
      "cnt: 16 - val loss: 2.4663788825273514 - train loss: 8.720628753304482\n",
      "cnt: 17 - val loss: 2.5084644705057144 - train loss: 8.727963969111443\n",
      "cnt: 18 - val loss: 2.4829559177160263 - train loss: 8.726006492972374\n",
      "cnt: 19 - val loss: 2.4817960262298584 - train loss: 8.71599866449833\n",
      "cnt: 20 - val loss: 2.5092411637306213 - train loss: 8.72544713318348\n",
      "cnt: 21 - val loss: 2.491004765033722 - train loss: 8.715698838233948\n",
      "cnt: 22 - val loss: 2.4780571460723877 - train loss: 8.726124942302704\n",
      "cnt: 23 - val loss: 2.409296005964279 - train loss: 8.720322027802467\n",
      "cnt: 24 - val loss: 2.422432601451874 - train loss: 8.718560099601746\n",
      "cnt: 25 - val loss: 2.446007549762726 - train loss: 8.7242631316185\n",
      "cnt: 26 - val loss: 2.4083677232265472 - train loss: 8.724362447857857\n",
      "cnt: 27 - val loss: 2.5140287578105927 - train loss: 8.722069263458252\n",
      "cnt: 28 - val loss: 2.487698256969452 - train loss: 8.724070861935616\n",
      "cnt: 29 - val loss: 2.540562704205513 - train loss: 8.712970837950706\n",
      "cnt: 30 - val loss: 2.471313089132309 - train loss: 8.712558686733246\n",
      "cnt: 31 - val loss: 2.4857855439186096 - train loss: 8.704653561115265\n",
      "cnt: 32 - val loss: 2.4577215909957886 - train loss: 8.702504128217697\n",
      "cnt: 33 - val loss: 2.4355679154396057 - train loss: 8.71539568901062\n",
      "cnt: 34 - val loss: 2.4154824018478394 - train loss: 8.712916404008865\n",
      "cnt: 35 - val loss: 2.481472611427307 - train loss: 8.715066656470299\n",
      "cnt: 36 - val loss: 2.5010391771793365 - train loss: 8.714036762714386\n",
      "cnt: 37 - val loss: 2.5036736130714417 - train loss: 8.713673248887062\n",
      "cnt: 38 - val loss: 2.454246312379837 - train loss: 8.706301420927048\n",
      "cnt: 39 - val loss: 2.4601006507873535 - train loss: 8.699877187609673\n",
      "cnt: 40 - val loss: 2.4929433465003967 - train loss: 8.701961144804955\n",
      "cnt: 41 - val loss: 2.5260025709867477 - train loss: 8.710357502102852\n",
      "cnt: 42 - val loss: 2.488458216190338 - train loss: 8.70452792942524\n",
      "cnt: 43 - val loss: 2.54640132188797 - train loss: 8.701169490814209\n",
      "cnt: 44 - val loss: 2.476800560951233 - train loss: 8.692106068134308\n",
      "cnt: 45 - val loss: 2.4563708156347275 - train loss: 8.69044403731823\n",
      "cnt: 46 - val loss: 2.4904837608337402 - train loss: 8.695503503084183\n",
      "cnt: 47 - val loss: 2.448696330189705 - train loss: 8.708888217806816\n",
      "cnt: 48 - val loss: 2.4756503105163574 - train loss: 8.699475556612015\n",
      "cnt: 49 - val loss: 2.5043386816978455 - train loss: 8.695519864559174\n",
      "cnt: 50 - val loss: 2.4531034231185913 - train loss: 8.680373653769493\n",
      "cnt: 51 - val loss: 2.449541300535202 - train loss: 8.691802605986595\n",
      "cnt: 52 - val loss: 2.4111442118883133 - train loss: 8.67924328148365\n",
      "cnt: 53 - val loss: 2.440428227186203 - train loss: 8.67452535033226\n",
      "cnt: 54 - val loss: 2.5072151869535446 - train loss: 8.685621827840805\n",
      "cnt: 55 - val loss: 2.453187882900238 - train loss: 8.698671996593475\n",
      "cnt: 56 - val loss: 2.5766597986221313 - train loss: 8.687395498156548\n",
      "cnt: 57 - val loss: 2.520618051290512 - train loss: 8.683779299259186\n",
      "cnt: 58 - val loss: 2.459690734744072 - train loss: 8.676386222243309\n",
      "cnt: 59 - val loss: 2.4242454916238785 - train loss: 8.674763724207878\n",
      "cnt: 60 - val loss: 2.4065617620944977 - train loss: 8.673100277781487\n",
      "cnt: 61 - val loss: 2.4612656235694885 - train loss: 8.675204709172249\n",
      "cnt: 62 - val loss: 2.4066721349954605 - train loss: 8.675532698631287\n",
      "cnt: 63 - val loss: 2.380232200026512 - train loss: 8.678085193037987\n",
      "cnt: 0 - val loss: 2.4369440376758575 - train loss: 8.671824276447296\n",
      "cnt: 1 - val loss: 2.4111920297145844 - train loss: 8.669124841690063\n",
      "cnt: 2 - val loss: 2.509417474269867 - train loss: 8.680860102176666\n",
      "cnt: 3 - val loss: 2.453900009393692 - train loss: 8.671149760484695\n",
      "cnt: 4 - val loss: 2.4386848509311676 - train loss: 8.668469995260239\n",
      "cnt: 5 - val loss: 2.448787361383438 - train loss: 8.669044256210327\n",
      "cnt: 6 - val loss: 2.513535737991333 - train loss: 8.681247189640999\n",
      "cnt: 7 - val loss: 2.4566002637147903 - train loss: 8.669802844524384\n",
      "cnt: 8 - val loss: 2.477627784013748 - train loss: 8.66709916293621\n",
      "cnt: 9 - val loss: 2.3948785960674286 - train loss: 8.66274780035019\n",
      "cnt: 10 - val loss: 2.5013061463832855 - train loss: 8.661840036511421\n",
      "cnt: 11 - val loss: 2.4687139093875885 - train loss: 8.669119536876678\n",
      "cnt: 12 - val loss: 2.449514627456665 - train loss: 8.654863446950912\n",
      "cnt: 13 - val loss: 2.5038624554872513 - train loss: 8.664858803153038\n",
      "cnt: 14 - val loss: 2.583373725414276 - train loss: 8.654153630137444\n",
      "cnt: 15 - val loss: 2.446249008178711 - train loss: 8.664879202842712\n",
      "cnt: 16 - val loss: 2.482960894703865 - train loss: 8.651004061102867\n",
      "cnt: 17 - val loss: 2.483228638768196 - train loss: 8.66053307056427\n",
      "cnt: 18 - val loss: 2.5570164620876312 - train loss: 8.65451680123806\n",
      "cnt: 19 - val loss: 2.494865983724594 - train loss: 8.649020984768867\n",
      "cnt: 20 - val loss: 2.513988494873047 - train loss: 8.65443554520607\n",
      "cnt: 21 - val loss: 2.461971491575241 - train loss: 8.66133987903595\n",
      "cnt: 22 - val loss: 2.44296295940876 - train loss: 8.660425901412964\n",
      "cnt: 23 - val loss: 2.4341021180152893 - train loss: 8.652310341596603\n",
      "cnt: 24 - val loss: 2.4335565119981766 - train loss: 8.659633114933968\n",
      "cnt: 25 - val loss: 2.45331634581089 - train loss: 8.642099007964134\n",
      "cnt: 26 - val loss: 2.4758695363998413 - train loss: 8.644291177392006\n",
      "cnt: 27 - val loss: 2.4371183663606644 - train loss: 8.639627784490585\n",
      "cnt: 28 - val loss: 2.485826939344406 - train loss: 8.646200567483902\n",
      "cnt: 29 - val loss: 2.480671137571335 - train loss: 8.64502888917923\n",
      "cnt: 30 - val loss: 2.5870250463485718 - train loss: 8.642490103840828\n",
      "cnt: 31 - val loss: 2.398380935192108 - train loss: 8.63796104490757\n",
      "cnt: 32 - val loss: 2.44552806019783 - train loss: 8.647782444953918\n",
      "cnt: 33 - val loss: 2.4378917515277863 - train loss: 8.63774687051773\n",
      "cnt: 34 - val loss: 2.3901472240686417 - train loss: 8.636901274323463\n",
      "cnt: 35 - val loss: 2.511179894208908 - train loss: 8.634231179952621\n",
      "cnt: 36 - val loss: 2.5119749158620834 - train loss: 8.629030987620354\n",
      "cnt: 37 - val loss: 2.449250802397728 - train loss: 8.634012639522552\n",
      "cnt: 38 - val loss: 2.415864035487175 - train loss: 8.645344376564026\n",
      "cnt: 39 - val loss: 2.471122920513153 - train loss: 8.635791137814522\n",
      "cnt: 40 - val loss: 2.505624905228615 - train loss: 8.633558094501495\n",
      "cnt: 41 - val loss: 2.418336510658264 - train loss: 8.622248858213425\n",
      "cnt: 42 - val loss: 2.393096163868904 - train loss: 8.63341960310936\n",
      "cnt: 43 - val loss: 2.5112266540527344 - train loss: 8.633870989084244\n",
      "cnt: 44 - val loss: 2.528114140033722 - train loss: 8.62134899199009\n",
      "cnt: 45 - val loss: 2.4411350041627884 - train loss: 8.629506602883339\n",
      "cnt: 46 - val loss: 2.4637393951416016 - train loss: 8.619046747684479\n",
      "cnt: 47 - val loss: 2.4212137013673782 - train loss: 8.618366584181786\n",
      "cnt: 48 - val loss: 2.5086770951747894 - train loss: 8.622732758522034\n",
      "cnt: 49 - val loss: 2.422492891550064 - train loss: 8.633162781596184\n",
      "cnt: 50 - val loss: 2.4279967844486237 - train loss: 8.62154558300972\n",
      "cnt: 51 - val loss: 2.4484194815158844 - train loss: 8.626659318804741\n",
      "cnt: 52 - val loss: 2.435140982270241 - train loss: 8.618062555789948\n",
      "cnt: 53 - val loss: 2.3814005702733994 - train loss: 8.629973888397217\n",
      "cnt: 54 - val loss: 2.4358577728271484 - train loss: 8.620326384902\n",
      "cnt: 55 - val loss: 2.4519362300634384 - train loss: 8.603728994727135\n",
      "cnt: 56 - val loss: 2.4402390271425247 - train loss: 8.609401121735573\n",
      "cnt: 57 - val loss: 2.4833300411701202 - train loss: 8.63312853872776\n",
      "cnt: 58 - val loss: 2.4761784076690674 - train loss: 8.625647246837616\n",
      "cnt: 59 - val loss: 2.4579473584890366 - train loss: 8.611790806055069\n",
      "cnt: 60 - val loss: 2.447999358177185 - train loss: 8.609637007117271\n",
      "cnt: 61 - val loss: 2.4472025632858276 - train loss: 8.619617819786072\n",
      "cnt: 62 - val loss: 2.4362845420837402 - train loss: 8.60818538069725\n",
      "cnt: 63 - val loss: 2.4624686539173126 - train loss: 8.600708693265915\n",
      "cnt: 64 - val loss: 2.3730954974889755 - train loss: 8.601806372404099\n",
      "cnt: 0 - val loss: 2.4054806530475616 - train loss: 8.606328010559082\n",
      "cnt: 1 - val loss: 2.388117551803589 - train loss: 8.623581260442734\n",
      "cnt: 2 - val loss: 2.455460876226425 - train loss: 8.594091027975082\n",
      "cnt: 3 - val loss: 2.420074447989464 - train loss: 8.603267207741737\n",
      "cnt: 4 - val loss: 2.4524716585874557 - train loss: 8.60390491783619\n",
      "cnt: 5 - val loss: 2.4793180227279663 - train loss: 8.598800733685493\n",
      "cnt: 6 - val loss: 2.3964142203330994 - train loss: 8.596174165606499\n",
      "cnt: 7 - val loss: 2.459746167063713 - train loss: 8.59910224378109\n",
      "cnt: 8 - val loss: 2.429283693432808 - train loss: 8.596958249807358\n",
      "cnt: 9 - val loss: 2.514726370573044 - train loss: 8.588523149490356\n",
      "cnt: 10 - val loss: 2.513483315706253 - train loss: 8.597204506397247\n",
      "cnt: 11 - val loss: 2.4359209835529327 - train loss: 8.597220242023468\n",
      "cnt: 12 - val loss: 2.42171074450016 - train loss: 8.589733436703682\n",
      "cnt: 13 - val loss: 2.435290202498436 - train loss: 8.599086627364159\n",
      "cnt: 14 - val loss: 2.4010461270809174 - train loss: 8.59512796998024\n",
      "cnt: 15 - val loss: 2.460644617676735 - train loss: 8.602269023656845\n",
      "cnt: 16 - val loss: 2.430441737174988 - train loss: 8.581653594970703\n",
      "cnt: 17 - val loss: 2.503548264503479 - train loss: 8.588645219802856\n",
      "cnt: 18 - val loss: 2.5229115337133408 - train loss: 8.584875896573067\n",
      "cnt: 19 - val loss: 2.431949406862259 - train loss: 8.597234204411507\n",
      "cnt: 20 - val loss: 2.438944101333618 - train loss: 8.57814335823059\n",
      "cnt: 21 - val loss: 2.4401868730783463 - train loss: 8.586087107658386\n",
      "cnt: 22 - val loss: 2.437057852745056 - train loss: 8.582586854696274\n",
      "cnt: 23 - val loss: 2.4422251135110855 - train loss: 8.581172838807106\n",
      "cnt: 24 - val loss: 2.4823913872241974 - train loss: 8.58004754781723\n",
      "cnt: 25 - val loss: 2.4025921374559402 - train loss: 8.564117223024368\n",
      "cnt: 26 - val loss: 2.6222472339868546 - train loss: 8.5831129103899\n",
      "cnt: 27 - val loss: 2.4331354051828384 - train loss: 8.579866871237755\n",
      "cnt: 28 - val loss: 2.5406515896320343 - train loss: 8.578972727060318\n",
      "cnt: 29 - val loss: 2.391112118959427 - train loss: 8.57448959350586\n",
      "cnt: 30 - val loss: 2.470784991979599 - train loss: 8.571688428521156\n",
      "cnt: 31 - val loss: 2.4558285921812057 - train loss: 8.569348528981209\n",
      "cnt: 32 - val loss: 2.4710408449172974 - train loss: 8.57396300137043\n",
      "cnt: 33 - val loss: 2.4668727070093155 - train loss: 8.571782395243645\n",
      "cnt: 34 - val loss: 2.4603929817676544 - train loss: 8.567993864417076\n",
      "cnt: 35 - val loss: 2.5029890090227127 - train loss: 8.56186492741108\n",
      "cnt: 36 - val loss: 2.4661869257688522 - train loss: 8.566137328743935\n",
      "cnt: 37 - val loss: 2.4304816126823425 - train loss: 8.564404413104057\n",
      "cnt: 38 - val loss: 2.4340019673109055 - train loss: 8.564914301037788\n",
      "cnt: 39 - val loss: 2.384946569800377 - train loss: 8.567350029945374\n",
      "cnt: 40 - val loss: 2.421697661280632 - train loss: 8.553274437785149\n",
      "cnt: 41 - val loss: 2.4402956366539 - train loss: 8.554076567292213\n",
      "cnt: 42 - val loss: 2.4716040194034576 - train loss: 8.561039254069328\n",
      "cnt: 43 - val loss: 2.3438512980937958 - train loss: 8.564219996333122\n",
      "cnt: 0 - val loss: 2.461666688323021 - train loss: 8.554382935166359\n",
      "cnt: 1 - val loss: 2.369355693459511 - train loss: 8.562737226486206\n",
      "cnt: 2 - val loss: 2.436779037117958 - train loss: 8.549427703022957\n",
      "cnt: 3 - val loss: 2.4494728595018387 - train loss: 8.546675905585289\n",
      "cnt: 4 - val loss: 2.4077252596616745 - train loss: 8.553645387291908\n",
      "cnt: 5 - val loss: 2.4114347398281097 - train loss: 8.550471037626266\n",
      "cnt: 6 - val loss: 2.434696525335312 - train loss: 8.554258942604065\n",
      "cnt: 7 - val loss: 2.4282379746437073 - train loss: 8.555809080600739\n",
      "cnt: 8 - val loss: 2.386958420276642 - train loss: 8.55711179971695\n",
      "cnt: 9 - val loss: 2.400133103132248 - train loss: 8.534913524985313\n",
      "cnt: 10 - val loss: 2.415163606405258 - train loss: 8.544204428792\n",
      "cnt: 11 - val loss: 2.465457960963249 - train loss: 8.545043364167213\n",
      "cnt: 12 - val loss: 2.448013424873352 - train loss: 8.553402304649353\n",
      "cnt: 13 - val loss: 2.429494261741638 - train loss: 8.544506430625916\n",
      "cnt: 14 - val loss: 2.532043620944023 - train loss: 8.53498101234436\n",
      "cnt: 15 - val loss: 2.4035275280475616 - train loss: 8.549728885293007\n",
      "cnt: 16 - val loss: 2.396352931857109 - train loss: 8.533887475728989\n",
      "cnt: 17 - val loss: 2.387979879975319 - train loss: 8.533780723810196\n",
      "cnt: 18 - val loss: 2.4550786912441254 - train loss: 8.528156280517578\n",
      "cnt: 19 - val loss: 2.3806536197662354 - train loss: 8.535059601068497\n",
      "cnt: 20 - val loss: 2.4324756115674973 - train loss: 8.53913226723671\n",
      "cnt: 21 - val loss: 2.4665008932352066 - train loss: 8.529332667589188\n",
      "cnt: 22 - val loss: 2.4589036405086517 - train loss: 8.530732840299606\n",
      "cnt: 23 - val loss: 2.4480004757642746 - train loss: 8.544532507658005\n",
      "cnt: 24 - val loss: 2.4513964653015137 - train loss: 8.536409839987755\n",
      "cnt: 25 - val loss: 2.3626183569431305 - train loss: 8.52686856687069\n",
      "cnt: 26 - val loss: 2.3827882409095764 - train loss: 8.51706312596798\n",
      "cnt: 27 - val loss: 2.4002126306295395 - train loss: 8.53103007376194\n",
      "cnt: 28 - val loss: 2.508577913045883 - train loss: 8.522670939564705\n",
      "cnt: 29 - val loss: 2.441526472568512 - train loss: 8.533652350306511\n",
      "cnt: 30 - val loss: 2.5129857063293457 - train loss: 8.512955650687218\n",
      "cnt: 31 - val loss: 2.45018707215786 - train loss: 8.531315237283707\n",
      "cnt: 32 - val loss: 2.424474135041237 - train loss: 8.530052855610847\n",
      "cnt: 33 - val loss: 2.4483540803194046 - train loss: 8.513453796505928\n",
      "cnt: 34 - val loss: 2.3850380331277847 - train loss: 8.514687225222588\n",
      "cnt: 35 - val loss: 2.318727731704712 - train loss: 8.516863748431206\n",
      "cnt: 0 - val loss: 2.402407869696617 - train loss: 8.532123744487762\n",
      "cnt: 1 - val loss: 2.430600628256798 - train loss: 8.518185794353485\n",
      "cnt: 2 - val loss: 2.371941402554512 - train loss: 8.51454159617424\n",
      "cnt: 3 - val loss: 2.40311461687088 - train loss: 8.51487486064434\n",
      "cnt: 4 - val loss: 2.4040715247392654 - train loss: 8.507546916604042\n",
      "cnt: 5 - val loss: 2.478555679321289 - train loss: 8.523364543914795\n",
      "cnt: 6 - val loss: 2.4436516016721725 - train loss: 8.510687723755836\n",
      "cnt: 7 - val loss: 2.4741086810827255 - train loss: 8.515581533312798\n",
      "cnt: 8 - val loss: 2.508037954568863 - train loss: 8.51289615035057\n",
      "cnt: 9 - val loss: 2.395018607378006 - train loss: 8.502541050314903\n",
      "cnt: 10 - val loss: 2.3719768673181534 - train loss: 8.516683116555214\n",
      "cnt: 11 - val loss: 2.488764464855194 - train loss: 8.505429148674011\n",
      "cnt: 12 - val loss: 2.443637251853943 - train loss: 8.504194527864456\n",
      "cnt: 13 - val loss: 2.4670735746622086 - train loss: 8.505768179893494\n",
      "cnt: 14 - val loss: 2.423209100961685 - train loss: 8.49960432946682\n",
      "cnt: 15 - val loss: 2.499894440174103 - train loss: 8.505590945482254\n",
      "cnt: 16 - val loss: 2.3948022723197937 - train loss: 8.510103225708008\n",
      "cnt: 17 - val loss: 2.4563417732715607 - train loss: 8.496813327074051\n",
      "cnt: 18 - val loss: 2.396289363503456 - train loss: 8.498564675450325\n",
      "cnt: 19 - val loss: 2.4622447937726974 - train loss: 8.493170484900475\n",
      "cnt: 20 - val loss: 2.481116846203804 - train loss: 8.49126909673214\n",
      "cnt: 21 - val loss: 2.3902316987514496 - train loss: 8.494723603129387\n",
      "cnt: 22 - val loss: 2.4980867356061935 - train loss: 8.511142998933792\n",
      "cnt: 23 - val loss: 2.4149386882781982 - train loss: 8.492044821381569\n",
      "cnt: 24 - val loss: 2.445004492998123 - train loss: 8.489437773823738\n",
      "cnt: 25 - val loss: 2.4708725661039352 - train loss: 8.491711854934692\n",
      "cnt: 26 - val loss: 2.4744337499141693 - train loss: 8.50357310473919\n",
      "cnt: 27 - val loss: 2.4065732210874557 - train loss: 8.50196224451065\n",
      "cnt: 28 - val loss: 2.393086165189743 - train loss: 8.493173032999039\n",
      "cnt: 29 - val loss: 2.3487110882997513 - train loss: 8.489223331212997\n",
      "cnt: 30 - val loss: 2.391828253865242 - train loss: 8.491855174303055\n",
      "cnt: 31 - val loss: 2.3685503751039505 - train loss: 8.502071022987366\n",
      "cnt: 32 - val loss: 2.4044222682714462 - train loss: 8.478848919272423\n",
      "cnt: 33 - val loss: 2.4723742604255676 - train loss: 8.483204439282417\n",
      "cnt: 34 - val loss: 2.407465159893036 - train loss: 8.47912447154522\n",
      "cnt: 35 - val loss: 2.4593274891376495 - train loss: 8.492281347513199\n",
      "cnt: 36 - val loss: 2.404234156012535 - train loss: 8.475359380245209\n",
      "cnt: 37 - val loss: 2.4457706660032272 - train loss: 8.479323416948318\n",
      "cnt: 38 - val loss: 2.4292309880256653 - train loss: 8.483225524425507\n",
      "cnt: 39 - val loss: 2.4318074584007263 - train loss: 8.477348119020462\n",
      "cnt: 40 - val loss: 2.3648664951324463 - train loss: 8.481276959180832\n",
      "cnt: 41 - val loss: 2.395453616976738 - train loss: 8.480370342731476\n",
      "cnt: 42 - val loss: 2.5096729695796967 - train loss: 8.478485748171806\n",
      "cnt: 43 - val loss: 2.3894400894641876 - train loss: 8.47430132329464\n",
      "cnt: 44 - val loss: 2.411016136407852 - train loss: 8.465502008795738\n",
      "cnt: 45 - val loss: 2.3457359820604324 - train loss: 8.474644377827644\n",
      "cnt: 46 - val loss: 2.5024633556604385 - train loss: 8.46833086013794\n",
      "cnt: 47 - val loss: 2.3870954513549805 - train loss: 8.469567000865936\n",
      "cnt: 48 - val loss: 2.378573387861252 - train loss: 8.476905599236488\n",
      "cnt: 49 - val loss: 2.410701885819435 - train loss: 8.454199567437172\n",
      "cnt: 50 - val loss: 2.362687185406685 - train loss: 8.471582800149918\n",
      "cnt: 51 - val loss: 2.399367779493332 - train loss: 8.458289295434952\n",
      "cnt: 52 - val loss: 2.4980834126472473 - train loss: 8.464551910758018\n",
      "cnt: 53 - val loss: 2.481976106762886 - train loss: 8.452770620584488\n",
      "cnt: 54 - val loss: 2.4960352927446365 - train loss: 8.467174351215363\n",
      "cnt: 55 - val loss: 2.4209387004375458 - train loss: 8.450262114405632\n",
      "cnt: 56 - val loss: 2.3616328090429306 - train loss: 8.45073951780796\n",
      "cnt: 57 - val loss: 2.484817236661911 - train loss: 8.45600089430809\n",
      "cnt: 58 - val loss: 2.4612846076488495 - train loss: 8.445454359054565\n",
      "cnt: 59 - val loss: 2.455893963575363 - train loss: 8.455488219857216\n",
      "cnt: 60 - val loss: 2.4711751639842987 - train loss: 8.465073987841606\n",
      "cnt: 61 - val loss: 2.4801166653633118 - train loss: 8.45610298216343\n",
      "cnt: 62 - val loss: 2.4498825520277023 - train loss: 8.451167792081833\n",
      "cnt: 63 - val loss: 2.4603327065706253 - train loss: 8.442954391241074\n",
      "cnt: 64 - val loss: 2.3860657662153244 - train loss: 8.440290167927742\n",
      "cnt: 65 - val loss: 2.4694597125053406 - train loss: 8.447974249720573\n",
      "cnt: 66 - val loss: 2.4815075397491455 - train loss: 8.4479428678751\n",
      "cnt: 67 - val loss: 2.492022067308426 - train loss: 8.436764195561409\n",
      "cnt: 68 - val loss: 2.445128247141838 - train loss: 8.43630763888359\n",
      "cnt: 69 - val loss: 2.3519336730241776 - train loss: 8.440496698021889\n",
      "cnt: 70 - val loss: 2.377916246652603 - train loss: 8.435902759432793\n",
      "cnt: 71 - val loss: 2.3250833600759506 - train loss: 8.433100700378418\n",
      "cnt: 72 - val loss: 2.389614909887314 - train loss: 8.43011225759983\n",
      "cnt: 73 - val loss: 2.5008659809827805 - train loss: 8.43936112523079\n",
      "cnt: 74 - val loss: 2.4395525753498077 - train loss: 8.430832371115685\n",
      "cnt: 75 - val loss: 2.399620294570923 - train loss: 8.435843229293823\n",
      "cnt: 76 - val loss: 2.360102355480194 - train loss: 8.427314952015877\n",
      "cnt: 77 - val loss: 2.4539202451705933 - train loss: 8.422969534993172\n",
      "cnt: 78 - val loss: 2.355670228600502 - train loss: 8.422220557928085\n",
      "cnt: 79 - val loss: 2.431015759706497 - train loss: 8.42384560406208\n",
      "cnt: 80 - val loss: 2.41191728413105 - train loss: 8.43754032254219\n",
      "cnt: 81 - val loss: 2.417434573173523 - train loss: 8.433106645941734\n",
      "cnt: 82 - val loss: 2.4310605973005295 - train loss: 8.422280386090279\n",
      "cnt: 83 - val loss: 2.3806872963905334 - train loss: 8.415255919098854\n",
      "cnt: 84 - val loss: 2.393031507730484 - train loss: 8.417658686637878\n",
      "cnt: 85 - val loss: 2.4097134172916412 - train loss: 8.401201456785202\n",
      "cnt: 86 - val loss: 2.4712851643562317 - train loss: 8.427328258752823\n",
      "cnt: 87 - val loss: 2.404278188943863 - train loss: 8.416966781020164\n",
      "cnt: 88 - val loss: 2.4487049877643585 - train loss: 8.417487919330597\n",
      "cnt: 89 - val loss: 2.4384925067424774 - train loss: 8.418834865093231\n",
      "cnt: 90 - val loss: 2.346530959010124 - train loss: 8.410550281405449\n",
      "cnt: 91 - val loss: 2.4460445046424866 - train loss: 8.41299369931221\n",
      "cnt: 92 - val loss: 2.416949287056923 - train loss: 8.419231295585632\n",
      "cnt: 93 - val loss: 2.4265149235725403 - train loss: 8.407754585146904\n",
      "cnt: 94 - val loss: 2.42416612803936 - train loss: 8.399864748120308\n",
      "cnt: 95 - val loss: 2.4356787353754044 - train loss: 8.39681401848793\n",
      "cnt: 96 - val loss: 2.336205318570137 - train loss: 8.398353978991508\n",
      "cnt: 97 - val loss: 2.399575710296631 - train loss: 8.404500588774681\n",
      "cnt: 98 - val loss: 2.3310753852128983 - train loss: 8.394180789589882\n",
      "cnt: 99 - val loss: 2.4277748316526413 - train loss: 8.40530426800251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14f954f40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainHistory,valHistory,=[],[]\n",
    "previousLoss = float('inf')\n",
    "cnt = -1\n",
    "bestModel = model\n",
    "while(cnt<100):\n",
    "    trainLoss = trainAI(trainLoader, model, loss_fn, optimizer) \n",
    "    valLoss = valAI(valLoader, model, loss_fn)\n",
    "    # scheduler.step()\n",
    "\n",
    "    trainHistory.append(trainLoss)\n",
    "    valHistory.append(valLoss)\n",
    "\n",
    "    print(f'cnt: {cnt} - val loss: {valLoss} - train loss: {trainLoss}')\n",
    "\n",
    "    if cnt<0 or previousLoss < valLoss:\n",
    "        cnt +=1\n",
    "    else:\n",
    "        previousLoss = valLoss\n",
    "        bestModel = model\n",
    "        cnt =0 \n",
    "\n",
    "plt.plot(trainHistory,label='train')\n",
    "plt.plot(valHistory, label='val')\n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2],\n",
       " [2, 0],\n",
       " [3, 8],\n",
       " [4, 7],\n",
       " [5, 2],\n",
       " [6, 7],\n",
       " [7, 0],\n",
       " [8, 3],\n",
       " [9, 0],\n",
       " [10, 3],\n",
       " [11, 5],\n",
       " [12, 7],\n",
       " [13, 3],\n",
       " [14, 0],\n",
       " [15, 4],\n",
       " [16, 3],\n",
       " [17, 3],\n",
       " [18, 1],\n",
       " [19, 9],\n",
       " [20, 0],\n",
       " [21, 9],\n",
       " [22, 1],\n",
       " [23, 1],\n",
       " [24, 5],\n",
       " [25, 7],\n",
       " [26, 4],\n",
       " [27, 2],\n",
       " [28, 7],\n",
       " [29, 9],\n",
       " [30, 7],\n",
       " [31, 7],\n",
       " [32, 2],\n",
       " [33, 4],\n",
       " [34, 2],\n",
       " [35, 6],\n",
       " [36, 2],\n",
       " [37, 0],\n",
       " [38, 5],\n",
       " [39, 1],\n",
       " [40, 6],\n",
       " [41, 7],\n",
       " [42, 7],\n",
       " [43, 4],\n",
       " [44, 9],\n",
       " [45, 8],\n",
       " [46, 7],\n",
       " [47, 8],\n",
       " [48, 2],\n",
       " [49, 8],\n",
       " [50, 7],\n",
       " [51, 6],\n",
       " [52, 8],\n",
       " [53, 8],\n",
       " [54, 3],\n",
       " [55, 8],\n",
       " [56, 2],\n",
       " [57, 1],\n",
       " [58, 2],\n",
       " [59, 2],\n",
       " [60, 5],\n",
       " [61, 4],\n",
       " [62, 1],\n",
       " [63, 7],\n",
       " [64, 0],\n",
       " [65, 0],\n",
       " [66, 0],\n",
       " [67, 1],\n",
       " [68, 9],\n",
       " [69, 0],\n",
       " [70, 1],\n",
       " [71, 6],\n",
       " [72, 5],\n",
       " [73, 8],\n",
       " [74, 8],\n",
       " [75, 2],\n",
       " [76, 8],\n",
       " [77, 3],\n",
       " [78, 9],\n",
       " [79, 2],\n",
       " [80, 3],\n",
       " [81, 5],\n",
       " [82, 9],\n",
       " [83, 1],\n",
       " [84, 0],\n",
       " [85, 9],\n",
       " [86, 2],\n",
       " [87, 4],\n",
       " [88, 3],\n",
       " [89, 6],\n",
       " [90, 7],\n",
       " [91, 2],\n",
       " [92, 0],\n",
       " [93, 6],\n",
       " [94, 6],\n",
       " [95, 1],\n",
       " [96, 4],\n",
       " [97, 3],\n",
       " [98, 9],\n",
       " [99, 7],\n",
       " [100, 4],\n",
       " [101, 0],\n",
       " [102, 3],\n",
       " [103, 2],\n",
       " [104, 0],\n",
       " [105, 7],\n",
       " [106, 3],\n",
       " [107, 0],\n",
       " [108, 5],\n",
       " [109, 0],\n",
       " [110, 9],\n",
       " [111, 0],\n",
       " [112, 0],\n",
       " [113, 6],\n",
       " [114, 7],\n",
       " [115, 1],\n",
       " [116, 7],\n",
       " [117, 1],\n",
       " [118, 1],\n",
       " [119, 3],\n",
       " [120, 3],\n",
       " [121, 3],\n",
       " [122, 7],\n",
       " [123, 2],\n",
       " [124, 8],\n",
       " [125, 6],\n",
       " [126, 3],\n",
       " [127, 8],\n",
       " [128, 7],\n",
       " [129, 8],\n",
       " [130, 4],\n",
       " [131, 3],\n",
       " [132, 5],\n",
       " [133, 6],\n",
       " [134, 0],\n",
       " [135, 0],\n",
       " [136, 0],\n",
       " [137, 3],\n",
       " [138, 1],\n",
       " [139, 3],\n",
       " [140, 6],\n",
       " [141, 4],\n",
       " [142, 3],\n",
       " [143, 4],\n",
       " [144, 5],\n",
       " [145, 5],\n",
       " [146, 8],\n",
       " [147, 7],\n",
       " [148, 7],\n",
       " [149, 2],\n",
       " [150, 8],\n",
       " [151, 4],\n",
       " [152, 3],\n",
       " [153, 5],\n",
       " [154, 6],\n",
       " [155, 5],\n",
       " [156, 3],\n",
       " [157, 7],\n",
       " [158, 5],\n",
       " [159, 7],\n",
       " [160, 8],\n",
       " [161, 3],\n",
       " [162, 0],\n",
       " [163, 4],\n",
       " [164, 5],\n",
       " [165, 1],\n",
       " [166, 3],\n",
       " [167, 7],\n",
       " [168, 6],\n",
       " [169, 5],\n",
       " [170, 0],\n",
       " [171, 3],\n",
       " [172, 7],\n",
       " [173, 8],\n",
       " [174, 6],\n",
       " [175, 1],\n",
       " [176, 3],\n",
       " [177, 7],\n",
       " [178, 4],\n",
       " [179, 1],\n",
       " [180, 2],\n",
       " [181, 4],\n",
       " [182, 7],\n",
       " [183, 5],\n",
       " [184, 2],\n",
       " [185, 4],\n",
       " [186, 9],\n",
       " [187, 2],\n",
       " [188, 1],\n",
       " [189, 6],\n",
       " [190, 0],\n",
       " [191, 6],\n",
       " [192, 1],\n",
       " [193, 4],\n",
       " [194, 9],\n",
       " [195, 6],\n",
       " [196, 0],\n",
       " [197, 9],\n",
       " [198, 7],\n",
       " [199, 6],\n",
       " [200, 9],\n",
       " [201, 1],\n",
       " [202, 9],\n",
       " [203, 0],\n",
       " [204, 9],\n",
       " [205, 9],\n",
       " [206, 0],\n",
       " [207, 8],\n",
       " [208, 4],\n",
       " [209, 6],\n",
       " [210, 2],\n",
       " [211, 0],\n",
       " [212, 9],\n",
       " [213, 3],\n",
       " [214, 6],\n",
       " [215, 9],\n",
       " [216, 2],\n",
       " [217, 1],\n",
       " [218, 6],\n",
       " [219, 3],\n",
       " [220, 4],\n",
       " [221, 2],\n",
       " [222, 3],\n",
       " [223, 1],\n",
       " [224, 2],\n",
       " [225, 2],\n",
       " [226, 0],\n",
       " [227, 4],\n",
       " [228, 6],\n",
       " [229, 1],\n",
       " [230, 0],\n",
       " [231, 0],\n",
       " [232, 4],\n",
       " [233, 9],\n",
       " [234, 1],\n",
       " [235, 7],\n",
       " [236, 3],\n",
       " [237, 2],\n",
       " [238, 2],\n",
       " [239, 8],\n",
       " [240, 6],\n",
       " [241, 8],\n",
       " [242, 6],\n",
       " [243, 2],\n",
       " [244, 8],\n",
       " [245, 5],\n",
       " [246, 5],\n",
       " [247, 9],\n",
       " [248, 8],\n",
       " [249, 3],\n",
       " [250, 8],\n",
       " [251, 9],\n",
       " [252, 7],\n",
       " [253, 1],\n",
       " [254, 3],\n",
       " [255, 8],\n",
       " [256, 4],\n",
       " [257, 5],\n",
       " [258, 1],\n",
       " [259, 4],\n",
       " [260, 3],\n",
       " [261, 6],\n",
       " [262, 3],\n",
       " [263, 8],\n",
       " [264, 5],\n",
       " [265, 7],\n",
       " [266, 0],\n",
       " [267, 6],\n",
       " [268, 8],\n",
       " [269, 6],\n",
       " [270, 1],\n",
       " [271, 6],\n",
       " [272, 0],\n",
       " [273, 6],\n",
       " [274, 3],\n",
       " [275, 9],\n",
       " [276, 9],\n",
       " [277, 1],\n",
       " [278, 5],\n",
       " [279, 8],\n",
       " [280, 4],\n",
       " [281, 0],\n",
       " [282, 9],\n",
       " [283, 2],\n",
       " [284, 0],\n",
       " [285, 5],\n",
       " [286, 3],\n",
       " [287, 7],\n",
       " [288, 8],\n",
       " [289, 9],\n",
       " [290, 9],\n",
       " [291, 5],\n",
       " [292, 7],\n",
       " [293, 7],\n",
       " [294, 9],\n",
       " [295, 9],\n",
       " [296, 6],\n",
       " [297, 3],\n",
       " [298, 0],\n",
       " [299, 3],\n",
       " [300, 3],\n",
       " [301, 6],\n",
       " [302, 4],\n",
       " [303, 8],\n",
       " [304, 2],\n",
       " [305, 6],\n",
       " [306, 3],\n",
       " [307, 7],\n",
       " [308, 1],\n",
       " [309, 4],\n",
       " [310, 5],\n",
       " [311, 8],\n",
       " [312, 5],\n",
       " [313, 4],\n",
       " [314, 0],\n",
       " [315, 0],\n",
       " [316, 3],\n",
       " [317, 8],\n",
       " [318, 4],\n",
       " [319, 1],\n",
       " [320, 8],\n",
       " [321, 4],\n",
       " [322, 1],\n",
       " [323, 1],\n",
       " [324, 9],\n",
       " [325, 8],\n",
       " [326, 4],\n",
       " [327, 5],\n",
       " [328, 1],\n",
       " [329, 5],\n",
       " [330, 7],\n",
       " [331, 6],\n",
       " [332, 3],\n",
       " [333, 1],\n",
       " [334, 2],\n",
       " [335, 0],\n",
       " [336, 9],\n",
       " [337, 0],\n",
       " [338, 0],\n",
       " [339, 6],\n",
       " [340, 0],\n",
       " [341, 6],\n",
       " [342, 7],\n",
       " [343, 1],\n",
       " [344, 8],\n",
       " [345, 6],\n",
       " [346, 0],\n",
       " [347, 6],\n",
       " [348, 5],\n",
       " [349, 2],\n",
       " [350, 2],\n",
       " [351, 6],\n",
       " [352, 7],\n",
       " [353, 7],\n",
       " [354, 2],\n",
       " [355, 5],\n",
       " [356, 8],\n",
       " [357, 8],\n",
       " [358, 9],\n",
       " [359, 2],\n",
       " [360, 7],\n",
       " [361, 8],\n",
       " [362, 6],\n",
       " [363, 3],\n",
       " [364, 8],\n",
       " [365, 9],\n",
       " [366, 2],\n",
       " [367, 3],\n",
       " [368, 8],\n",
       " [369, 1],\n",
       " [370, 6],\n",
       " [371, 4],\n",
       " [372, 8],\n",
       " [373, 9],\n",
       " [374, 9],\n",
       " [375, 7],\n",
       " [376, 6],\n",
       " [377, 9],\n",
       " [378, 5],\n",
       " [379, 3],\n",
       " [380, 7],\n",
       " [381, 6],\n",
       " [382, 5],\n",
       " [383, 5],\n",
       " [384, 9],\n",
       " [385, 2],\n",
       " [386, 6],\n",
       " [387, 2],\n",
       " [388, 1],\n",
       " [389, 3],\n",
       " [390, 7],\n",
       " [391, 1],\n",
       " [392, 7],\n",
       " [393, 9],\n",
       " [394, 9],\n",
       " [395, 6],\n",
       " [396, 1],\n",
       " [397, 1],\n",
       " [398, 1],\n",
       " [399, 7],\n",
       " [400, 3],\n",
       " [401, 9],\n",
       " [402, 7],\n",
       " [403, 6],\n",
       " [404, 1],\n",
       " [405, 1],\n",
       " [406, 1],\n",
       " [407, 9],\n",
       " [408, 3],\n",
       " [409, 3],\n",
       " [410, 5],\n",
       " [411, 5],\n",
       " [412, 0],\n",
       " [413, 4],\n",
       " [414, 1],\n",
       " [415, 2],\n",
       " [416, 3],\n",
       " [417, 1],\n",
       " [418, 1],\n",
       " [419, 3],\n",
       " [420, 8],\n",
       " [421, 9],\n",
       " [422, 6],\n",
       " [423, 6],\n",
       " [424, 5],\n",
       " [425, 3],\n",
       " [426, 1],\n",
       " [427, 4],\n",
       " [428, 7],\n",
       " [429, 7],\n",
       " [430, 7],\n",
       " [431, 4],\n",
       " [432, 8],\n",
       " [433, 5],\n",
       " [434, 2],\n",
       " [435, 6],\n",
       " [436, 1],\n",
       " [437, 3],\n",
       " [438, 9],\n",
       " [439, 5],\n",
       " [440, 0],\n",
       " [441, 8],\n",
       " [442, 4],\n",
       " [443, 7],\n",
       " [444, 4],\n",
       " [445, 4],\n",
       " [446, 4],\n",
       " [447, 1],\n",
       " [448, 5],\n",
       " [449, 3],\n",
       " [450, 9],\n",
       " [451, 5],\n",
       " [452, 7],\n",
       " [453, 6],\n",
       " [454, 9],\n",
       " [455, 5],\n",
       " [456, 9],\n",
       " [457, 2],\n",
       " [458, 3],\n",
       " [459, 7],\n",
       " [460, 8],\n",
       " [461, 6],\n",
       " [462, 7],\n",
       " [463, 5],\n",
       " [464, 0],\n",
       " [465, 5],\n",
       " [466, 1],\n",
       " [467, 7],\n",
       " [468, 4],\n",
       " [469, 4],\n",
       " [470, 1],\n",
       " [471, 1],\n",
       " [472, 4],\n",
       " [473, 9],\n",
       " [474, 5],\n",
       " [475, 6],\n",
       " [476, 0],\n",
       " [477, 1],\n",
       " [478, 3],\n",
       " [479, 1],\n",
       " [480, 0],\n",
       " [481, 4],\n",
       " [482, 8],\n",
       " [483, 1],\n",
       " [484, 2],\n",
       " [485, 9],\n",
       " [486, 9],\n",
       " [487, 9],\n",
       " [488, 8],\n",
       " [489, 3],\n",
       " [490, 7],\n",
       " [491, 7],\n",
       " [492, 4],\n",
       " [493, 2],\n",
       " [494, 4],\n",
       " [495, 6],\n",
       " [496, 7],\n",
       " [497, 5],\n",
       " [498, 3],\n",
       " [499, 2],\n",
       " [500, 0],\n",
       " [501, 6],\n",
       " [502, 5],\n",
       " [503, 9],\n",
       " [504, 4],\n",
       " [505, 1],\n",
       " [506, 8],\n",
       " [507, 3],\n",
       " [508, 3],\n",
       " [509, 0],\n",
       " [510, 6],\n",
       " [511, 7],\n",
       " [512, 5],\n",
       " [513, 8],\n",
       " [514, 7],\n",
       " [515, 3],\n",
       " [516, 3],\n",
       " [517, 1],\n",
       " [518, 7],\n",
       " [519, 6],\n",
       " [520, 3],\n",
       " [521, 2],\n",
       " [522, 9],\n",
       " [523, 0],\n",
       " [524, 7],\n",
       " [525, 7],\n",
       " [526, 1],\n",
       " [527, 0],\n",
       " [528, 1],\n",
       " [529, 1],\n",
       " [530, 7],\n",
       " [531, 0],\n",
       " [532, 5],\n",
       " [533, 3],\n",
       " [534, 8],\n",
       " [535, 3],\n",
       " [536, 5],\n",
       " [537, 6],\n",
       " [538, 5],\n",
       " [539, 4],\n",
       " [540, 3],\n",
       " [541, 8],\n",
       " [542, 2],\n",
       " [543, 8],\n",
       " [544, 2],\n",
       " [545, 0],\n",
       " [546, 3],\n",
       " [547, 0],\n",
       " [548, 9],\n",
       " [549, 2],\n",
       " [550, 1],\n",
       " [551, 1],\n",
       " [552, 3],\n",
       " [553, 0],\n",
       " [554, 5],\n",
       " [555, 0],\n",
       " [556, 0],\n",
       " [557, 7],\n",
       " [558, 5],\n",
       " [559, 6],\n",
       " [560, 2],\n",
       " [561, 0],\n",
       " [562, 3],\n",
       " [563, 8],\n",
       " [564, 1],\n",
       " [565, 6],\n",
       " [566, 5],\n",
       " [567, 4],\n",
       " [568, 1],\n",
       " [569, 1],\n",
       " [570, 4],\n",
       " [571, 2],\n",
       " [572, 5],\n",
       " [573, 3],\n",
       " [574, 6],\n",
       " [575, 0],\n",
       " [576, 4],\n",
       " [577, 8],\n",
       " [578, 2],\n",
       " [579, 4],\n",
       " [580, 2],\n",
       " [581, 5],\n",
       " [582, 1],\n",
       " [583, 2],\n",
       " [584, 6],\n",
       " [585, 9],\n",
       " [586, 1],\n",
       " [587, 7],\n",
       " [588, 5],\n",
       " [589, 8],\n",
       " [590, 0],\n",
       " [591, 8],\n",
       " [592, 8],\n",
       " [593, 4],\n",
       " [594, 5],\n",
       " [595, 2],\n",
       " [596, 6],\n",
       " [597, 6],\n",
       " [598, 6],\n",
       " [599, 0],\n",
       " [600, 3],\n",
       " [601, 5],\n",
       " [602, 1],\n",
       " [603, 7],\n",
       " [604, 1],\n",
       " [605, 6],\n",
       " [606, 2],\n",
       " [607, 8],\n",
       " [608, 5],\n",
       " [609, 6],\n",
       " [610, 4],\n",
       " [611, 7],\n",
       " [612, 4],\n",
       " [613, 3],\n",
       " [614, 3],\n",
       " [615, 2],\n",
       " [616, 4],\n",
       " [617, 7],\n",
       " [618, 0],\n",
       " [619, 0],\n",
       " [620, 9],\n",
       " [621, 8],\n",
       " [622, 5],\n",
       " [623, 9],\n",
       " [624, 4],\n",
       " [625, 0],\n",
       " [626, 8],\n",
       " [627, 3],\n",
       " [628, 3],\n",
       " [629, 6],\n",
       " [630, 7],\n",
       " [631, 6],\n",
       " [632, 1],\n",
       " [633, 8],\n",
       " [634, 6],\n",
       " [635, 1],\n",
       " [636, 4],\n",
       " [637, 7],\n",
       " [638, 7],\n",
       " [639, 8],\n",
       " [640, 3],\n",
       " [641, 0],\n",
       " [642, 9],\n",
       " [643, 9],\n",
       " [644, 6],\n",
       " [645, 7],\n",
       " [646, 2],\n",
       " [647, 4],\n",
       " [648, 4],\n",
       " [649, 1],\n",
       " [650, 8],\n",
       " [651, 4],\n",
       " [652, 8],\n",
       " [653, 0],\n",
       " [654, 2],\n",
       " [655, 8],\n",
       " [656, 2],\n",
       " [657, 4],\n",
       " [658, 3],\n",
       " [659, 3],\n",
       " [660, 7],\n",
       " [661, 2],\n",
       " [662, 3],\n",
       " [663, 4],\n",
       " [664, 0],\n",
       " [665, 9],\n",
       " [666, 8],\n",
       " [667, 1],\n",
       " [668, 3],\n",
       " [669, 5],\n",
       " [670, 6],\n",
       " [671, 3],\n",
       " [672, 9],\n",
       " [673, 4],\n",
       " [674, 3],\n",
       " [675, 8],\n",
       " [676, 7],\n",
       " [677, 7],\n",
       " [678, 2],\n",
       " [679, 6],\n",
       " [680, 0],\n",
       " [681, 6],\n",
       " [682, 9],\n",
       " [683, 8],\n",
       " [684, 1],\n",
       " [685, 1],\n",
       " [686, 3],\n",
       " [687, 4],\n",
       " [688, 6],\n",
       " [689, 9],\n",
       " [690, 9],\n",
       " [691, 2],\n",
       " [692, 6],\n",
       " [693, 0],\n",
       " [694, 1],\n",
       " [695, 8],\n",
       " [696, 4],\n",
       " [697, 3],\n",
       " [698, 9],\n",
       " [699, 8],\n",
       " [700, 8],\n",
       " [701, 4],\n",
       " [702, 0],\n",
       " [703, 8],\n",
       " [704, 0],\n",
       " [705, 6],\n",
       " [706, 0],\n",
       " [707, 9],\n",
       " [708, 4],\n",
       " [709, 6],\n",
       " [710, 5],\n",
       " [711, 1],\n",
       " [712, 8],\n",
       " [713, 1],\n",
       " [714, 5],\n",
       " [715, 3],\n",
       " [716, 6],\n",
       " [717, 2],\n",
       " [718, 3],\n",
       " [719, 7],\n",
       " [720, 8],\n",
       " [721, 9],\n",
       " [722, 3],\n",
       " [723, 1],\n",
       " [724, 0],\n",
       " [725, 1],\n",
       " [726, 0],\n",
       " [727, 6],\n",
       " [728, 4],\n",
       " [729, 7],\n",
       " [730, 5],\n",
       " [731, 7],\n",
       " [732, 1],\n",
       " [733, 3],\n",
       " [734, 2],\n",
       " [735, 7],\n",
       " [736, 7],\n",
       " [737, 1],\n",
       " [738, 5],\n",
       " [739, 1],\n",
       " [740, 5],\n",
       " [741, 4],\n",
       " [742, 4],\n",
       " [743, 3],\n",
       " [744, 4],\n",
       " [745, 3],\n",
       " [746, 9],\n",
       " [747, 0],\n",
       " [748, 7],\n",
       " [749, 8],\n",
       " [750, 6],\n",
       " [751, 4],\n",
       " [752, 9],\n",
       " [753, 4],\n",
       " [754, 4],\n",
       " [755, 7],\n",
       " [756, 4],\n",
       " [757, 7],\n",
       " [758, 1],\n",
       " [759, 1],\n",
       " [760, 8],\n",
       " [761, 7],\n",
       " [762, 0],\n",
       " [763, 4],\n",
       " [764, 0],\n",
       " [765, 4],\n",
       " [766, 0],\n",
       " [767, 0],\n",
       " [768, 5],\n",
       " [769, 1],\n",
       " [770, 8],\n",
       " [771, 6],\n",
       " [772, 5],\n",
       " [773, 0],\n",
       " [774, 1],\n",
       " [775, 4],\n",
       " [776, 3],\n",
       " [777, 4],\n",
       " [778, 6],\n",
       " [779, 3],\n",
       " [780, 1],\n",
       " [781, 1],\n",
       " [782, 6],\n",
       " [783, 4],\n",
       " [784, 8],\n",
       " [785, 3],\n",
       " [786, 5],\n",
       " [787, 3],\n",
       " [788, 4],\n",
       " [789, 9],\n",
       " [790, 5],\n",
       " [791, 5],\n",
       " [792, 0],\n",
       " [793, 4],\n",
       " [794, 0],\n",
       " [795, 4],\n",
       " [796, 3],\n",
       " [797, 1],\n",
       " [798, 6],\n",
       " [799, 9],\n",
       " [800, 7],\n",
       " [801, 1],\n",
       " [802, 1],\n",
       " [803, 3],\n",
       " [804, 3],\n",
       " [805, 1],\n",
       " [806, 4],\n",
       " [807, 9],\n",
       " [808, 6],\n",
       " [809, 9],\n",
       " [810, 1],\n",
       " [811, 5],\n",
       " [812, 4],\n",
       " [813, 2],\n",
       " [814, 3],\n",
       " [815, 2],\n",
       " [816, 4],\n",
       " [817, 0],\n",
       " [818, 9],\n",
       " [819, 7],\n",
       " [820, 5],\n",
       " [821, 3],\n",
       " [822, 0],\n",
       " [823, 5],\n",
       " [824, 0],\n",
       " [825, 1],\n",
       " [826, 9],\n",
       " [827, 0],\n",
       " [828, 4],\n",
       " [829, 5],\n",
       " [830, 2],\n",
       " [831, 8],\n",
       " [832, 5],\n",
       " [833, 5],\n",
       " [834, 9],\n",
       " [835, 3],\n",
       " [836, 9],\n",
       " [837, 6],\n",
       " [838, 1],\n",
       " [839, 5],\n",
       " [840, 1],\n",
       " [841, 1],\n",
       " [842, 9],\n",
       " [843, 0],\n",
       " [844, 8],\n",
       " [845, 4],\n",
       " [846, 6],\n",
       " [847, 7],\n",
       " [848, 2],\n",
       " [849, 3],\n",
       " [850, 5],\n",
       " [851, 8],\n",
       " [852, 9],\n",
       " [853, 9],\n",
       " [854, 7],\n",
       " [855, 2],\n",
       " [856, 8],\n",
       " [857, 1],\n",
       " [858, 3],\n",
       " [859, 6],\n",
       " [860, 5],\n",
       " [861, 0],\n",
       " [862, 4],\n",
       " [863, 1],\n",
       " [864, 4],\n",
       " [865, 2],\n",
       " [866, 3],\n",
       " [867, 6],\n",
       " [868, 9],\n",
       " [869, 2],\n",
       " [870, 3],\n",
       " [871, 4],\n",
       " [872, 5],\n",
       " [873, 4],\n",
       " [874, 3],\n",
       " [875, 3],\n",
       " [876, 9],\n",
       " [877, 1],\n",
       " [878, 1],\n",
       " [879, 0],\n",
       " [880, 1],\n",
       " [881, 4],\n",
       " [882, 9],\n",
       " [883, 1],\n",
       " [884, 1],\n",
       " [885, 2],\n",
       " [886, 7],\n",
       " [887, 1],\n",
       " [888, 5],\n",
       " [889, 4],\n",
       " [890, 9],\n",
       " [891, 1],\n",
       " [892, 7],\n",
       " [893, 6],\n",
       " [894, 0],\n",
       " [895, 4],\n",
       " [896, 2],\n",
       " [897, 9],\n",
       " [898, 4],\n",
       " [899, 1],\n",
       " [900, 1],\n",
       " [901, 5],\n",
       " [902, 3],\n",
       " [903, 5],\n",
       " [904, 7],\n",
       " [905, 9],\n",
       " [906, 7],\n",
       " [907, 8],\n",
       " [908, 3],\n",
       " [909, 2],\n",
       " [910, 7],\n",
       " [911, 2],\n",
       " [912, 0],\n",
       " [913, 4],\n",
       " [914, 7],\n",
       " [915, 1],\n",
       " [916, 6],\n",
       " [917, 4],\n",
       " [918, 5],\n",
       " [919, 1],\n",
       " [920, 5],\n",
       " [921, 7],\n",
       " [922, 3],\n",
       " [923, 6],\n",
       " [924, 7],\n",
       " [925, 4],\n",
       " [926, 7],\n",
       " [927, 9],\n",
       " [928, 6],\n",
       " [929, 6],\n",
       " [930, 3],\n",
       " [931, 3],\n",
       " [932, 2],\n",
       " [933, 1],\n",
       " [934, 4],\n",
       " [935, 5],\n",
       " [936, 3],\n",
       " [937, 7],\n",
       " [938, 7],\n",
       " [939, 9],\n",
       " [940, 5],\n",
       " [941, 6],\n",
       " [942, 0],\n",
       " [943, 6],\n",
       " [944, 1],\n",
       " [945, 0],\n",
       " [946, 9],\n",
       " [947, 3],\n",
       " [948, 2],\n",
       " [949, 9],\n",
       " [950, 2],\n",
       " [951, 6],\n",
       " [952, 7],\n",
       " [953, 5],\n",
       " [954, 2],\n",
       " [955, 3],\n",
       " [956, 2],\n",
       " [957, 8],\n",
       " [958, 3],\n",
       " [959, 6],\n",
       " [960, 2],\n",
       " [961, 7],\n",
       " [962, 9],\n",
       " [963, 4],\n",
       " [964, 0],\n",
       " [965, 9],\n",
       " [966, 5],\n",
       " [967, 1],\n",
       " [968, 8],\n",
       " [969, 8],\n",
       " [970, 5],\n",
       " [971, 3],\n",
       " [972, 2],\n",
       " [973, 9],\n",
       " [974, 6],\n",
       " [975, 7],\n",
       " [976, 0],\n",
       " [977, 8],\n",
       " [978, 0],\n",
       " [979, 7],\n",
       " [980, 4],\n",
       " [981, 5],\n",
       " [982, 8],\n",
       " [983, 7],\n",
       " [984, 9],\n",
       " [985, 7],\n",
       " [986, 7],\n",
       " [987, 0],\n",
       " [988, 5],\n",
       " [989, 3],\n",
       " [990, 2],\n",
       " [991, 1],\n",
       " [992, 9],\n",
       " [993, 0],\n",
       " [994, 6],\n",
       " [995, 8],\n",
       " [996, 3],\n",
       " [997, 6],\n",
       " [998, 2],\n",
       " [999, 2],\n",
       " [1000, 9],\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testAI(dataloader, model):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    y=1\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            X  = X.to(device)\n",
    "            pred = model(X)\n",
    "            for i in pred:\n",
    "                out.append([y, torch.argmax(i).item()])\n",
    "                y+=1\n",
    "\n",
    "    \n",
    "    return out\n",
    "\n",
    "result = testAI(testLoader, bestModel) \n",
    "# result = testAI(testLoader[0], model[0])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      8\n",
       "3            4      7\n",
       "4            5      2\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outDF= pd.DataFrame(result)\n",
    "outDF= outDF.astype(int)\n",
    "outDF.columns=['ImageId','Label']\n",
    "outDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDF.to_csv('result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
