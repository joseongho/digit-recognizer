{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n"
     ]
    }
   ],
   "source": [
    "trainDF = pd.read_csv('train.csv')\n",
    "trainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28000 entries, 0 to 27999\n",
      "Columns: 784 entries, pixel0 to pixel783\n",
      "dtypes: int64(784)\n",
      "memory usage: 167.5 MB\n"
     ]
    }
   ],
   "source": [
    "testDF = pd.read_csv('test.csv')\n",
    "testDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 70000 entries, 0 to 27999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: float64(1), int64(784)\n",
      "memory usage: 419.8 MB\n"
     ]
    }
   ],
   "source": [
    "concatDF = pd.concat([trainDF,testDF])\n",
    "concatDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 188., 255.,\n",
       "           94.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 191., 250., 253.,\n",
       "           93.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0., 123., 248., 253., 167.,\n",
       "           10.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,  80., 247., 253., 208.,  13.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,  29., 207., 253., 235.,  77.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,  54., 209., 253., 253.,  88.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,  93., 254., 253., 238., 170.,  17.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,  23., 210., 254., 253., 159.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,  16., 209., 253., 254., 240.,  81.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,  27., 253., 253., 254.,  13.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           20., 206., 254., 254., 198.,   7.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          168., 253., 253., 196.,   7.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  20.,\n",
       "          203., 253., 248.,  76.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  22., 188.,\n",
       "          253., 245.,  93.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 103., 253.,\n",
       "          253., 191.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  89., 240., 253.,\n",
       "          195.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  15., 220., 253., 253.,\n",
       "           80.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  94., 253., 253., 253.,\n",
       "           94.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  89., 251., 253., 250.,\n",
       "          131.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 214., 218.,  95.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.]]])"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, DF):\n",
    "        if 'label' in DF.columns:\n",
    "            self.label = pd.get_dummies(DF['label']).values\n",
    "            DF = DF.drop(columns=['label'])\n",
    "        self.data = DF.values\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x=self.data[idx].reshape(1,28,28)\n",
    "        x=torch.FloatTensor(x)\n",
    "\n",
    "        if hasattr(self,'label'):\n",
    "            y=self.label[idx]\n",
    "            y=torch.FloatTensor(y)\n",
    "\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "dataSet= MyDataset(DF=trainDF)\n",
    "testSet = MyDataset(DF= testDF)\n",
    "dataSet[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataset.Subset at 0x383bd5ac0>,\n",
       " <torch.utils.data.dataset.Subset at 0x383bd53a0>]"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitSet = torch.utils.data.random_split(dataSet,(0.8,0.2))\n",
    "splitSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x390defa00>"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader=torch.utils.data.DataLoader(splitSet[0],batch_size=2048,sampler=torch.utils.data.RandomSampler(splitSet[0]))\n",
    "valLoader=torch.utils.data.DataLoader(splitSet[1],batch_size=2048,sampler=torch.utils.data.RandomSampler(splitSet[1]))\n",
    "testLoader = torch.utils.data.DataLoader(testSet,batch_size=2048)\n",
    "trainLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x384286790>"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainLoader)\n",
    "# images, labels = next(dataiter)\n",
    "images, label = next(dataiter)\n",
    "\n",
    "# show images\n",
    "plt.imshow(images[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convStack = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 1, 3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "            torch.nn.Conv2d(1, 1, 3),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.MaxPool2d(2, 2),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(121, 60),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(60, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.convStack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAI(dataLoader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    trainLoss=0\n",
    "    for  X, y in dataLoader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        trainLoss +=loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return trainLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valAI(dataLoader, model, loss_fn,):\n",
    "    model.eval()\n",
    "    valLoss=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataLoader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            valLoss+=loss.item()\n",
    "    \n",
    "    return valLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - train loss: 4.263714239001274 - val loss: 1.4076517820358276\n",
      "epoch: 1 - train loss: 4.257432505488396 - val loss: 1.288486272096634\n",
      "epoch: 2 - train loss: 4.252646505832672 - val loss: 1.3551431447267532\n",
      "epoch: 3 - train loss: 4.215101137757301 - val loss: 1.2997724413871765\n",
      "epoch: 4 - train loss: 4.223192542791367 - val loss: 1.3383262157440186\n",
      "epoch: 5 - train loss: 4.178636208176613 - val loss: 1.285864219069481\n",
      "epoch: 6 - train loss: 4.159013092517853 - val loss: 1.2436471283435822\n",
      "epoch: 7 - train loss: 4.145845115184784 - val loss: 1.3396171629428864\n",
      "epoch: 8 - train loss: 4.137630686163902 - val loss: 1.37748184800148\n",
      "epoch: 9 - train loss: 4.108215108513832 - val loss: 1.289389580488205\n",
      "epoch: 10 - train loss: 4.142012819647789 - val loss: 1.281483992934227\n",
      "epoch: 11 - train loss: 4.104003936052322 - val loss: 1.3033248037099838\n",
      "epoch: 12 - train loss: 4.095970839262009 - val loss: 1.2369097620248795\n",
      "epoch: 13 - train loss: 4.042378917336464 - val loss: 1.2992754876613617\n",
      "epoch: 14 - train loss: 4.045068562030792 - val loss: 1.283673033118248\n",
      "epoch: 15 - train loss: 4.0364285707473755 - val loss: 1.3003955036401749\n",
      "epoch: 16 - train loss: 4.025880455970764 - val loss: 1.245683714747429\n",
      "epoch: 17 - train loss: 4.003814205527306 - val loss: 1.323217198252678\n",
      "epoch: 18 - train loss: 4.010799542069435 - val loss: 1.3305373936891556\n",
      "epoch: 19 - train loss: 3.987984374165535 - val loss: 1.1899233311414719\n",
      "epoch: 20 - train loss: 3.9690721184015274 - val loss: 1.3274263143539429\n",
      "epoch: 21 - train loss: 3.968775227665901 - val loss: 1.2841403037309647\n",
      "epoch: 22 - train loss: 3.9562654942274094 - val loss: 1.36364284157753\n",
      "epoch: 23 - train loss: 3.9370070695877075 - val loss: 1.3132651299238205\n",
      "epoch: 24 - train loss: 3.9415666610002518 - val loss: 1.3222861140966415\n",
      "epoch: 25 - train loss: 3.908601611852646 - val loss: 1.2306845933198929\n",
      "epoch: 26 - train loss: 3.8882103264331818 - val loss: 1.2800216525793076\n",
      "epoch: 27 - train loss: 3.8850335776805878 - val loss: 1.1420617327094078\n",
      "epoch: 28 - train loss: 3.898391991853714 - val loss: 1.2210520952939987\n",
      "epoch: 29 - train loss: 3.849080353975296 - val loss: 1.251401886343956\n",
      "epoch: 30 - train loss: 3.8414201587438583 - val loss: 1.3546239733695984\n",
      "epoch: 31 - train loss: 3.8530524522066116 - val loss: 1.2706050425767899\n",
      "epoch: 32 - train loss: 3.8311930000782013 - val loss: 1.3462932705879211\n",
      "epoch: 33 - train loss: 3.785137802362442 - val loss: 1.1715275645256042\n",
      "epoch: 34 - train loss: 3.81912699341774 - val loss: 1.2914267927408218\n",
      "epoch: 35 - train loss: 3.7844400852918625 - val loss: 1.234253153204918\n",
      "epoch: 36 - train loss: 3.7792693078517914 - val loss: 1.2295545488595963\n",
      "epoch: 37 - train loss: 3.7613099217414856 - val loss: 1.1345025449991226\n",
      "epoch: 38 - train loss: 3.7738253325223923 - val loss: 1.1619754135608673\n",
      "epoch: 39 - train loss: 3.7656960785388947 - val loss: 1.1621623039245605\n",
      "epoch: 40 - train loss: 3.758489802479744 - val loss: 1.2274125218391418\n",
      "epoch: 41 - train loss: 3.7267071902751923 - val loss: 1.314434751868248\n",
      "epoch: 42 - train loss: 3.7351552098989487 - val loss: 1.1666145473718643\n",
      "epoch: 43 - train loss: 3.7166247963905334 - val loss: 1.280937671661377\n",
      "epoch: 44 - train loss: 3.691355809569359 - val loss: 1.2394472509622574\n",
      "epoch: 45 - train loss: 3.677475869655609 - val loss: 1.2598682940006256\n",
      "epoch: 46 - train loss: 3.673502638936043 - val loss: 1.227312833070755\n",
      "epoch: 47 - train loss: 3.655907228589058 - val loss: 1.1596701592206955\n",
      "epoch: 48 - train loss: 3.661239340901375 - val loss: 1.2339809238910675\n",
      "epoch: 49 - train loss: 3.6652812361717224 - val loss: 1.1844543814659119\n",
      "epoch: 50 - train loss: 3.643357053399086 - val loss: 1.2925564646720886\n",
      "epoch: 51 - train loss: 3.647269383072853 - val loss: 1.2298996597528458\n",
      "epoch: 52 - train loss: 3.6408004611730576 - val loss: 1.1436550617218018\n",
      "epoch: 53 - train loss: 3.6055052429437637 - val loss: 1.2826236337423325\n",
      "epoch: 54 - train loss: 3.587042063474655 - val loss: 1.2311693578958511\n",
      "epoch: 55 - train loss: 3.6007267385721207 - val loss: 1.3090275526046753\n",
      "epoch: 56 - train loss: 3.6099154502153397 - val loss: 1.205070823431015\n",
      "epoch: 57 - train loss: 3.5895320922136307 - val loss: 1.2128988951444626\n",
      "epoch: 58 - train loss: 3.5392084568738937 - val loss: 1.1636903434991837\n",
      "epoch: 59 - train loss: 3.570347860455513 - val loss: 1.1474061161279678\n",
      "epoch: 60 - train loss: 3.544645294547081 - val loss: 1.1808831095695496\n",
      "epoch: 61 - train loss: 3.5532133877277374 - val loss: 1.2882619351148605\n",
      "epoch: 62 - train loss: 3.548413649201393 - val loss: 1.1562878489494324\n",
      "epoch: 63 - train loss: 3.525051474571228 - val loss: 1.1716033965349197\n",
      "epoch: 64 - train loss: 3.4844873547554016 - val loss: 1.1480364203453064\n",
      "epoch: 65 - train loss: 3.5035339444875717 - val loss: 1.1934130936861038\n",
      "epoch: 66 - train loss: 3.4944509267807007 - val loss: 1.1777874827384949\n",
      "epoch: 67 - train loss: 3.520246833562851 - val loss: 1.1542832553386688\n",
      "epoch: 68 - train loss: 3.4921045154333115 - val loss: 1.1973558366298676\n",
      "epoch: 69 - train loss: 3.4670057892799377 - val loss: 1.1115220040082932\n",
      "epoch: 70 - train loss: 3.4388371855020523 - val loss: 1.254997730255127\n",
      "epoch: 71 - train loss: 3.4489278346300125 - val loss: 1.1286406368017197\n",
      "epoch: 72 - train loss: 3.4643640518188477 - val loss: 1.1807379871606827\n",
      "epoch: 73 - train loss: 3.4416763484477997 - val loss: 1.0900011509656906\n",
      "epoch: 74 - train loss: 3.4308658689260483 - val loss: 1.2264850288629532\n",
      "epoch: 75 - train loss: 3.4453976303339005 - val loss: 1.1310147792100906\n",
      "epoch: 76 - train loss: 3.4276436418294907 - val loss: 1.1499231904745102\n",
      "epoch: 77 - train loss: 3.391969174146652 - val loss: 1.2007589638233185\n",
      "epoch: 78 - train loss: 3.3838677257299423 - val loss: 1.1533109992742538\n",
      "epoch: 79 - train loss: 3.3860683888196945 - val loss: 1.2034631073474884\n",
      "epoch: 80 - train loss: 3.397211939096451 - val loss: 1.177787408232689\n",
      "epoch: 81 - train loss: 3.3936822712421417 - val loss: 1.1933633834123611\n",
      "epoch: 82 - train loss: 3.364972323179245 - val loss: 1.2015508264303207\n",
      "epoch: 83 - train loss: 3.347908467054367 - val loss: 1.1526424884796143\n",
      "epoch: 84 - train loss: 3.3794784396886826 - val loss: 1.2644555568695068\n",
      "epoch: 85 - train loss: 3.3744974583387375 - val loss: 1.1873559802770615\n",
      "epoch: 86 - train loss: 3.3501662611961365 - val loss: 1.17154061794281\n",
      "epoch: 87 - train loss: 3.3257340788841248 - val loss: 1.1377577781677246\n",
      "epoch: 88 - train loss: 3.3140897005796432 - val loss: 1.0933347344398499\n",
      "epoch: 89 - train loss: 3.3127975463867188 - val loss: 1.10501067340374\n",
      "epoch: 90 - train loss: 3.3247646540403366 - val loss: 1.165141612291336\n",
      "epoch: 91 - train loss: 3.326291963458061 - val loss: 1.0736252516508102\n",
      "epoch: 92 - train loss: 3.323791280388832 - val loss: 1.1323418468236923\n",
      "epoch: 93 - train loss: 3.285095453262329 - val loss: 1.0833042114973068\n",
      "epoch: 94 - train loss: 3.2955637723207474 - val loss: 1.109437644481659\n",
      "epoch: 95 - train loss: 3.288439780473709 - val loss: 1.0983884036540985\n",
      "epoch: 96 - train loss: 3.275934115052223 - val loss: 1.1690704375505447\n",
      "epoch: 97 - train loss: 3.297605186700821 - val loss: 1.1403729617595673\n",
      "epoch: 98 - train loss: 3.2476093024015427 - val loss: 1.2761954963207245\n",
      "epoch: 99 - train loss: 3.2342740446329117 - val loss: 1.1188699901103973\n",
      "epoch: 100 - train loss: 3.2339499443769455 - val loss: 1.0601245909929276\n",
      "epoch: 101 - train loss: 3.2362285256385803 - val loss: 1.1799286156892776\n",
      "epoch: 102 - train loss: 3.242744669318199 - val loss: 1.1463728249073029\n",
      "epoch: 103 - train loss: 3.2347304672002792 - val loss: 1.0850883573293686\n",
      "epoch: 104 - train loss: 3.2100937366485596 - val loss: 1.085589274764061\n",
      "epoch: 105 - train loss: 3.2347384244203568 - val loss: 1.1203382760286331\n",
      "epoch: 106 - train loss: 3.1905260384082794 - val loss: 1.1456489711999893\n",
      "epoch: 107 - train loss: 3.222657561302185 - val loss: 1.121587112545967\n",
      "epoch: 108 - train loss: 3.1908070147037506 - val loss: 1.1076915711164474\n",
      "epoch: 109 - train loss: 3.175839379429817 - val loss: 1.1696437299251556\n",
      "epoch: 110 - train loss: 3.1883647739887238 - val loss: 1.0941343009471893\n",
      "epoch: 111 - train loss: 3.1661078333854675 - val loss: 1.1910369843244553\n",
      "epoch: 112 - train loss: 3.172400861978531 - val loss: 1.039545014500618\n",
      "epoch: 113 - train loss: 3.174824893474579 - val loss: 1.1260974258184433\n",
      "epoch: 114 - train loss: 3.151737228035927 - val loss: 1.0474285632371902\n",
      "epoch: 115 - train loss: 3.1784251928329468 - val loss: 1.0652010142803192\n",
      "epoch: 116 - train loss: 3.152595043182373 - val loss: 1.0742845237255096\n",
      "epoch: 117 - train loss: 3.174572855234146 - val loss: 1.1901509910821915\n",
      "epoch: 118 - train loss: 3.1173466444015503 - val loss: 1.142840027809143\n",
      "epoch: 119 - train loss: 3.110021337866783 - val loss: 1.0298092365264893\n",
      "epoch: 120 - train loss: 3.129787355661392 - val loss: 1.016706794500351\n",
      "epoch: 121 - train loss: 3.1180920153856277 - val loss: 1.036759153008461\n",
      "epoch: 122 - train loss: 3.1325717717409134 - val loss: 1.041038379073143\n",
      "epoch: 123 - train loss: 3.117879197001457 - val loss: 1.0838173180818558\n",
      "epoch: 124 - train loss: 3.111258313059807 - val loss: 1.1348360627889633\n",
      "epoch: 125 - train loss: 3.113931640982628 - val loss: 1.0999811887741089\n",
      "epoch: 126 - train loss: 3.067234545946121 - val loss: 1.0728581249713898\n",
      "epoch: 127 - train loss: 3.0855916142463684 - val loss: 1.0657315403223038\n",
      "epoch: 128 - train loss: 3.08197420835495 - val loss: 1.0144552141427994\n",
      "epoch: 129 - train loss: 3.0730590373277664 - val loss: 1.14284186065197\n",
      "epoch: 130 - train loss: 3.080869644880295 - val loss: 1.000418797135353\n",
      "epoch: 131 - train loss: 3.0916680693626404 - val loss: 1.1097532957792282\n",
      "epoch: 132 - train loss: 3.0591879934072495 - val loss: 1.1445552110671997\n",
      "epoch: 133 - train loss: 3.0540679693222046 - val loss: 1.0760148167610168\n",
      "epoch: 134 - train loss: 3.0476744771003723 - val loss: 1.0600043088197708\n",
      "epoch: 135 - train loss: 3.0502319633960724 - val loss: 1.095000296831131\n",
      "epoch: 136 - train loss: 3.034230723977089 - val loss: 1.1609774231910706\n",
      "epoch: 137 - train loss: 3.038443073630333 - val loss: 1.0171409845352173\n",
      "epoch: 138 - train loss: 3.0222768783569336 - val loss: 1.116092786192894\n",
      "epoch: 139 - train loss: 3.0556212216615677 - val loss: 0.9932112544775009\n",
      "epoch: 140 - train loss: 3.046039029955864 - val loss: 1.0394449532032013\n",
      "epoch: 141 - train loss: 3.02529938519001 - val loss: 1.1093829274177551\n",
      "epoch: 142 - train loss: 3.019772633910179 - val loss: 1.021043211221695\n",
      "epoch: 143 - train loss: 3.0047396421432495 - val loss: 1.053758293390274\n",
      "epoch: 144 - train loss: 3.0159737169742584 - val loss: 0.9777976348996162\n",
      "epoch: 145 - train loss: 2.9846754372119904 - val loss: 1.0720096677541733\n",
      "epoch: 146 - train loss: 3.013112708926201 - val loss: 1.139818236231804\n",
      "epoch: 147 - train loss: 2.987545371055603 - val loss: 1.0431301593780518\n",
      "epoch: 148 - train loss: 3.0031840205192566 - val loss: 1.0882126986980438\n",
      "epoch: 149 - train loss: 2.975755587220192 - val loss: 1.0089251101016998\n",
      "epoch: 150 - train loss: 2.985850304365158 - val loss: 1.0698782801628113\n",
      "epoch: 151 - train loss: 2.964297577738762 - val loss: 1.0762225240468979\n",
      "epoch: 152 - train loss: 2.9756848514080048 - val loss: 0.9672218859195709\n",
      "epoch: 153 - train loss: 2.965452492237091 - val loss: 1.0751369893550873\n",
      "epoch: 154 - train loss: 2.9540534764528275 - val loss: 1.0188650488853455\n",
      "epoch: 155 - train loss: 2.9620573818683624 - val loss: 1.0222749561071396\n",
      "epoch: 156 - train loss: 2.9462980031967163 - val loss: 1.1868441849946976\n",
      "epoch: 157 - train loss: 2.9129435047507286 - val loss: 1.0739666372537613\n",
      "epoch: 158 - train loss: 2.9662861227989197 - val loss: 1.0242877304553986\n",
      "epoch: 159 - train loss: 2.92646586894989 - val loss: 1.048770859837532\n",
      "epoch: 160 - train loss: 2.9261970818042755 - val loss: 1.0185506343841553\n",
      "epoch: 161 - train loss: 2.937179520726204 - val loss: 1.1199076920747757\n",
      "epoch: 162 - train loss: 2.9403479993343353 - val loss: 1.043059915304184\n",
      "epoch: 163 - train loss: 2.9000960886478424 - val loss: 0.994212806224823\n",
      "epoch: 164 - train loss: 2.903670608997345 - val loss: 1.0829089134931564\n",
      "epoch: 165 - train loss: 2.9049642384052277 - val loss: 0.9804957211017609\n",
      "epoch: 166 - train loss: 2.8921314775943756 - val loss: 0.9996126294136047\n",
      "epoch: 167 - train loss: 2.896861106157303 - val loss: 1.0345937311649323\n",
      "epoch: 168 - train loss: 2.9079976677894592 - val loss: 0.978346437215805\n",
      "epoch: 169 - train loss: 2.8946674168109894 - val loss: 1.052433431148529\n",
      "epoch: 170 - train loss: 2.8948908746242523 - val loss: 1.1124002933502197\n",
      "epoch: 171 - train loss: 2.8660792112350464 - val loss: 1.0950832962989807\n",
      "epoch: 172 - train loss: 2.882686287164688 - val loss: 1.005164161324501\n",
      "epoch: 173 - train loss: 2.875084787607193 - val loss: 1.0079287737607956\n",
      "epoch: 174 - train loss: 2.8738931715488434 - val loss: 1.0586750507354736\n",
      "epoch: 175 - train loss: 2.883231997489929 - val loss: 1.0633834898471832\n",
      "epoch: 176 - train loss: 2.86194621771574 - val loss: 1.036131113767624\n",
      "epoch: 177 - train loss: 2.867631584405899 - val loss: 1.048920065164566\n",
      "epoch: 178 - train loss: 2.8605796694755554 - val loss: 1.0291305631399155\n",
      "epoch: 179 - train loss: 2.8340651988983154 - val loss: 1.0725324749946594\n",
      "epoch: 180 - train loss: 2.820502370595932 - val loss: 0.9897653460502625\n",
      "epoch: 181 - train loss: 2.823036476969719 - val loss: 1.0799400359392166\n",
      "epoch: 182 - train loss: 2.8279365599155426 - val loss: 1.0230679661035538\n",
      "epoch: 183 - train loss: 2.821507006883621 - val loss: 1.083750694990158\n",
      "epoch: 184 - train loss: 2.8504866659641266 - val loss: 1.0629709213972092\n",
      "epoch: 185 - train loss: 2.811957821249962 - val loss: 1.0176837742328644\n",
      "epoch: 186 - train loss: 2.8291990756988525 - val loss: 1.1007743179798126\n",
      "epoch: 187 - train loss: 2.8047651946544647 - val loss: 1.007148489356041\n",
      "epoch: 188 - train loss: 2.819374442100525 - val loss: 0.9939763993024826\n",
      "epoch: 189 - train loss: 2.80130298435688 - val loss: 1.0031467527151108\n",
      "epoch: 190 - train loss: 2.8238267302513123 - val loss: 1.0232019126415253\n",
      "epoch: 191 - train loss: 2.8109983950853348 - val loss: 0.9933017641305923\n",
      "epoch: 192 - train loss: 2.7850516736507416 - val loss: 0.919588640332222\n",
      "epoch: 193 - train loss: 2.8340926617383957 - val loss: 0.9768266081809998\n",
      "epoch: 194 - train loss: 2.7990255504846573 - val loss: 1.0740805566310883\n",
      "epoch: 195 - train loss: 2.817738488316536 - val loss: 0.9616064429283142\n",
      "epoch: 196 - train loss: 2.7871114164590836 - val loss: 1.026927873492241\n",
      "epoch: 197 - train loss: 2.793710619211197 - val loss: 0.9650061279535294\n",
      "epoch: 198 - train loss: 2.7853748202323914 - val loss: 1.0174577981233597\n",
      "epoch: 199 - train loss: 2.7669956237077713 - val loss: 1.0656485557556152\n",
      "epoch: 200 - train loss: 2.79402457177639 - val loss: 0.9281795471906662\n",
      "epoch: 201 - train loss: 2.7447698563337326 - val loss: 0.9685705602169037\n",
      "epoch: 202 - train loss: 2.7866565585136414 - val loss: 1.0455325841903687\n",
      "epoch: 203 - train loss: 2.752375304698944 - val loss: 0.9955935627222061\n",
      "epoch: 204 - train loss: 2.7617869675159454 - val loss: 0.952977642416954\n",
      "epoch: 205 - train loss: 2.7338623255491257 - val loss: 1.00298373401165\n",
      "epoch: 206 - train loss: 2.7393579483032227 - val loss: 0.9942057877779007\n",
      "epoch: 207 - train loss: 2.7747448086738586 - val loss: 1.0428689271211624\n",
      "epoch: 208 - train loss: 2.748901292681694 - val loss: 0.9648846834897995\n",
      "epoch: 209 - train loss: 2.7384430170059204 - val loss: 1.0271982252597809\n",
      "epoch: 210 - train loss: 2.7396736294031143 - val loss: 1.0480316132307053\n",
      "epoch: 211 - train loss: 2.7200387716293335 - val loss: 0.9511882215738297\n",
      "epoch: 212 - train loss: 2.7130855470895767 - val loss: 1.0692720264196396\n",
      "epoch: 213 - train loss: 2.720689579844475 - val loss: 1.0340722352266312\n",
      "epoch: 214 - train loss: 2.7375048100948334 - val loss: 0.984389066696167\n",
      "epoch: 215 - train loss: 2.727599263191223 - val loss: 0.9688492566347122\n",
      "epoch: 216 - train loss: 2.7081535905599594 - val loss: 0.9805389195680618\n",
      "epoch: 217 - train loss: 2.7214128524065018 - val loss: 0.9769156873226166\n",
      "epoch: 218 - train loss: 2.6949424892663956 - val loss: 0.920272208750248\n",
      "epoch: 219 - train loss: 2.711426541209221 - val loss: 0.99505415558815\n",
      "epoch: 220 - train loss: 2.6829471588134766 - val loss: 0.9327386915683746\n",
      "epoch: 221 - train loss: 2.705539807677269 - val loss: 1.010723501443863\n",
      "epoch: 222 - train loss: 2.6983200758695602 - val loss: 0.929923765361309\n",
      "epoch: 223 - train loss: 2.679252043366432 - val loss: 1.021235540509224\n",
      "epoch: 224 - train loss: 2.6658167615532875 - val loss: 1.1126993894577026\n",
      "epoch: 225 - train loss: 2.691896513104439 - val loss: 0.9662902802228928\n",
      "epoch: 226 - train loss: 2.6817637979984283 - val loss: 1.0104341506958008\n",
      "epoch: 227 - train loss: 2.6855165362358093 - val loss: 1.0343970507383347\n",
      "epoch: 228 - train loss: 2.6513750329613686 - val loss: 1.0590385049581528\n",
      "epoch: 229 - train loss: 2.680624157190323 - val loss: 0.9689906090497971\n",
      "epoch: 230 - train loss: 2.6874502152204514 - val loss: 0.9722066968679428\n",
      "epoch: 231 - train loss: 2.6568194180727005 - val loss: 1.0044750422239304\n",
      "epoch: 232 - train loss: 2.65276437997818 - val loss: 1.0364011824131012\n",
      "epoch: 233 - train loss: 2.647541642189026 - val loss: 0.9460952579975128\n",
      "epoch: 234 - train loss: 2.6694665998220444 - val loss: 0.9379366487264633\n",
      "epoch: 235 - train loss: 2.657906860113144 - val loss: 1.012528657913208\n",
      "epoch: 236 - train loss: 2.6683474332094193 - val loss: 1.0434726625680923\n",
      "epoch: 237 - train loss: 2.6539602130651474 - val loss: 0.9917655289173126\n",
      "epoch: 238 - train loss: 2.6471676528453827 - val loss: 0.9025594517588615\n",
      "epoch: 239 - train loss: 2.6328340619802475 - val loss: 1.032853052020073\n",
      "epoch: 240 - train loss: 2.6595232039690018 - val loss: 1.0413726568222046\n",
      "epoch: 241 - train loss: 2.6411515325307846 - val loss: 0.991835355758667\n",
      "epoch: 242 - train loss: 2.644085854291916 - val loss: 0.8772512823343277\n",
      "epoch: 243 - train loss: 2.6318774968385696 - val loss: 0.9928117394447327\n",
      "epoch: 244 - train loss: 2.6110670417547226 - val loss: 0.9999672025442123\n",
      "epoch: 245 - train loss: 2.6059894114732742 - val loss: 0.9322516024112701\n",
      "epoch: 246 - train loss: 2.637072041630745 - val loss: 0.910970613360405\n",
      "epoch: 247 - train loss: 2.5873163118958473 - val loss: 0.9501107335090637\n",
      "epoch: 248 - train loss: 2.595554694533348 - val loss: 0.972301721572876\n",
      "epoch: 249 - train loss: 2.590410239994526 - val loss: 0.9263205379247665\n",
      "epoch: 250 - train loss: 2.6021598875522614 - val loss: 0.94997139275074\n",
      "epoch: 251 - train loss: 2.6058050841093063 - val loss: 0.9202573150396347\n",
      "epoch: 252 - train loss: 2.595335930585861 - val loss: 0.9619135409593582\n",
      "epoch: 253 - train loss: 2.6157272458076477 - val loss: 1.0307755768299103\n",
      "epoch: 254 - train loss: 2.6238119155168533 - val loss: 0.9158054292201996\n",
      "epoch: 255 - train loss: 2.585082307457924 - val loss: 0.9640560001134872\n",
      "epoch: 256 - train loss: 2.5934016406536102 - val loss: 0.8881505876779556\n",
      "epoch: 257 - train loss: 2.5875851660966873 - val loss: 0.925737589597702\n",
      "epoch: 258 - train loss: 2.580043986439705 - val loss: 1.0045785307884216\n",
      "epoch: 259 - train loss: 2.5845002830028534 - val loss: 0.9929324686527252\n",
      "epoch: 260 - train loss: 2.5645634829998016 - val loss: 0.9809517711400986\n",
      "epoch: 261 - train loss: 2.578166291117668 - val loss: 0.9242834597826004\n",
      "epoch: 262 - train loss: 2.5606966614723206 - val loss: 0.9465073645114899\n",
      "epoch: 263 - train loss: 2.5709471851587296 - val loss: 0.9013543054461479\n",
      "epoch: 264 - train loss: 2.5437925457954407 - val loss: 1.0078352838754654\n",
      "epoch: 265 - train loss: 2.576997935771942 - val loss: 0.9352800846099854\n",
      "epoch: 266 - train loss: 2.5497206449508667 - val loss: 0.9487266689538956\n",
      "epoch: 267 - train loss: 2.5724466890096664 - val loss: 0.8938450366258621\n",
      "epoch: 268 - train loss: 2.567996233701706 - val loss: 0.9348477274179459\n",
      "epoch: 269 - train loss: 2.5436334162950516 - val loss: 0.9211966246366501\n",
      "epoch: 270 - train loss: 2.542940527200699 - val loss: 1.0649276375770569\n",
      "epoch: 271 - train loss: 2.5357112288475037 - val loss: 0.9100290536880493\n",
      "epoch: 272 - train loss: 2.534094199538231 - val loss: 0.9002054184675217\n",
      "epoch: 273 - train loss: 2.5419494807720184 - val loss: 0.9629086703062057\n",
      "epoch: 274 - train loss: 2.497239053249359 - val loss: 0.9100535959005356\n",
      "epoch: 275 - train loss: 2.530045971274376 - val loss: 0.9230283200740814\n",
      "epoch: 276 - train loss: 2.5330816507339478 - val loss: 0.9335928410291672\n",
      "epoch: 277 - train loss: 2.5138594284653664 - val loss: 1.0680362284183502\n",
      "epoch: 278 - train loss: 2.5291072577238083 - val loss: 0.8977069109678268\n",
      "epoch: 279 - train loss: 2.516118660569191 - val loss: 0.9050852209329605\n",
      "epoch: 280 - train loss: 2.5253096595406532 - val loss: 0.9639661461114883\n",
      "epoch: 281 - train loss: 2.5369833111763 - val loss: 0.9572443813085556\n",
      "epoch: 282 - train loss: 2.529376596212387 - val loss: 0.9667302072048187\n",
      "epoch: 283 - train loss: 2.5258756577968597 - val loss: 0.9671076685190201\n",
      "epoch: 284 - train loss: 2.500305563211441 - val loss: 0.9825231581926346\n",
      "epoch: 285 - train loss: 2.5186767280101776 - val loss: 0.8522610440850258\n",
      "epoch: 286 - train loss: 2.5022620856761932 - val loss: 1.0534951388835907\n",
      "epoch: 287 - train loss: 2.519513264298439 - val loss: 0.9277447313070297\n",
      "epoch: 288 - train loss: 2.506148010492325 - val loss: 0.9534373730421066\n",
      "epoch: 289 - train loss: 2.5024581104516983 - val loss: 0.934961810708046\n",
      "epoch: 290 - train loss: 2.5066090375185013 - val loss: 0.8794575110077858\n",
      "epoch: 291 - train loss: 2.4821834415197372 - val loss: 0.9235834926366806\n",
      "epoch: 292 - train loss: 2.4740429297089577 - val loss: 0.9239745438098907\n",
      "epoch: 293 - train loss: 2.479706145823002 - val loss: 0.9084062576293945\n",
      "epoch: 294 - train loss: 2.4697295799851418 - val loss: 0.9116652309894562\n",
      "epoch: 295 - train loss: 2.464475654065609 - val loss: 0.9550470262765884\n",
      "epoch: 296 - train loss: 2.4837194085121155 - val loss: 1.001917690038681\n",
      "epoch: 297 - train loss: 2.4703037291765213 - val loss: 0.8922795355319977\n",
      "epoch: 298 - train loss: 2.46139757335186 - val loss: 0.964698314666748\n",
      "epoch: 299 - train loss: 2.4714870154857635 - val loss: 0.9156099557876587\n",
      "epoch: 300 - train loss: 2.4849686324596405 - val loss: 0.9135568737983704\n",
      "epoch: 301 - train loss: 2.461071826517582 - val loss: 0.9708239436149597\n",
      "epoch: 302 - train loss: 2.4563664570450783 - val loss: 0.8753433600068092\n",
      "epoch: 303 - train loss: 2.4590150266885757 - val loss: 0.9537647664546967\n",
      "epoch: 304 - train loss: 2.4492568373680115 - val loss: 0.9788098186254501\n",
      "epoch: 305 - train loss: 2.470662884414196 - val loss: 0.9582970440387726\n",
      "epoch: 306 - train loss: 2.446578711271286 - val loss: 0.9667098820209503\n",
      "epoch: 307 - train loss: 2.4573855996131897 - val loss: 0.8938127905130386\n",
      "epoch: 308 - train loss: 2.4533445090055466 - val loss: 0.9236612468957901\n",
      "epoch: 309 - train loss: 2.4604693576693535 - val loss: 0.9368522763252258\n",
      "epoch: 310 - train loss: 2.4457596763968468 - val loss: 0.922855332493782\n",
      "epoch: 311 - train loss: 2.422543376684189 - val loss: 0.9741628617048264\n",
      "epoch: 312 - train loss: 2.4215219616889954 - val loss: 0.9442496299743652\n",
      "epoch: 313 - train loss: 2.440999999642372 - val loss: 0.9637990742921829\n",
      "epoch: 314 - train loss: 2.4259178787469864 - val loss: 0.9150077253580093\n",
      "epoch: 315 - train loss: 2.4112360551953316 - val loss: 0.9225582331418991\n",
      "epoch: 316 - train loss: 2.425616681575775 - val loss: 0.9321662932634354\n",
      "epoch: 317 - train loss: 2.437442898750305 - val loss: 0.8772940337657928\n",
      "epoch: 318 - train loss: 2.4098557755351067 - val loss: 0.9098034203052521\n",
      "epoch: 319 - train loss: 2.4285143613815308 - val loss: 0.90952467918396\n",
      "epoch: 320 - train loss: 2.4253969937562943 - val loss: 0.9000987112522125\n",
      "epoch: 321 - train loss: 2.4256029576063156 - val loss: 0.8873677551746368\n",
      "epoch: 322 - train loss: 2.4283201098442078 - val loss: 0.9124766737222672\n",
      "epoch: 323 - train loss: 2.3941374346613884 - val loss: 0.9296481013298035\n",
      "epoch: 324 - train loss: 2.4206944182515144 - val loss: 1.018514484167099\n",
      "epoch: 325 - train loss: 2.4085683375597 - val loss: 0.9613931179046631\n",
      "epoch: 326 - train loss: 2.4173691496253014 - val loss: 0.9364140182733536\n",
      "epoch: 327 - train loss: 2.3943229392170906 - val loss: 0.9854959696531296\n",
      "epoch: 328 - train loss: 2.392301484942436 - val loss: 1.0055369138717651\n",
      "epoch: 329 - train loss: 2.3879265040159225 - val loss: 0.9843844920396805\n",
      "epoch: 330 - train loss: 2.3960521295666695 - val loss: 0.8868382722139359\n",
      "epoch: 331 - train loss: 2.4023322686553 - val loss: 0.853973776102066\n",
      "epoch: 332 - train loss: 2.3912038281559944 - val loss: 0.9030103236436844\n",
      "epoch: 333 - train loss: 2.4006888419389725 - val loss: 0.8871437609195709\n",
      "epoch: 334 - train loss: 2.381887309253216 - val loss: 0.8924403339624405\n",
      "epoch: 335 - train loss: 2.3799673318862915 - val loss: 0.9017746299505234\n",
      "epoch: 336 - train loss: 2.3953931480646133 - val loss: 0.8982791751623154\n",
      "epoch: 337 - train loss: 2.4033589735627174 - val loss: 0.9361853450536728\n",
      "epoch: 338 - train loss: 2.3749994412064552 - val loss: 0.9796790182590485\n",
      "epoch: 339 - train loss: 2.376026473939419 - val loss: 0.9184680730104446\n",
      "epoch: 340 - train loss: 2.3605304956436157 - val loss: 0.9554615318775177\n",
      "epoch: 341 - train loss: 2.371952071785927 - val loss: 0.9888252317905426\n",
      "epoch: 342 - train loss: 2.358830451965332 - val loss: 0.971631333231926\n",
      "epoch: 343 - train loss: 2.342875398695469 - val loss: 0.8384780436754227\n",
      "epoch: 344 - train loss: 2.368159703910351 - val loss: 0.9491472244262695\n",
      "epoch: 345 - train loss: 2.352184936404228 - val loss: 0.9033841788768768\n",
      "epoch: 346 - train loss: 2.3494115322828293 - val loss: 0.8969437927007675\n",
      "epoch: 347 - train loss: 2.363197349011898 - val loss: 0.9813168048858643\n",
      "epoch: 348 - train loss: 2.377719059586525 - val loss: 0.9616842716932297\n",
      "epoch: 349 - train loss: 2.3674928173422813 - val loss: 0.9265942275524139\n",
      "epoch: 350 - train loss: 2.3501822277903557 - val loss: 0.9085246920585632\n",
      "epoch: 351 - train loss: 2.3419710621237755 - val loss: 0.8936867564916611\n",
      "epoch: 352 - train loss: 2.3291933238506317 - val loss: 0.9938155114650726\n",
      "epoch: 353 - train loss: 2.3334551826119423 - val loss: 0.9040884077548981\n",
      "epoch: 354 - train loss: 2.32856322824955 - val loss: 0.9109886884689331\n",
      "epoch: 355 - train loss: 2.3388578444719315 - val loss: 0.904229149222374\n",
      "epoch: 356 - train loss: 2.329925037920475 - val loss: 0.9838394373655319\n",
      "epoch: 357 - train loss: 2.3215529918670654 - val loss: 1.0031661987304688\n",
      "epoch: 358 - train loss: 2.332556076347828 - val loss: 0.9124155938625336\n",
      "epoch: 359 - train loss: 2.3167848959565163 - val loss: 0.8672152012586594\n",
      "epoch: 360 - train loss: 2.303970053792 - val loss: 0.8803594708442688\n",
      "epoch: 361 - train loss: 2.3291205018758774 - val loss: 0.8053732737898827\n",
      "epoch: 362 - train loss: 2.331235595047474 - val loss: 0.8358167409896851\n",
      "epoch: 363 - train loss: 2.292238362133503 - val loss: 0.8798435628414154\n",
      "epoch: 364 - train loss: 2.328720010817051 - val loss: 0.8771729022264481\n",
      "epoch: 365 - train loss: 2.305890716612339 - val loss: 0.8662675619125366\n",
      "epoch: 366 - train loss: 2.314089648425579 - val loss: 0.8875324577093124\n",
      "epoch: 367 - train loss: 2.306829482316971 - val loss: 0.8634660542011261\n",
      "epoch: 368 - train loss: 2.2888455241918564 - val loss: 0.8562736362218857\n",
      "epoch: 369 - train loss: 2.306369222700596 - val loss: 0.9016664177179337\n",
      "epoch: 370 - train loss: 2.2918912693858147 - val loss: 0.9808109104633331\n",
      "epoch: 371 - train loss: 2.2922298684716225 - val loss: 0.9238612502813339\n",
      "epoch: 372 - train loss: 2.301458440721035 - val loss: 0.8817075341939926\n",
      "epoch: 373 - train loss: 2.290654920041561 - val loss: 0.9793506860733032\n",
      "epoch: 374 - train loss: 2.303450368344784 - val loss: 0.8733840882778168\n",
      "epoch: 375 - train loss: 2.2996692284941673 - val loss: 0.8792593628168106\n",
      "epoch: 376 - train loss: 2.289148136973381 - val loss: 0.8382906019687653\n",
      "epoch: 377 - train loss: 2.2991763204336166 - val loss: 0.9026063829660416\n",
      "epoch: 378 - train loss: 2.282095231115818 - val loss: 0.9661508202552795\n",
      "epoch: 379 - train loss: 2.2830595448613167 - val loss: 0.9547507613897324\n",
      "epoch: 380 - train loss: 2.2712511122226715 - val loss: 0.9934780299663544\n",
      "epoch: 381 - train loss: 2.282486453652382 - val loss: 0.8325579091906548\n",
      "epoch: 382 - train loss: 2.3031297847628593 - val loss: 0.9088237434625626\n",
      "epoch: 383 - train loss: 2.302075743675232 - val loss: 0.8598727583885193\n",
      "epoch: 384 - train loss: 2.261022739112377 - val loss: 0.900073766708374\n",
      "epoch: 385 - train loss: 2.2674682587385178 - val loss: 0.8257038667798042\n",
      "epoch: 386 - train loss: 2.2669614776968956 - val loss: 0.9598100781440735\n",
      "epoch: 387 - train loss: 2.265816755592823 - val loss: 0.9214954823255539\n",
      "epoch: 388 - train loss: 2.2527725771069527 - val loss: 0.9071905165910721\n",
      "epoch: 389 - train loss: 2.259380027651787 - val loss: 0.9647707045078278\n",
      "epoch: 390 - train loss: 2.2529444471001625 - val loss: 0.8424967378377914\n",
      "epoch: 391 - train loss: 2.243316166102886 - val loss: 0.8569729626178741\n",
      "epoch: 392 - train loss: 2.2682921066880226 - val loss: 0.9679218828678131\n",
      "epoch: 393 - train loss: 2.2648660615086555 - val loss: 0.9431550949811935\n",
      "epoch: 394 - train loss: 2.2706276774406433 - val loss: 0.9312993735074997\n",
      "epoch: 395 - train loss: 2.2330058813095093 - val loss: 0.8302523046731949\n",
      "epoch: 396 - train loss: 2.2426400184631348 - val loss: 0.8144674822688103\n",
      "epoch: 397 - train loss: 2.262799561023712 - val loss: 0.9506638497114182\n",
      "epoch: 398 - train loss: 2.251989357173443 - val loss: 0.8456306010484695\n",
      "epoch: 399 - train loss: 2.236933670938015 - val loss: 0.8932498693466187\n",
      "epoch: 400 - train loss: 2.230113707482815 - val loss: 0.8961161822080612\n",
      "epoch: 401 - train loss: 2.233168974518776 - val loss: 0.8782520741224289\n",
      "epoch: 402 - train loss: 2.2267919331789017 - val loss: 0.8167578428983688\n",
      "epoch: 403 - train loss: 2.2465096786618233 - val loss: 0.9299949407577515\n",
      "epoch: 404 - train loss: 2.2259151116013527 - val loss: 0.928528755903244\n",
      "epoch: 405 - train loss: 2.212671183049679 - val loss: 0.9137676507234573\n",
      "epoch: 406 - train loss: 2.2197086960077286 - val loss: 0.8376381248235703\n",
      "epoch: 407 - train loss: 2.235434405505657 - val loss: 0.8420752957463264\n",
      "epoch: 408 - train loss: 2.2475075125694275 - val loss: 0.9406703412532806\n",
      "epoch: 409 - train loss: 2.243922732770443 - val loss: 0.8384313732385635\n",
      "epoch: 410 - train loss: 2.207227312028408 - val loss: 0.8514556437730789\n",
      "epoch: 411 - train loss: 2.1942691653966904 - val loss: 0.872153028845787\n",
      "epoch: 412 - train loss: 2.2348010390996933 - val loss: 0.9549796432256699\n",
      "epoch: 413 - train loss: 2.2273353934288025 - val loss: 0.9403141736984253\n",
      "epoch: 414 - train loss: 2.21546620875597 - val loss: 0.9607035219669342\n",
      "epoch: 415 - train loss: 2.2159691751003265 - val loss: 0.8290025293827057\n",
      "epoch: 416 - train loss: 2.215314418077469 - val loss: 0.9862209856510162\n",
      "epoch: 417 - train loss: 2.223840542137623 - val loss: 0.8711096346378326\n",
      "epoch: 418 - train loss: 2.227471739053726 - val loss: 0.8111416399478912\n",
      "epoch: 419 - train loss: 2.1760702580213547 - val loss: 0.8580041229724884\n",
      "epoch: 420 - train loss: 2.209439091384411 - val loss: 0.8563948273658752\n",
      "epoch: 421 - train loss: 2.1996996253728867 - val loss: 0.8545129150152206\n",
      "epoch: 422 - train loss: 2.181971102952957 - val loss: 0.8625408858060837\n",
      "epoch: 423 - train loss: 2.1828133910894394 - val loss: 0.8059346079826355\n",
      "epoch: 424 - train loss: 2.2225427106022835 - val loss: 0.8963770121335983\n",
      "epoch: 425 - train loss: 2.2024079486727715 - val loss: 0.8262666761875153\n",
      "epoch: 426 - train loss: 2.1784054711461067 - val loss: 0.8718225210905075\n",
      "epoch: 427 - train loss: 2.1902498677372932 - val loss: 0.8899417072534561\n",
      "epoch: 428 - train loss: 2.1674022376537323 - val loss: 0.8546562194824219\n",
      "epoch: 429 - train loss: 2.1825591921806335 - val loss: 0.8820031732320786\n",
      "epoch: 430 - train loss: 2.1795407235622406 - val loss: 0.8424679338932037\n",
      "epoch: 431 - train loss: 2.189452178776264 - val loss: 0.828446552157402\n",
      "epoch: 432 - train loss: 2.1670359298586845 - val loss: 0.8907006531953812\n",
      "epoch: 433 - train loss: 2.1759404838085175 - val loss: 0.8370635807514191\n",
      "epoch: 434 - train loss: 2.168369710445404 - val loss: 0.8359744101762772\n",
      "epoch: 435 - train loss: 2.1736292466521263 - val loss: 0.8961082994937897\n",
      "epoch: 436 - train loss: 2.1526177376508713 - val loss: 0.8244952857494354\n",
      "epoch: 437 - train loss: 2.1574728041887283 - val loss: 0.8609876036643982\n",
      "epoch: 438 - train loss: 2.1700530722737312 - val loss: 0.8167738169431686\n",
      "epoch: 439 - train loss: 2.1706238985061646 - val loss: 0.8497860431671143\n",
      "epoch: 440 - train loss: 2.173884741961956 - val loss: 0.8703091740608215\n",
      "epoch: 441 - train loss: 2.154494158923626 - val loss: 0.9518302232027054\n",
      "epoch: 442 - train loss: 2.161736808717251 - val loss: 0.8297709226608276\n",
      "epoch: 443 - train loss: 2.1707680225372314 - val loss: 0.8353816866874695\n",
      "epoch: 444 - train loss: 2.1616090312600136 - val loss: 0.9206628799438477\n",
      "epoch: 445 - train loss: 2.1578610986471176 - val loss: 0.8639117330312729\n",
      "epoch: 446 - train loss: 2.157436065375805 - val loss: 0.8184833154082298\n",
      "epoch: 447 - train loss: 2.1622732430696487 - val loss: 0.8463055491447449\n",
      "epoch: 448 - train loss: 2.147506073117256 - val loss: 0.8708905279636383\n",
      "epoch: 449 - train loss: 2.146397687494755 - val loss: 0.9437858909368515\n",
      "epoch: 450 - train loss: 2.144636608660221 - val loss: 0.9161999970674515\n",
      "epoch: 451 - train loss: 2.1295953318476677 - val loss: 0.8858376741409302\n",
      "epoch: 452 - train loss: 2.143744371831417 - val loss: 0.7992719262838364\n",
      "epoch: 453 - train loss: 2.140746995806694 - val loss: 0.8388855755329132\n",
      "epoch: 454 - train loss: 2.1583274602890015 - val loss: 0.8374375998973846\n",
      "epoch: 455 - train loss: 2.151982069015503 - val loss: 1.0485652834177017\n",
      "epoch: 456 - train loss: 2.1428029760718346 - val loss: 0.8268506526947021\n",
      "epoch: 457 - train loss: 2.125534564256668 - val loss: 0.8604348599910736\n",
      "epoch: 458 - train loss: 2.1237641870975494 - val loss: 0.8558946251869202\n",
      "epoch: 459 - train loss: 2.125991754233837 - val loss: 0.8263687789440155\n",
      "epoch: 460 - train loss: 2.1198323518037796 - val loss: 0.854311615228653\n",
      "epoch: 461 - train loss: 2.109855994582176 - val loss: 0.9010628759860992\n",
      "epoch: 462 - train loss: 2.1103034168481827 - val loss: 0.918771430850029\n",
      "epoch: 463 - train loss: 2.1210615411400795 - val loss: 0.892108753323555\n",
      "epoch: 464 - train loss: 2.1224708408117294 - val loss: 0.8502290099859238\n",
      "epoch: 465 - train loss: 2.118949517607689 - val loss: 0.8329466581344604\n",
      "epoch: 466 - train loss: 2.102458730340004 - val loss: 0.8044922351837158\n",
      "epoch: 467 - train loss: 2.1150297820568085 - val loss: 0.7780845388770103\n",
      "epoch: 468 - train loss: 2.1035717204213142 - val loss: 0.8340224772691727\n",
      "epoch: 469 - train loss: 2.1232916489243507 - val loss: 0.9319641292095184\n",
      "epoch: 470 - train loss: 2.08896903693676 - val loss: 0.8645195066928864\n",
      "epoch: 471 - train loss: 2.099951297044754 - val loss: 0.9274609535932541\n",
      "epoch: 472 - train loss: 2.1035623103380203 - val loss: 0.8985827565193176\n",
      "epoch: 473 - train loss: 2.125301517546177 - val loss: 0.863511249423027\n",
      "epoch: 474 - train loss: 2.095776781439781 - val loss: 0.882808193564415\n",
      "epoch: 475 - train loss: 2.10830832272768 - val loss: 0.8622986078262329\n",
      "epoch: 476 - train loss: 2.098316974937916 - val loss: 0.8082794845104218\n",
      "epoch: 477 - train loss: 2.086301274597645 - val loss: 0.7813177183270454\n",
      "epoch: 478 - train loss: 2.117494471371174 - val loss: 0.8824740052223206\n",
      "epoch: 479 - train loss: 2.111775651574135 - val loss: 0.9273566901683807\n",
      "epoch: 480 - train loss: 2.076450392603874 - val loss: 0.8325609415769577\n",
      "epoch: 481 - train loss: 2.0853354036808014 - val loss: 0.8711350709199905\n",
      "epoch: 482 - train loss: 2.0759789049625397 - val loss: 0.8350312411785126\n",
      "epoch: 483 - train loss: 2.08574365824461 - val loss: 0.9516310542821884\n",
      "epoch: 484 - train loss: 2.0951151996850967 - val loss: 0.9322691708803177\n",
      "epoch: 485 - train loss: 2.0784322544932365 - val loss: 0.8562591522932053\n",
      "epoch: 486 - train loss: 2.0767777785658836 - val loss: 0.9120166599750519\n",
      "epoch: 487 - train loss: 2.072971098124981 - val loss: 0.8751843720674515\n",
      "epoch: 488 - train loss: 2.086375907063484 - val loss: 0.8885108381509781\n",
      "epoch: 489 - train loss: 2.073017403483391 - val loss: 0.7767913639545441\n",
      "epoch: 490 - train loss: 2.0710569992661476 - val loss: 0.8635831028223038\n",
      "epoch: 491 - train loss: 2.071236103773117 - val loss: 0.8310595005750656\n",
      "epoch: 492 - train loss: 2.0733490511775017 - val loss: 0.7521640732884407\n",
      "epoch: 493 - train loss: 2.0672209709882736 - val loss: 1.0414472073316574\n",
      "epoch: 494 - train loss: 2.0677422136068344 - val loss: 0.8017677292227745\n",
      "epoch: 495 - train loss: 2.0624404549598694 - val loss: 0.950172945857048\n",
      "epoch: 496 - train loss: 2.0598623901605606 - val loss: 0.8363584280014038\n",
      "epoch: 497 - train loss: 2.0554506108164787 - val loss: 0.8768911957740784\n",
      "epoch: 498 - train loss: 2.0881367325782776 - val loss: 1.0196559727191925\n",
      "epoch: 499 - train loss: 2.0589602142572403 - val loss: 0.8013699427247047\n",
      "epoch: 500 - train loss: 2.0512626692652702 - val loss: 0.8315203934907913\n",
      "epoch: 501 - train loss: 2.056974805891514 - val loss: 0.8160291910171509\n",
      "epoch: 502 - train loss: 2.0542297288775444 - val loss: 0.8767030835151672\n",
      "epoch: 503 - train loss: 2.0673882961273193 - val loss: 0.819018617272377\n",
      "epoch: 504 - train loss: 2.0466160401701927 - val loss: 0.8133639842271805\n",
      "epoch: 505 - train loss: 2.0549271628260612 - val loss: 0.8735374063253403\n",
      "epoch: 506 - train loss: 2.062510997056961 - val loss: 0.8515352308750153\n",
      "epoch: 507 - train loss: 2.0572193041443825 - val loss: 0.8135954141616821\n",
      "epoch: 508 - train loss: 2.027009814977646 - val loss: 0.8736968487501144\n",
      "epoch: 509 - train loss: 2.0311142951250076 - val loss: 0.8364070504903793\n",
      "epoch: 510 - train loss: 2.039755329489708 - val loss: 0.9089044332504272\n",
      "epoch: 511 - train loss: 2.0361518040299416 - val loss: 0.8224497586488724\n",
      "epoch: 512 - train loss: 2.0478451177477837 - val loss: 0.8023351728916168\n",
      "epoch: 513 - train loss: 2.045536257326603 - val loss: 0.8249873369932175\n",
      "epoch: 514 - train loss: 2.0436015129089355 - val loss: 0.8839807212352753\n",
      "epoch: 515 - train loss: 2.030099131166935 - val loss: 0.8390656560659409\n",
      "epoch: 516 - train loss: 2.049183301627636 - val loss: 0.8940000832080841\n",
      "epoch: 517 - train loss: 2.0152721852064133 - val loss: 0.9192538559436798\n",
      "epoch: 518 - train loss: 2.0408894941210747 - val loss: 0.8638544082641602\n",
      "epoch: 519 - train loss: 2.028238594532013 - val loss: 0.8410205841064453\n",
      "epoch: 520 - train loss: 2.0287707448005676 - val loss: 0.8296972811222076\n",
      "epoch: 521 - train loss: 2.0322689190506935 - val loss: 0.855455070734024\n",
      "epoch: 522 - train loss: 2.021771751344204 - val loss: 0.8163534998893738\n",
      "epoch: 523 - train loss: 2.0252732038497925 - val loss: 0.8064102977514267\n",
      "epoch: 524 - train loss: 2.022909924387932 - val loss: 0.8081476241350174\n",
      "epoch: 525 - train loss: 2.0094144344329834 - val loss: 0.8693954646587372\n",
      "epoch: 526 - train loss: 2.0051458552479744 - val loss: 0.8000880181789398\n",
      "epoch: 527 - train loss: 2.0037418454885483 - val loss: 0.8900668323040009\n",
      "epoch: 528 - train loss: 2.0002142637968063 - val loss: 0.8382502943277359\n",
      "epoch: 529 - train loss: 2.00253251940012 - val loss: 0.8776523172855377\n",
      "epoch: 530 - train loss: 2.002510964870453 - val loss: 0.9090376198291779\n",
      "epoch: 531 - train loss: 2.007086619734764 - val loss: 0.8469390422105789\n",
      "epoch: 532 - train loss: 2.0244119688868523 - val loss: 0.7684692367911339\n",
      "epoch: 533 - train loss: 1.9967724829912186 - val loss: 0.8720607906579971\n",
      "epoch: 534 - train loss: 1.978432647883892 - val loss: 0.8735054433345795\n",
      "epoch: 535 - train loss: 1.9937875270843506 - val loss: 0.8386804610490799\n",
      "epoch: 536 - train loss: 1.9882442653179169 - val loss: 0.9175786077976227\n",
      "epoch: 537 - train loss: 1.9941744729876518 - val loss: 0.8532870411872864\n",
      "epoch: 538 - train loss: 2.0042167752981186 - val loss: 0.7731971144676208\n",
      "epoch: 539 - train loss: 2.0125087797641754 - val loss: 0.8264079540967941\n",
      "epoch: 540 - train loss: 1.985628068447113 - val loss: 0.8385509997606277\n",
      "epoch: 541 - train loss: 1.9890785664319992 - val loss: 0.8856463879346848\n",
      "epoch: 542 - train loss: 1.9753301069140434 - val loss: 0.7930530086159706\n",
      "epoch: 543 - train loss: 1.9716318845748901 - val loss: 0.7413797564804554\n",
      "epoch: 544 - train loss: 1.9681479334831238 - val loss: 0.924461618065834\n",
      "epoch: 545 - train loss: 1.9844011515378952 - val loss: 0.8518744558095932\n",
      "epoch: 546 - train loss: 1.9752997010946274 - val loss: 0.8617062121629715\n",
      "epoch: 547 - train loss: 1.9724359065294266 - val loss: 0.8268702775239944\n",
      "epoch: 548 - train loss: 1.978640079498291 - val loss: 0.8145946264266968\n",
      "epoch: 549 - train loss: 1.9827794060111046 - val loss: 0.8955820202827454\n",
      "epoch: 550 - train loss: 1.9733814895153046 - val loss: 0.8249835073947906\n",
      "epoch: 551 - train loss: 1.9520660191774368 - val loss: 0.7532619908452034\n",
      "epoch: 552 - train loss: 1.9626890122890472 - val loss: 0.8829490393400192\n",
      "epoch: 553 - train loss: 1.9636154174804688 - val loss: 0.8398912250995636\n",
      "epoch: 554 - train loss: 1.9792998060584068 - val loss: 0.8310362249612808\n",
      "epoch: 555 - train loss: 1.9656562209129333 - val loss: 0.8061551004648209\n",
      "epoch: 556 - train loss: 1.9717894718050957 - val loss: 0.8568125814199448\n",
      "epoch: 557 - train loss: 1.9762553423643112 - val loss: 0.7931099534034729\n",
      "epoch: 558 - train loss: 1.9715612307190895 - val loss: 0.8940947204828262\n",
      "epoch: 559 - train loss: 1.95315682888031 - val loss: 0.7863131240010262\n",
      "epoch: 560 - train loss: 1.9575445801019669 - val loss: 0.8503647446632385\n",
      "epoch: 561 - train loss: 1.970065325498581 - val loss: 0.8508488833904266\n",
      "epoch: 562 - train loss: 1.9653126299381256 - val loss: 0.9321099817752838\n",
      "epoch: 563 - train loss: 1.957494579255581 - val loss: 0.7840208634734154\n",
      "epoch: 564 - train loss: 1.9664924144744873 - val loss: 0.7986060529947281\n",
      "epoch: 565 - train loss: 1.95183427631855 - val loss: 0.8260387182235718\n",
      "epoch: 566 - train loss: 1.9620748981833458 - val loss: 0.8046508133411407\n",
      "epoch: 567 - train loss: 1.9530982300639153 - val loss: 0.9192978590726852\n",
      "epoch: 568 - train loss: 1.9471545070409775 - val loss: 0.8056881129741669\n",
      "epoch: 569 - train loss: 1.94147440046072 - val loss: 0.8052446097135544\n",
      "epoch: 570 - train loss: 1.933261439204216 - val loss: 0.8410444259643555\n",
      "epoch: 571 - train loss: 1.942356437444687 - val loss: 0.8998647481203079\n",
      "epoch: 572 - train loss: 1.9417481571435928 - val loss: 0.839025542140007\n",
      "epoch: 573 - train loss: 1.9224875420331955 - val loss: 0.8112452775239944\n",
      "epoch: 574 - train loss: 1.9457610696554184 - val loss: 0.7734481021761894\n",
      "epoch: 575 - train loss: 1.9379610866308212 - val loss: 0.7478933557868004\n",
      "epoch: 576 - train loss: 1.9435670971870422 - val loss: 0.804606705904007\n",
      "epoch: 577 - train loss: 1.9508550390601158 - val loss: 0.9179827123880386\n",
      "epoch: 578 - train loss: 1.9225299134850502 - val loss: 0.8245319426059723\n",
      "epoch: 579 - train loss: 1.9136019498109818 - val loss: 0.8300355523824692\n",
      "epoch: 580 - train loss: 1.9161457866430283 - val loss: 0.7871982380747795\n",
      "epoch: 581 - train loss: 1.9347099363803864 - val loss: 0.7364403046667576\n",
      "epoch: 582 - train loss: 1.9424944519996643 - val loss: 0.9551732838153839\n",
      "epoch: 583 - train loss: 1.9157471358776093 - val loss: 0.8560901284217834\n",
      "epoch: 584 - train loss: 1.9261553585529327 - val loss: 0.8333365768194199\n",
      "epoch: 585 - train loss: 1.9289829283952713 - val loss: 0.82209812104702\n",
      "epoch: 586 - train loss: 1.9240878820419312 - val loss: 0.7584774792194366\n",
      "epoch: 587 - train loss: 1.9273849204182625 - val loss: 0.7347771376371384\n",
      "epoch: 588 - train loss: 1.939633160829544 - val loss: 0.7604119554162025\n",
      "epoch: 589 - train loss: 1.9316482618451118 - val loss: 0.817285031080246\n",
      "epoch: 590 - train loss: 1.901614099740982 - val loss: 0.7783068418502808\n",
      "epoch: 591 - train loss: 1.9276700988411903 - val loss: 0.8093305677175522\n",
      "epoch: 592 - train loss: 1.9196274727582932 - val loss: 0.9108679890632629\n",
      "epoch: 593 - train loss: 1.9020041227340698 - val loss: 0.8633808791637421\n",
      "epoch: 594 - train loss: 1.923215575516224 - val loss: 0.8014568090438843\n",
      "epoch: 595 - train loss: 1.8994135782122612 - val loss: 0.8521555811166763\n",
      "epoch: 596 - train loss: 1.8900299593806267 - val loss: 0.8786379098892212\n",
      "epoch: 597 - train loss: 1.9275748804211617 - val loss: 0.8061349987983704\n",
      "epoch: 598 - train loss: 1.9123114496469498 - val loss: 0.7793578952550888\n",
      "epoch: 599 - train loss: 1.9094780683517456 - val loss: 0.8018494546413422\n",
      "epoch: 600 - train loss: 1.8996068239212036 - val loss: 0.7971154004335403\n",
      "epoch: 601 - train loss: 1.9012335389852524 - val loss: 0.7977113127708435\n",
      "epoch: 602 - train loss: 1.9053328111767769 - val loss: 0.7737253308296204\n",
      "epoch: 603 - train loss: 1.88988446444273 - val loss: 0.8207554221153259\n",
      "epoch: 604 - train loss: 1.8960393443703651 - val loss: 0.7872390896081924\n",
      "epoch: 605 - train loss: 1.89325500279665 - val loss: 0.8429939299821854\n",
      "epoch: 606 - train loss: 1.8864665105938911 - val loss: 0.7669917270541191\n",
      "epoch: 607 - train loss: 1.8800112009048462 - val loss: 0.7585307955741882\n",
      "epoch: 608 - train loss: 1.882999710738659 - val loss: 0.7545673549175262\n",
      "epoch: 609 - train loss: 1.8850029408931732 - val loss: 0.8068589419126511\n",
      "epoch: 610 - train loss: 1.8993210718035698 - val loss: 0.719745971262455\n",
      "epoch: 611 - train loss: 1.8877379447221756 - val loss: 0.871672973036766\n",
      "epoch: 612 - train loss: 1.8766138553619385 - val loss: 0.9560140371322632\n",
      "epoch: 613 - train loss: 1.882511630654335 - val loss: 0.801380380988121\n",
      "epoch: 614 - train loss: 1.8654780834913254 - val loss: 0.7834793627262115\n",
      "epoch: 615 - train loss: 1.868664301931858 - val loss: 0.843127578496933\n",
      "epoch: 616 - train loss: 1.878405824303627 - val loss: 0.8625637888908386\n",
      "epoch: 617 - train loss: 1.873091146349907 - val loss: 0.8183780610561371\n",
      "epoch: 618 - train loss: 1.854676015675068 - val loss: 0.8074871152639389\n",
      "epoch: 619 - train loss: 1.8670217990875244 - val loss: 0.8267100304365158\n",
      "epoch: 620 - train loss: 1.8780591636896133 - val loss: 0.8782310038805008\n",
      "epoch: 621 - train loss: 1.865901917219162 - val loss: 0.7846744358539581\n",
      "epoch: 622 - train loss: 1.8807873278856277 - val loss: 0.8677659332752228\n",
      "epoch: 623 - train loss: 1.8695524707436562 - val loss: 0.7307885214686394\n",
      "epoch: 624 - train loss: 1.8693153634667397 - val loss: 0.7599386125802994\n",
      "epoch: 625 - train loss: 1.8780008405447006 - val loss: 0.7488746494054794\n",
      "epoch: 626 - train loss: 1.857963964343071 - val loss: 0.8103715777397156\n",
      "epoch: 627 - train loss: 1.8716149032115936 - val loss: 0.7678067684173584\n",
      "epoch: 628 - train loss: 1.8621595799922943 - val loss: 0.8976065218448639\n",
      "epoch: 629 - train loss: 1.8524981439113617 - val loss: 0.978848859667778\n",
      "epoch: 630 - train loss: 1.875945508480072 - val loss: 0.8001462817192078\n",
      "epoch: 631 - train loss: 1.849864773452282 - val loss: 0.8696902990341187\n",
      "epoch: 632 - train loss: 1.8825762197375298 - val loss: 0.8384397774934769\n",
      "epoch: 633 - train loss: 1.8525806367397308 - val loss: 0.8741949498653412\n",
      "epoch: 634 - train loss: 1.8645214512944221 - val loss: 0.8027998358011246\n",
      "epoch: 635 - train loss: 1.8605637177824974 - val loss: 0.7644613087177277\n",
      "epoch: 636 - train loss: 1.8468013778328896 - val loss: 0.8760597407817841\n",
      "epoch: 637 - train loss: 1.8568003475666046 - val loss: 0.8811047226190567\n",
      "epoch: 638 - train loss: 1.8427370265126228 - val loss: 0.7848775088787079\n",
      "epoch: 639 - train loss: 1.8470292389392853 - val loss: 0.8531583547592163\n",
      "epoch: 640 - train loss: 1.8457239419221878 - val loss: 0.8991071879863739\n",
      "epoch: 641 - train loss: 1.8253041133284569 - val loss: 0.808204859495163\n",
      "epoch: 642 - train loss: 1.8460744321346283 - val loss: 0.7411987483501434\n",
      "epoch: 643 - train loss: 1.8400004133582115 - val loss: 0.9274548888206482\n",
      "epoch: 644 - train loss: 1.8270279243588448 - val loss: 0.8572155237197876\n",
      "epoch: 645 - train loss: 1.847068801522255 - val loss: 0.7302721589803696\n",
      "epoch: 646 - train loss: 1.8321346566081047 - val loss: 0.8096942752599716\n",
      "epoch: 647 - train loss: 1.8377755209803581 - val loss: 0.7896578013896942\n",
      "epoch: 648 - train loss: 1.8279093280434608 - val loss: 0.8812874555587769\n",
      "epoch: 649 - train loss: 1.8263394385576248 - val loss: 0.849703773856163\n",
      "epoch: 650 - train loss: 1.8505397513508797 - val loss: 0.8171212077140808\n",
      "epoch: 651 - train loss: 1.8296648263931274 - val loss: 0.9456705451011658\n",
      "epoch: 652 - train loss: 1.8358132094144821 - val loss: 0.8860970139503479\n",
      "epoch: 653 - train loss: 1.8365544080734253 - val loss: 0.7285766676068306\n",
      "epoch: 654 - train loss: 1.809722475707531 - val loss: 0.8230494558811188\n",
      "epoch: 655 - train loss: 1.8204446956515312 - val loss: 0.7351068779826164\n",
      "epoch: 656 - train loss: 1.82492845505476 - val loss: 0.7690102159976959\n",
      "epoch: 657 - train loss: 1.8225184306502342 - val loss: 0.8153131455183029\n",
      "epoch: 658 - train loss: 1.8147120624780655 - val loss: 0.8457697480916977\n",
      "epoch: 659 - train loss: 1.825539991259575 - val loss: 0.7910002171993256\n",
      "epoch: 660 - train loss: 1.7972217053174973 - val loss: 0.8151081055402756\n",
      "epoch: 661 - train loss: 1.843907244503498 - val loss: 0.7394232079386711\n",
      "epoch: 662 - train loss: 1.8206344321370125 - val loss: 0.7216224446892738\n",
      "epoch: 663 - train loss: 1.8004907593131065 - val loss: 0.7308243587613106\n",
      "epoch: 664 - train loss: 1.848175324499607 - val loss: 0.8022244870662689\n",
      "epoch: 665 - train loss: 1.8221517950296402 - val loss: 0.7455842569470406\n",
      "epoch: 666 - train loss: 1.8013374954462051 - val loss: 0.8208649605512619\n",
      "epoch: 667 - train loss: 1.8249134719371796 - val loss: 0.8058021813631058\n",
      "epoch: 668 - train loss: 1.803367294371128 - val loss: 0.778241753578186\n",
      "epoch: 669 - train loss: 1.7974561229348183 - val loss: 0.7471836805343628\n",
      "epoch: 670 - train loss: 1.8105296343564987 - val loss: 0.8516130149364471\n",
      "epoch: 671 - train loss: 1.80732062458992 - val loss: 0.8078201711177826\n",
      "epoch: 672 - train loss: 1.8059528842568398 - val loss: 0.7804139852523804\n",
      "epoch: 673 - train loss: 1.8228136152029037 - val loss: 0.7937348335981369\n",
      "epoch: 674 - train loss: 1.7994607537984848 - val loss: 0.7679775133728981\n",
      "epoch: 675 - train loss: 1.7775207310914993 - val loss: 0.7988904118537903\n",
      "epoch: 676 - train loss: 1.807024471461773 - val loss: 0.7730816900730133\n",
      "epoch: 677 - train loss: 1.7860335782170296 - val loss: 0.7341830134391785\n",
      "epoch: 678 - train loss: 1.79909897595644 - val loss: 0.8575380444526672\n",
      "epoch: 679 - train loss: 1.7894687503576279 - val loss: 0.7793154269456863\n",
      "epoch: 680 - train loss: 1.8040799796581268 - val loss: 0.7383091747760773\n",
      "epoch: 681 - train loss: 1.7836178243160248 - val loss: 0.8640898168087006\n",
      "epoch: 682 - train loss: 1.8003202229738235 - val loss: 0.7962988615036011\n",
      "epoch: 683 - train loss: 1.7839304506778717 - val loss: 0.8701915144920349\n",
      "epoch: 684 - train loss: 1.7786359041929245 - val loss: 0.8432471752166748\n",
      "epoch: 685 - train loss: 1.7826554998755455 - val loss: 0.8367318361997604\n",
      "epoch: 686 - train loss: 1.779458150267601 - val loss: 0.7632504850625992\n",
      "epoch: 687 - train loss: 1.771411508321762 - val loss: 0.8253248780965805\n",
      "epoch: 688 - train loss: 1.7955098897218704 - val loss: 0.7510170862078667\n",
      "epoch: 689 - train loss: 1.7750447914004326 - val loss: 0.9447370022535324\n",
      "epoch: 690 - train loss: 1.767576314508915 - val loss: 0.7551359608769417\n",
      "epoch: 691 - train loss: 1.7637460604310036 - val loss: 0.7442111000418663\n",
      "epoch: 692 - train loss: 1.7852439060807228 - val loss: 0.8155569434165955\n",
      "epoch: 693 - train loss: 1.782231256365776 - val loss: 0.7339453250169754\n",
      "epoch: 694 - train loss: 1.7651788294315338 - val loss: 0.7398500591516495\n",
      "epoch: 695 - train loss: 1.765048235654831 - val loss: 0.9333382993936539\n",
      "epoch: 696 - train loss: 1.7635000497102737 - val loss: 0.7814304381608963\n",
      "epoch: 697 - train loss: 1.7677299156785011 - val loss: 0.8154501020908356\n",
      "epoch: 698 - train loss: 1.7531342431902885 - val loss: 0.8616979867219925\n",
      "epoch: 699 - train loss: 1.7639504969120026 - val loss: 0.8893536329269409\n",
      "epoch: 700 - train loss: 1.7547589391469955 - val loss: 0.8028556257486343\n",
      "epoch: 701 - train loss: 1.749133139848709 - val loss: 0.7923102974891663\n",
      "epoch: 702 - train loss: 1.778441958129406 - val loss: 0.7518924176692963\n",
      "epoch: 703 - train loss: 1.7557463273406029 - val loss: 0.7290898710489273\n",
      "epoch: 704 - train loss: 1.7579972818493843 - val loss: 0.8035252541303635\n",
      "epoch: 705 - train loss: 1.7714980617165565 - val loss: 0.8394852131605148\n",
      "epoch: 706 - train loss: 1.7644486874341965 - val loss: 0.7304599434137344\n",
      "epoch: 707 - train loss: 1.7534855008125305 - val loss: 0.7888211160898209\n",
      "epoch: 708 - train loss: 1.7527802735567093 - val loss: 0.8127480894327164\n",
      "epoch: 709 - train loss: 1.7572609186172485 - val loss: 0.7053488977253437\n",
      "epoch: 710 - train loss: 1.758567713201046 - val loss: 0.8046257048845291\n",
      "epoch: 711 - train loss: 1.7385448962450027 - val loss: 0.839600071310997\n",
      "epoch: 712 - train loss: 1.731448158621788 - val loss: 0.7939165532588959\n",
      "epoch: 713 - train loss: 1.7490729987621307 - val loss: 0.7803044021129608\n",
      "epoch: 714 - train loss: 1.7471450194716454 - val loss: 0.7381174340844154\n",
      "epoch: 715 - train loss: 1.7453037649393082 - val loss: 0.792685329914093\n",
      "epoch: 716 - train loss: 1.7515466958284378 - val loss: 0.8260725885629654\n",
      "epoch: 717 - train loss: 1.7468261793255806 - val loss: 0.8343553245067596\n",
      "epoch: 718 - train loss: 1.7465424686670303 - val loss: 0.8536684960126877\n",
      "epoch: 719 - train loss: 1.723580114543438 - val loss: 0.8326500803232193\n",
      "epoch: 720 - train loss: 1.7451550588011742 - val loss: 0.7414799705147743\n",
      "epoch: 721 - train loss: 1.7324246540665627 - val loss: 0.8458070456981659\n",
      "epoch: 722 - train loss: 1.7460525780916214 - val loss: 0.7599977850914001\n",
      "epoch: 723 - train loss: 1.7209998816251755 - val loss: 0.86754110455513\n",
      "epoch: 724 - train loss: 1.7259237244725227 - val loss: 0.8564195483922958\n",
      "epoch: 725 - train loss: 1.7367738410830498 - val loss: 0.8378256410360336\n",
      "epoch: 726 - train loss: 1.7395014688372612 - val loss: 0.7605203092098236\n",
      "epoch: 727 - train loss: 1.719611793756485 - val loss: 0.824453666806221\n",
      "epoch: 728 - train loss: 1.7273378372192383 - val loss: 0.7547051683068275\n",
      "epoch: 729 - train loss: 1.7328134328126907 - val loss: 0.8287904113531113\n",
      "epoch: 730 - train loss: 1.713247649371624 - val loss: 0.7745409905910492\n",
      "epoch: 731 - train loss: 1.7304212003946304 - val loss: 0.8592189401388168\n",
      "epoch: 732 - train loss: 1.7190630286931992 - val loss: 0.8336921185255051\n",
      "epoch: 733 - train loss: 1.717726856470108 - val loss: 0.8101474121212959\n",
      "epoch: 734 - train loss: 1.7504627108573914 - val loss: 0.8113042116165161\n",
      "epoch: 735 - train loss: 1.7279582917690277 - val loss: 0.7693633139133453\n",
      "epoch: 736 - train loss: 1.7173713147640228 - val loss: 0.7464096024632454\n",
      "epoch: 737 - train loss: 1.7392273843288422 - val loss: 0.7863126844167709\n",
      "epoch: 738 - train loss: 1.7224856615066528 - val loss: 0.8001818880438805\n",
      "epoch: 739 - train loss: 1.7174162343144417 - val loss: 0.7482026964426041\n",
      "epoch: 740 - train loss: 1.7302353531122208 - val loss: 0.7567847445607185\n",
      "epoch: 741 - train loss: 1.7190022617578506 - val loss: 0.7509109601378441\n",
      "epoch: 742 - train loss: 1.7057049050927162 - val loss: 0.8090052902698517\n",
      "epoch: 743 - train loss: 1.7056961357593536 - val loss: 0.773340255022049\n",
      "epoch: 744 - train loss: 1.7135446071624756 - val loss: 0.7833117991685867\n",
      "epoch: 745 - train loss: 1.710370510816574 - val loss: 0.755012720823288\n",
      "epoch: 746 - train loss: 1.7077669873833656 - val loss: 0.8900540322065353\n",
      "epoch: 747 - train loss: 1.7143968865275383 - val loss: 0.838957667350769\n",
      "epoch: 748 - train loss: 1.6989737153053284 - val loss: 0.7920676320791245\n",
      "epoch: 749 - train loss: 1.6882360354065895 - val loss: 0.824151948094368\n",
      "epoch: 750 - train loss: 1.7114507555961609 - val loss: 0.8203138113021851\n",
      "epoch: 751 - train loss: 1.6982960924506187 - val loss: 0.7920372784137726\n",
      "epoch: 752 - train loss: 1.709439791738987 - val loss: 0.7275029420852661\n",
      "epoch: 753 - train loss: 1.6814896315336227 - val loss: 0.8003451079130173\n",
      "epoch: 754 - train loss: 1.7023465782403946 - val loss: 0.7509340643882751\n",
      "epoch: 755 - train loss: 1.6947210654616356 - val loss: 0.726290337741375\n",
      "epoch: 756 - train loss: 1.7038079351186752 - val loss: 0.8425361961126328\n",
      "epoch: 757 - train loss: 1.7094448432326317 - val loss: 0.8418078571557999\n",
      "epoch: 758 - train loss: 1.705958604812622 - val loss: 0.7927823066711426\n",
      "epoch: 759 - train loss: 1.7109469547867775 - val loss: 0.7808060795068741\n",
      "epoch: 760 - train loss: 1.6763899475336075 - val loss: 0.7614972442388535\n",
      "epoch: 761 - train loss: 1.691595308482647 - val loss: 0.7882928997278214\n",
      "epoch: 762 - train loss: 1.702055774629116 - val loss: 0.8096641004085541\n",
      "epoch: 763 - train loss: 1.6958459168672562 - val loss: 0.8035088628530502\n",
      "epoch: 764 - train loss: 1.7002862691879272 - val loss: 0.7174920067191124\n",
      "epoch: 765 - train loss: 1.6838177293539047 - val loss: 0.7220473065972328\n",
      "epoch: 766 - train loss: 1.6919002383947372 - val loss: 0.7201544493436813\n",
      "epoch: 767 - train loss: 1.6893144398927689 - val loss: 0.7612350434064865\n",
      "epoch: 768 - train loss: 1.670358031988144 - val loss: 0.7319406270980835\n",
      "epoch: 769 - train loss: 1.6802871897816658 - val loss: 0.8164524286985397\n",
      "epoch: 770 - train loss: 1.6809618026018143 - val loss: 0.8347455412149429\n",
      "epoch: 771 - train loss: 1.6866269931197166 - val loss: 0.7249303236603737\n",
      "epoch: 772 - train loss: 1.6902631521224976 - val loss: 0.8194443583488464\n",
      "epoch: 773 - train loss: 1.6741917803883553 - val loss: 0.7763528227806091\n",
      "epoch: 774 - train loss: 1.6838189214468002 - val loss: 0.7563158124685287\n",
      "epoch: 775 - train loss: 1.6808220222592354 - val loss: 0.8689306378364563\n",
      "epoch: 776 - train loss: 1.6753664463758469 - val loss: 0.8311817944049835\n",
      "epoch: 777 - train loss: 1.6643049269914627 - val loss: 0.7671543210744858\n",
      "epoch: 778 - train loss: 1.6725769937038422 - val loss: 0.8361548781394958\n",
      "epoch: 779 - train loss: 1.680488996207714 - val loss: 0.8069459795951843\n",
      "epoch: 780 - train loss: 1.687426820397377 - val loss: 0.7073566094040871\n",
      "epoch: 781 - train loss: 1.675340659916401 - val loss: 0.7978727668523788\n",
      "epoch: 782 - train loss: 1.6692737266421318 - val loss: 0.7747437804937363\n",
      "epoch: 783 - train loss: 1.6720017492771149 - val loss: 0.7552056610584259\n",
      "epoch: 784 - train loss: 1.6664797961711884 - val loss: 0.7385345250368118\n",
      "epoch: 785 - train loss: 1.6752953454852104 - val loss: 0.7513778582215309\n",
      "epoch: 786 - train loss: 1.6471763849258423 - val loss: 0.7408367842435837\n",
      "epoch: 787 - train loss: 1.6643683537840843 - val loss: 0.758748009800911\n",
      "epoch: 788 - train loss: 1.6567269191145897 - val loss: 0.7853961288928986\n",
      "epoch: 789 - train loss: 1.6596669927239418 - val loss: 0.7376883029937744\n",
      "epoch: 790 - train loss: 1.6541559994220734 - val loss: 0.7905825972557068\n",
      "epoch: 791 - train loss: 1.6604156494140625 - val loss: 0.753583163022995\n",
      "epoch: 792 - train loss: 1.6526212617754936 - val loss: 0.768829420208931\n",
      "epoch: 793 - train loss: 1.6707351803779602 - val loss: 0.8147483468055725\n",
      "epoch: 794 - train loss: 1.6818756386637688 - val loss: 0.7376198396086693\n",
      "epoch: 795 - train loss: 1.6470379084348679 - val loss: 0.8258544355630875\n",
      "epoch: 796 - train loss: 1.6590587496757507 - val loss: 0.8047440648078918\n",
      "epoch: 797 - train loss: 1.6474664956331253 - val loss: 0.7366403639316559\n",
      "epoch: 798 - train loss: 1.6547937095165253 - val loss: 0.8663288950920105\n",
      "epoch: 799 - train loss: 1.6503231674432755 - val loss: 0.8988598436117172\n",
      "epoch: 800 - train loss: 1.6505208760499954 - val loss: 0.8324325233697891\n",
      "epoch: 801 - train loss: 1.6567302942276 - val loss: 0.7870706170797348\n",
      "epoch: 802 - train loss: 1.6667654439806938 - val loss: 0.7350638881325722\n",
      "epoch: 803 - train loss: 1.6351460590958595 - val loss: 0.8692048043012619\n",
      "epoch: 804 - train loss: 1.6449928507208824 - val loss: 0.7397969663143158\n",
      "epoch: 805 - train loss: 1.6443306282162666 - val loss: 0.7578855156898499\n",
      "epoch: 806 - train loss: 1.6555250510573387 - val loss: 0.7646168023347855\n",
      "epoch: 807 - train loss: 1.6354545578360558 - val loss: 0.8750362247228622\n",
      "epoch: 808 - train loss: 1.6278788074851036 - val loss: 0.7684231549501419\n",
      "epoch: 809 - train loss: 1.630697838962078 - val loss: 0.7309004366397858\n",
      "epoch: 810 - train loss: 1.6267518028616905 - val loss: 0.7737711519002914\n",
      "epoch: 811 - train loss: 1.6482512801885605 - val loss: 0.7049035578966141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x392598fa0>"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABtAUlEQVR4nO3dd3iTVfsH8G+60r2ge9BCoYxC2VBQ2SIigoqvCgq4B7yCuMC9sPzcoAiiAq8KIiigsveeBcreFFpKB1DadK88vz9OkzxpRtOZju/nunI1z0rO0xZy95z73EchSZIEIiIiIiuxsXYDiIiIqGljMEJERERWxWCEiIiIrIrBCBEREVkVgxEiIiKyKgYjREREZFUMRoiIiMiqGIwQERGRVdlZuwGWUKvVuH79Otzc3KBQKKzdHCIiIrKAJEnIzs5GYGAgbGxM9380iGDk+vXrCAkJsXYziIiIqAqSkpIQHBxs8niDCEbc3NwAiJtxd3e3cmuIiIjIEiqVCiEhIdrPcVMaRDCiGZpxd3dnMEJERNTAVJRiwQRWIiIisioGI0RERGRVDEaIiIjIqhpEzggREVFtkCQJJSUlKC0ttXZTGiRbW1vY2dlVu+wGgxEiImqSioqKkJKSgry8PGs3pUFzdnZGQEAAHBwcqvwaDEaIiKjJUavVSEhIgK2tLQIDA+Hg4MCimpUkSRKKiopw48YNJCQkoHXr1mYLm5nDYISIiJqcoqIiqNVqhISEwNnZ2drNabCcnJxgb2+Pq1evoqioCI6OjlV6HSawEhFRk1XVv+RJpya+h/wpEBERkVUxGCEiIiKrYjBCRETURIWFheGbb76xdjOYwEpERNSQ9O/fH507d66RIOLQoUNwcXGpfqOqqUn3jOw8fwPP/hKH/CIWuyEiosZBU8jNEj4+PvViNlGTDUbyikrwyh/x2HQ6DeMWHEBekWU/OCIiapwkSUJeUYlVHpIkWdTGCRMmYMeOHZg1axYUCgUUCgUWLVoEhUKBdevWoVu3blAqldi9ezcuXbqEkSNHws/PD66urujRowc2b96s93rlh2kUCgV++uknPPDAA3B2dkbr1q3xzz//1OS32agmO0zj7GCHH57ohicXHsKhK7fx15FkPNG7hbWbRUREVpJfXIr2722wynuf/mgonB0q/kieNWsWzp8/j6ioKHz00UcAgFOnTgEApk2bhi+++AItW7aEl5cXkpKScO+992LGjBlQKpX45ZdfMGLECJw7dw6hoaEm3+PDDz/EZ599hs8//xzffvstxo4di6tXr8Lb27tmbtaIJtszAgDdw7zxRIwIQM6lqqzcGiIiIvM8PDzg4OAAZ2dn+Pv7w9/fH7a2tgCAjz76CEOGDEGrVq3g7e2N6OhoPP/884iKikLr1q3x8ccfo1WrVhX2dEyYMAGPPfYYIiIi8OmnnyInJwcHDx6s1ftqsj0jGq18XAEAv+1PxNN3tER4c+sn8hARUd1zsrfF6Y+GWu29q6t79+562zk5Ofjggw+wZs0apKSkoKSkBPn5+UhMTDT7Op06ddI+d3Fxgbu7O9LT06vdPnMYjPi6ap/P33kJsQ92MnM2ERE1VgqFwqKhkvqq/KyY1157DZs2bcIXX3yBiIgIODk5YfTo0SgqKjL7Ovb29nrbCoUCarW6xtsr13C/6zWkrb+b9vnlG7lWbAkREVHFHBwcUFpa8SzQPXv2YMKECXjggQcAiJ6SK1eu1HLrqqZJ54wAgKO9LRY+2QMAkJFrPlokIiKytrCwMBw4cABXrlzBzZs3TfZatG7dGitWrEB8fDyOHTuGMWPG1HoPR1U1+WAEAMKbia6txIw8i6dXERERWcNrr70GW1tbtG/fHj4+PiZzQL766it4eXmhT58+GDFiBIYOHYquXbvWcWsto5AawKevSqWCh4cHsrKy4O7uXuOvX1SiRvv31qNELWH7a/0RxiRWIqJGraCgAAkJCQgPD6/ysvckmPteWvr5zZ4RAA52Nuge5gUA2Hq2djOGiYiISB+DkTKD2/kBAP46cg3FpWoO1xAREdURBiNlHuoaDEd7G5y6rkLrt9dhyh/x1m4SERFRk8BgpIyXiwOev6uVdvvv+OtQq9k7QkREVNsYjMiM7hast52iKrBSS4iIiJoOBiMywV5OetsX03Os1BIiIqKmg8GIjEKhwCPdQ7TbK49cs2JriIiImgYGI+V8PCoKc8eKojD/HLuO4tL6Wa2OiIiosWAwUo6DnQ2GdvCHva0CaglIzy60dpOIiIhqTFhYGL755htrN0MPgxEjbGwU8HMXVeRSs/Kt3BoiIqLGjcGICQEeIhhJyeKMGiIiotrEYMQEfw8xs2bSkqMoKK54qWYiIqLaNn/+fAQGBhqsvjty5Eg89dRTuHTpEkaOHAk/Pz+4urqiR48e2Lx5s5Vaa7lqBSMzZ86EQqHAlClTzJ63fPlytG3bFo6OjujYsSPWrl1bnbetE618dIvlfb/9khVbQkREdUKSgKJc6zwsXILk4Ycfxq1bt7Bt2zbtvoyMDKxfvx5jx45FTk4O7r33XmzZsgVHjx7FPffcgxEjRphc2be+sKvqhYcOHcIPP/yATp06mT1v7969eOyxxxAbG4v77rsPS5YswahRo3DkyBFERUVV9e1r3bN3tsS51GysO5mKedsvYUzPUPh7cGVHIqJGqzgP+DTQOu/91nXAoeIV4728vDBs2DAsWbIEgwYNAgD8+eefaN68OQYMGAAbGxtER0drz//444+xcuVK/PPPP5g0aVKtNb+6qtQzkpOTg7Fjx+LHH3+El5eX2XNnzZqFe+65B6+//jratWuHjz/+GF27dsV3331XpQbXFRelHb4f2xXRwR4oKlXjrs+3cbiGiIisbuzYsfjrr79QWChmey5evBiPPvoobGxskJOTg9deew3t2rWDp6cnXF1dcebMmcbZMzJx4kQMHz4cgwcPxieffGL23H379mHq1Kl6+4YOHYpVq1aZvKawsFD7TQYAlUpVlWZWm0KhQL9IXxy7loWiEjVWHEnGmF6hVmkLERHVMntn0UNhrfe20IgRIyBJEtasWYMePXpg165d+PrrrwEAr732GjZt2oQvvvgCERERcHJywujRo1FUVFRbLa8RlQ5Gli5diiNHjuDQoUMWnZ+amgo/Pz+9fX5+fkhNTTV5TWxsLD788MPKNq1WjOwciNlbLgAAtp9LZzBCRNRYKRQWDZVYm6OjIx588EEsXrwYFy9eRGRkJLp2FcU69+zZgwkTJuCBBx4AIEYyrly5YsXWWqZSwzRJSUmYPHkyFi9eDEfH2sufmD59OrKysrSPpKSkWnuvirTyccWqiX0BAHsv3WJFViIisrqxY8dizZo1WLBgAcaOHavd37p1a6xYsQLx8fE4duwYxowZYzDzpj6qVDBy+PBhpKeno2vXrrCzs4OdnR127NiB2bNnw87ODqWlhjkV/v7+SEtL09uXlpYGf39/k++jVCrh7u6u97CmTkEeaObigJzCEry2/JhV20JERDRw4EB4e3vj3LlzGDNmjHb/V199BS8vL/Tp0wcjRozA0KFDtb0m9VmlhmkGDRqEEydO6O178skn0bZtW7z55puwtbU1uCYmJgZbtmzRm/67adMmxMTEVK3FVmBjo8CQ9n5YeigJf8dfxwv9WqFdgHUDJCIiarpsbGxw/bphfktYWBi2bt2qt2/ixIl62/Vx2KZSwYibm5vBdFwXFxc0a9ZMu3/cuHEICgpCbGwsAGDy5Mno168fvvzySwwfPhxLly5FXFwc5s+fX0O3UDfeva89DiZk4PLNXMzdfgmzHu0MhUJh7WYRERE1eDVegTUxMREpKSna7T59+mDJkiWYP38+oqOj8eeff2LVqlX1usaIMS5KO7x7X3sAYjXfWWVJrURERFQ9VS56prF9+3az24CoGPfwww9X962sLqZVM+3z9SdTMWVwGyu2hoiIqHHg2jSV4Ghvi3mPdwMAJGbkQa22rHwvERERmcZgpJIGt/OFg60N8opKkZiRZ+3mEBERNXgMRirJztYGUUFiJk3/L7Zj6rJ46zaIiIiqTLJwgToyrSa+hwxGqqB/pK/2+YojyfxlJiJqYOzt7QEAeXns4a4uzfdQ8z2timonsDZFD3ULxg87LiG3SBR5U+WXwMO56j8EIiKqW7a2tvD09ER6ejoAwNnZmeUaKkmSJOTl5SE9PR2enp5Ga41ZisFIFQR5OmH/W4PQ8YONAIBrmXnwcPawcquIiKgyNJXANQEJVY2np6fZquqWYDBSRW6O9ugY5IETyVkYPns31k2+k1VZiYgaEIVCgYCAAPj6+qK4uNjazWmQ7O3tq9UjosFgpBpCvJ1wIjkLAPDe3yex/IU+Vm4RERFVlq2tbY18oFLVMYG1Gp7oHaZ9fujKbdYdISIiqgIGI9UQ06oZ/npRt+DfFxvPWbE1REREDRODkWrq1sIbLX1cAADfb7+EHedvWLlFREREDQuDkRrg5qib1ns2RWXFlhARETU8DEZqQH5Rifb59cx8K7aEiIio4WEwUgOignQ1Rn4/mISNp1JZlZWIiMhCDEZqwPRh7TC8UwAAoKhUjed+PYz/W89kViIiIkswGKkBPm5KzBnTFZ+MitLum7fjEnYymZWIiKhCDEZq0P2dA9E11FO7/Xf8des1hoiIqIFgMFKD3B3tseKlvvjhiW4AgL+OXMPp65xdQ0REZA6DkVrQXrZGzZt/HbdiS4iIiOo/BiO1INjLCUo78a09kZyFWzmFVm4RERFR/cVgpBYoFAocfncI/N0dAQAHEjKs3CIiIqL6i8FILXFV2mFoBz8AwKErDEaIiIhMYTBSi7qHeQMAtp1N54q+REREJjAYqUUD2/rCTWmHK7fy8H8bzqKUAQkREZEBBiO1yEVph8mDWwMAfthxGa3eWou/45Ot3CoiIqL6hcFILXvmzpZo6eOi3Z68NB45hSVmriAiImpaGIzUgalD2sDeVqHdHj13L4pK1FZsERERUf3BYKQO3NcpEOc/GYZOwWJ137Op2TiQcMvKrSIiIqofGIzUEYVCgZUv9cXgdr4AgPNpOVZuERERUf3AYKQO2doo0K6sVPz8nZfw6Px9uHIz18qtIiIisi4GI3WstZ8bACBNVYj9lzPw7daLVm4RERGRdTEYqWMxLZvpbe+/zNwRIiJq2hiM1DEfNyX6RugCkuTMfE71JSKiJo3BiBV880gX/PJUT7g52gEAot7fgLwiBiRERNQ0MRixAh83Je5q4wMHW923/51VJ1kunoiImiQGI1aUmV+sfb7iSDI+WXPaiq0hIiKyjkoFI3PnzkWnTp3g7u4Od3d3xMTEYN26dSbPX7RoERQKhd7D0dGx2o1uLJq7OuhtrziSzNV9iYioyalUMBIcHIyZM2fi8OHDiIuLw8CBAzFy5EicOnXK5DXu7u5ISUnRPq5evVrtRjcWsx/tglY+LhjbKxQAkJVfjJPXs7D7wk2WiycioibDrjInjxgxQm97xowZmDt3Lvbv348OHToYvUahUMDf37/qLWzEerVshi2v9gcAJGbkYdeFm3jmf3FIzy7Ec3e1xFv3trNuA4mIiOpAlXNGSktLsXTpUuTm5iImJsbkeTk5OWjRogVCQkIq7EVpylo2Fyv7pmcXAgDm77xszeYQERHVmUr1jADAiRMnEBMTg4KCAri6umLlypVo37690XMjIyOxYMECdOrUCVlZWfjiiy/Qp08fnDp1CsHBwSbfo7CwEIWFhdptlUpV2WY2OGFlwYhccaka9rbMMSYiosat0p90kZGRiI+Px4EDB/Diiy9i/PjxOH3a+CyQmJgYjBs3Dp07d0a/fv2wYsUK+Pj44IcffjD7HrGxsfDw8NA+QkJCKtvMBifAwzCx91xqthVaQkREVLcqHYw4ODggIiIC3bp1Q2xsLKKjozFr1iyLrrW3t0eXLl1w8aL59VimT5+OrKws7SMpKamyzWxwYlo1h7+7I/pH+qBPK1Gh9Ujibc6uISKiRq/aYwBqtVpvSMWc0tJSnDhxAgEBAWbPUyqV2unDmkdj5+Fkj/1vDcLCCT3QPcwbAPDe36fQ7r317CEhIqJGrVLByPTp07Fz505cuXIFJ06cwPTp07F9+3aMHTsWADBu3DhMnz5de/5HH32EjRs34vLlyzhy5Agef/xxXL16Fc8880zN3kUjolAo0CXUU7tdWKLGh/+eQkpWvvUaRUREVIsqlcCanp6OcePGISUlBR4eHujUqRM2bNiAIUOGAAASExNhY6OLb27fvo1nn30Wqamp8PLyQrdu3bB3716TCa8kdA721Nvee+kWRny7B7vfHABHe1vrNIqIiKiWKCRJqvdJCSqVCh4eHsjKymoSQzYAcO+sXTidoj+LaN7j3XBPFGu2EBFRw2Dp5zfnjdZT88d1w7LnYzBpQIR2364LN6zYIiIiotpR6TojVDeCvZwR7OWMHmFe8HFT4v1/TuFCeo61m0VERFTj2DNSzykUCnRr4QUAOHL1Nt788zh+3HkZDWB0jYiIyCLsGWkAWvm4QqEAStQS/ogTNVdCmzljSDs/2NgorNw6IiKi6mHPSAPg5GCLe6P0a7M8/+thDPpqBxJv5VmpVURERDWDwUgDEftQR0we1Br3dtTNpkm4mYs528xXsyUiIqrvOEzTQLg72uOVIW1wMT0ba0+kavcfSLhlxVYRERFVH3tGGphWPq5621du5SFNVWCl1hAREVUfg5EGRqFQYPZjXRDk6aTddzAhgwvqERFRg8UKrA3YB/+cwqK9VwAADnY28Hd3xM/ju6O1n5t1G0ZERARWYG0SBrb11T4vKlEjMSMPf8dft2KLiIiIKo/BSAN2Z+vmuLN1c719h65k4GZOIU5dz7JSq4iIiCqHwzQNXHGpGseSMpGcmY/JS+P1jm165S4O2RARkdVwmKaJsLe1Qfcwb0QHexoc25+QUfcNIiIiqiQGI41EqLcz7o8O1NuXV1hipdYQERFZjsFII2FjI6b87p8+SLvvema+FVtERERkGQYjjYy/hyM+GRUFAEjOzEdBcSkKikut3CoiIiLTWA6+EWrRzBkAsPlMOnrO2Aw3R3tMGdwag9v5wcvFwcqtIyIi0seekUaod8tmCPBwBACoCkqQnJmP1/88jtHz9iKXeSRERFTPMBhphOxtbTDzoU7o18YHvm5K7f5LN3IxackRK7aMiIjIEIdpGql+bXzQr40P0rML0HPGFu3+beduQK2WYGOjsGLriIiIdNgz0sj5ujlieMcAvX1Jt/Os1BoiIiJDDEaagDlju+pt9/t8O/ZfvmWl1hAREenjME0T8eH9HfD+P6e024/O3699PiDSB3PGdoWzA38diIio7rFnpIkY3ycMu94YYPTYtnM3MH/n5TpuERERkcBgpAkJ8XbG7jcHwNPZ3uDY7dwiK7SIiIiIwUiTE+zljP3TB+HlgRF6+0vU9X7xZiIiaqSYJNAEOdrbYvLgNgj2dsbSg4k4kpiJv45cQ2pWAUK8nfHO8Haws2WcSkREdYOfOE2UrY0C/+kegpf6ix6SgmI1tpxNx6K9V7D5TLqVW0dERE0Jg5Emzr+sbLzcC78dxow1p63QGiIiaooYjDRx7QLc0dbfzWD/v8dSrNAaIiJqipgz0sTZ2ijwx3Mx2HYuHcM6+uNSei7unb0LqaoCJGXk4cddl3E9Mx9zH+8Ge+aREBFRLWAwQvBwtseoLkEAoNdLcudn27TPjyVlonuYd523jYiIGj/+qUt6TC2gdyY1u45bQkRETQWDETLQNdTTYN/Rq7cBALmFJfh1/1UWSSMiohqjkCSp3le7UqlU8PDwQFZWFtzd3a3dnEbvemY+Dl3JwP3Rgdhx/gYmLDwEAHCwtcHwTgFYeTQZwV5O2PXGACgUxntSiIiILP38Zs8IGQj0dMLIzkFQKBTo18YHLw9qDQAoKlVj5dFkAMC12/k4kZxlzWYSEVEjUalgZO7cuejUqRPc3d3h7u6OmJgYrFu3zuw1y5cvR9u2beHo6IiOHTti7dq11Wow1S2FQoGpQ9pg2rC2Bsc2nEq1QouIiKixqVQwEhwcjJkzZ+Lw4cOIi4vDwIEDMXLkSJw6dcro+Xv37sVjjz2Gp59+GkePHsWoUaMwatQonDx5skYaT3Xn+btaYkKfML19c7Zdwp2fbcVPuy6jsKTUOg0jIqIGr9o5I97e3vj888/x9NNPGxx75JFHkJubi9WrV2v39e7dG507d8a8efMsfg/mjNQfP+y4BABYfyoVRxMz9Y4dfHsQfN0MK7oSEVHTVOs5I6WlpVi6dClyc3MRExNj9Jx9+/Zh8ODBevuGDh2Kffv2mX3twsJCqFQqvQfVD8/3a4Xn+7XCq0MiDY69tYI9XkREVHmVDkZOnDgBV1dXKJVKvPDCC1i5ciXat29v9NzU1FT4+fnp7fPz80Nqqvlcg9jYWHh4eGgfISEhlW0m1bI7WjfHmF6hevs2n0nDsrgkK7WIiIgaqkoHI5GRkYiPj8eBAwfw4osvYvz48Th9umYXVZs+fTqysrK0j6QkfsDVR1MGt4ZduSJpb/x5HLHrzgAAJElCSanaGk0jIqIGpNLl4B0cHBARIZad79atGw4dOoRZs2bhhx9+MDjX398faWlpevvS0tLg7+9v9j2USiWUSmVlm0Z1zNfNETvfGAAne1uMW3BQO9X3hx2XceByBtSShFs5Rdjwyl1wVXLlASIiMq7adUbUajUKCwuNHouJicGWLVv09m3atMlkjgk1PIGeTvByccA/k/qif6SPdn98UiaOX8tCcmY+dpy7YcUWEhFRfVepP1enT5+OYcOGITQ0FNnZ2ViyZAm2b9+ODRs2AADGjRuHoKAgxMbGAgAmT56Mfv364csvv8Tw4cOxdOlSxMXFYf78+TV/J2RVCoUCPzzRDWdTsjFyzh69YyuPXsPJ61mYOCCCPSRERGSgUj0j6enpGDduHCIjIzFo0CAcOnQIGzZswJAhQwAAiYmJSElJ0Z7fp08fLFmyBPPnz0d0dDT+/PNPrFq1ClFRUTV7F1QvKO1sER3iiWPv3a23f/OZdMzdfgmrj123UsuIiKg+49o0VCvCpq0x2PfsneF4e7jxmVdERNT4cG0asioHW8NfrbUnUnHlZi5+2HEJF9OzrdAqIiKqj9gzQrVi5/kbePG3w/hwZBT83R3x+M8HDM45/M5gNHPlrCkiosaKPSNkVXe18cGJD4ZidLdgtAtwg4Od4a/aI/P3o6C4FIm38lCqrvcxMRER1RL2jFCdSMrIw7Frmfh49WmkqQyngr8yuA0mD25thZYREVFtsfTzm/MsqU6EeDsjxNsZ93UKhCRJCJ++Vu/415vPo0OgOwa39zPxCkRE1FhxmIbqnEKhQO+W3gb7n/klDldu5gIACktK0QA67YiIqAYwGCGrmPVoF8x6tDMuf3ovJg5opd2/4/wN7Dh/A5HvrMfiA4lWbCEREdUVBiNkFX7ujhjZOQg2Ngq8PrQtXhncBgBwJPE2nl50CADwzqqT2t6RzLwifLP5PBJv5VmtzUREVDuYM0L1QvcwLwDA3/H6VVqn/XUCkf5u+GbzeagKSrDiSDJ2vjHAGk0kIqJawmCE6oVe4d4I8nRCcma+3v4/4pL0thMz8vD99ot4qX9EXTaPiIhqEYdpqF6ws7XBpw92xJD2fujewsvsuZ+tP1dHrSIiorrAnhGqN/q18UG/Nj4AgL2XbmLMj4ZVWzWm/hGPW7lFePXuNugU7FlHLSQiotrAomdUb+UWliDu6m0sj0vC6uMpJs97f0R7PNk3vA5bRkRElmA5eGrwXJR26NfGB7Me7YIOge5o7euK14dGGpz34b+nsSwuCf0/34aL6TlWaCkREVUHe0aoQShVS7BRiIJpG0+l4rlfDxs9r2uoJ1a81LeOW0dERMawZ4QaFVsbBRQKBQBgYFtfdAzyMHre5bIKrkRE1HAwGKEGx87WBv97qqfRY5l5xfi/9Wdx5WYuVh+/zpLyREQNAGfTUIPk5WyP+6MDcTuvCLsu3NQ7Nnf7Jfy8KwFFpWqk3VeIp+9gcisRUX3GnhFqkBQKBWY/1gW/Pt0Lrw5pg1Y+Lvh5fHft8aJSNQDg49WnMe2v49ZqJhERWYAJrNSo/H4wEdNXnDDYb6MAfniiO4a097NCq4iImiYmsFKT9J/uIRgX0wJ9I5rhjojm2v1qCXj2lzj8uv8qsvKKrdhCIiIqjz0j1Ghl5Rcj+sONRo91DPLAgLa+yMorwnsjOsDWRlHHrSMiavzYM0JNnoeTPfZNH2j02InkLMzecgH/23cVcVcy6rhlREQkx54RavTSVQX4aPVpbD6ThoJitcHxYC8nXLstVgte8mwv9A5vBkVZgTUiIqo6Sz+/GYxQk7Hrwg088fPBCs9T2tlgXEwLvD28fR20ioio8eIwDVE5d7b2wYqX+mDjK3eZPa+wRI0fdyVg/UnTi/MREVHNYc8INUkL9yQgr6gUgZ6OeHvlSfQK90bnEC98vfm83nndW3hBoQCWPhfDJFciokqy9PObFVipSXqyr64q67CoACjtbFBYojYIRuKu3gYAXLmVi1Y+rjh9XYUfd13Gy4NaI7y5S522mYiosWIwQk2eo72t9uual+9A8u18JGbk4ZM1Z7TnpGUVINjLCffO3gVA5JXMfKiTVdpLRNTYMGeESKZDoAfu7uCP/pG+evsX7LmCyHfWa7fPpWXXddOIiBotBiNERkT4umLqkDba7c1n0vSOH03MxJ+Hr3FVYCKiGsBghMiElwe1xsQBrUwef235MYRPX4t5Oy5BrZZw4loWCopL67CFRESNA2fTEJlx7XYepq84geauSrw+NBIBHo549pfDBj0lci/0a4U374lk0TQiavJY9IyoFpWUqjHmpwM4mGC8lPyP47hCMBERi54R1SI7Wxv8NL47Qr2djR5fd4IF04iILMWeEaJqiruSgdHz9hk91q2FF34a1x1ujnZQFZRgWVwSRncLRnNXZR23koio7tVKz0hsbCx69OgBNzc3+Pr6YtSoUTh37pzZaxYtWgSFQqH3cHR0rMzbEtVr3cO8MT6mBe6IaI4/X4hBS1kxtMNXb6PLx5sQ8fY6dP14E2auO4vHfzqANFWBFVtMRFS/VKro2Y4dOzBx4kT06NEDJSUleOutt3D33Xfj9OnTcHExXY3S3d1dL2hhYh81Nh+OjNI+3zy1H2ZvvYBvNl8weu7Z1Gz0+nQLAGDyoNZ4RTaFmIioKapUMLJ+/Xq97UWLFsHX1xeHDx/GXXeZXnxMoVDA39+/ai0kamBsbBSYMrgNNp5Kw+kUld4xFwdb5Bbppv/O2nIBh65kIPbBjmjRjOXliahpqlYCa1ZWFgDA29vb7Hk5OTlo0aIFQkJCMHLkSJw6dcrs+YWFhVCpVHoPooYmr6hE+3xQW1+8cU8k9kwbaHDe3ku3MHf7pbpsGhFRvVLlYEStVmPKlCno27cvoqKiTJ4XGRmJBQsW4O+//8Zvv/0GtVqNPn364Nq1ayaviY2NhYeHh/YREhJS1WYSWc2kga0BAA92CcLPE3rgpf4R8HR2wBO9WwAAwprpZuLsv3wLBcWlWHM8BSeTs7DmeAp+P5holXYTEdW1Ks+mefHFF7Fu3Trs3r0bwcHBFl9XXFyMdu3a4bHHHsPHH39s9JzCwkIUFhZqt1UqFUJCQjibhhoUtVrCqesqtPZz1S7GV15WXjGiP9po8jU2T+0HpZ0N3B3t4eFsX1tNJSKqFZbOpqnSqr2TJk3C6tWrsXPnzkoFIgBgb2+PLl264OLFiybPUSqVUCo59ZEaNhsbBToGe5g9x8PZHqO7BePPw8Z7Cr/ffhH/xF9Hx2APzBnTFQXFpWjp41obzSUisppKDdNIkoRJkyZh5cqV2Lp1K8LDwyv9hqWlpThx4gQCAgIqfS1RY/TJqCi8Mtj4jJoVR5JRopZwNDETfWZuxbBZu3A9M7+OW0hEVLsq1TMyceJELFmyBH///Tfc3NyQmpoKAPDw8ICTkxMAYNy4cQgKCkJsbCwA4KOPPkLv3r0RERGBzMxMfP7557h69SqeeeaZGr4VoobJ0d4Wkwe3xmM9Q3DyehbOpGSjsESNVUeTkZiRp3duYYkaX286jzvb+GBEpwDkFZXC2cGW0+WJqEGrVM6Iqf/wFi5ciAkTJgAA+vfvj7CwMCxatAgA8Morr2DFihVITU2Fl5cXunXrhk8++QRdunSxuJGswEpNUXGpGr/su4qPV582erxXuDcOJGTg6TvC8dQd4SguUSOsOacHE1H9wYXyiBqRUrWEmzmF2mJppgyL8sfEARFoF+AOWxv2lhCRdXGhPKJGxNZGAT93R0T6uZk9b93JVNz37W78vPtyHbWMiKj6GIwQNSCLnuqBmQ92RJdQT739A9v66m3/uCsByw4loeeMzXjulzhsO5eOYbN24e/45DpsLRGRZThMQ9QAqQqKsffiTbgq7ZFTWILCklJMXhpf4XU+bkocentw7TeQiAi1XGeEiKzL3dEe90TppsdnFxQj0s8N0SEeuJ5ZgN0Xbxq97kZ2IdJVBfB1FytnS5KEv44ko3OIByJ8zQ8BERHVFvaMEDUykiRh3clUJNzMRa9wb4yet8/gnPs6BaCVjytclXaYsfYMvF0csPvNAXB24N8nRFRzOJuGiAAAN3MKcfxaJuKTsjB7ywWT530yKgqPl62bQ0RUEzibhogAAM1dlRjY1g9Th7TBQ11NL9+w4sg1NIC/TYioEWIwQtSExD7YETMf7Ki3r18bH9jbKnAkMRNP/HwQD83dK6q/3hLVX4tL1dZoKhE1IRwgJmpCHOxs8HD3EMRdvQ1nB1uMi2mBUG8X/LDjEr7cdF6b+Hr46m2Da394ohuGdvCv6yYTURPAnBEigqqgGMO+2YXkChbh2/5af5acJyKLMYGViColr6gEagm4cjMXz/4Sh5SsAqPndQn1xDvD2yMzrwg5hSW4u70/HO1toJaAVUeT4e/hiL4Rzeu49URUHzEYIaJqGfTldly6kWvRuXe18cF9HQPwxl/HAQBrXr4DHQI9arN5RNQAcDYNEVXL78/1xvRhbbXbHYNMBxc7z9/QBiIAcCpZVattI6LGhcEIERnl6+aI5/u1wqxHO2PasLb4/bne6Fq2Jk7PMG+EejujT6tmRq89eT1LO024uFSN5Mx8bDuXDrW63nfEEpEVcJiGiKrl6q1c9Pt8u8F+L2d7vNCvFY5dy8TaE6kAgEFtfTGonR/G9Aqt41YSkTUwZ4SI6kx6dgHm77iMFs1d8O6qkxWe/8dzvdEz3Bulagl2tjbILSxBSakED2f7OmgtEdUVLpRHRHXG180R79zXHqVqCar8Yvi4KfHGn8dNnj9vxyXsvngTP+y4jKXP98ZbK07gYnoOvnm0M+7rFFiHLSei+oA9I0RUK86kqLA87hoW7ElAoIcjrpuYKlyen7sSsx7tgp5h3jiadBtt/d3houTfTUQNEYdpiKheuJieA2cHWwz+agd83JSI8HHFlrPpFV7X2tcVF9Jz8FDXYHz5n2jMWHMaxaUS3h/RHgqFog5aTkTVxWEaIqoXInxdAYjqrY4OtnCwtcHsLRewLO4abuYUmrzuQnoOAOCvI9cwoK0PftyVAAB4tGcI2vrzjxKixoRTe4moTvi6O8Ld0R6O9rZ44562iHtnMF67uw0A4PWhkfBw0iWvNnNx0Lt20pKj2ud7L96qmwYTUZ1hzwgRWc3EARF4tGcomrsqcep6lnYK8OF3hyArvxjRH240uOaj1acR5OWEo4mZCGvmjEd7cpowUUPHYISIrEahUKC5qxIA8PHIKGTmFWNgW18AgLuj6f+env/1sPa5s9IOPcO84e/hiDRVARztbDlFmKiBYTBCRPVCM1clljzbW7tdPknV2cEWeUWlBte9/LsYwvF1UyI9W+SgBHg4YuqQNri7vT9clLaws+WINFF9xtk0RFRv7b98C/sv38KkAREAgKTb+RjwxfZKvUb/SB8serInbuUUYtqKExjeMQCjugTVQmuJqDxO7SWiRikpIw+XbuSgc4gnNp5Kw/LDSTh05bbZax7rGYLdF28iKSMfALDkmV7oE9G8LppL1KQxGCGiJiFNVYBX/ojHwLa+GNzOD4v2XsHTd4Tj8s1cvPz7UWTlFxtc0y7AHesm32mF1hI1LQxGiKjJyyksQezaM1h8INHgWHSIJz59IAolpRIycotwZ+vmOJ6chZJSCWpJwu4LNzFlcGvmmxBVA4ueEVGT56q0w0cjo3DlVi6KStRY/ExvTFpyBBtPp+FYUiaGz95t9no/D0ccSsjArdxCzH60C5qVzfwhoprFnhEialJu5RTit/2J+Hrz+Upd93jvUHwyqmMttYqocbL085v9j0TUpDRzVWLy4Nb4Z1JftPV3w+B2vghv7lLhdSeSVdh0Og1P/HwA127nGT2nuFSNgmLD6cdEZB57RoioyUu4mYuVR65h9taLFp3v66aEBFHG/j/dQ7T7n1x4EPFJmdg0tZ+2mBtRU8YEViKiSrpyMxfTVhzHxAERiAr0wKr4ZHz472mz1wxp74fw5i547q6W6P7JZgDAh/d3wKjOQbCzVcBFydQ8aroYjBAR1YCSUjWmrTiBPw9fAwAEeTohOTPf7DV3RDTHwSsZsFEA90YF4P7Ogegf6VsXzSWqVxiMEBHVIEmSIEmAjY0Cl2/kICWrAEeu3saO8zdw7FomikvN/1e6Z9pA5BSUoLCkFEo7WyRn5mFgW786aj2RdTAYISKqI/lFpfhx12V8u/WCyaDETWmH7MISvX29wr2xYEIPDuVQo1Urs2liY2PRo0cPuLm5wdfXF6NGjcK5c+cqvG758uVo27YtHB0d0bFjR6xdu7Yyb0tEVK85Odji5UGtcf6TYYh7ZzD+91RPLHmmF3qFe6OtvxsAGAQiAHAgIQMd3t+AyUuP4kbZIn8aRSVqFJWo66T9RNZWqWBkx44dmDhxIvbv349NmzahuLgYd999N3Jzc01es3fvXjz22GN4+umncfToUYwaNQqjRo3CyZMnq914IqL6RKFQoLmrEv3a+KBPRHP88XwMnrojXO+cR2SzbzT+jr+OHjM2Y0lZpdhStYQHvt+DAV9sR76RlYqJGptqDdPcuHEDvr6+2LFjB+666y6j5zzyyCPIzc3F6tWrtft69+6Nzp07Y968eRa9D4dpiKihyi0swadrz6BnuDdGdg6CWi1h+Le7cSZFBQBwcbBFblnA4eFkj2XPx+DSjRy8tPgIAOD3Z3ujfaA7xvy4H75uSiyY0AMKhcJq90NUGXVSDj4rKwsA4O3tbfKcffv2YerUqXr7hg4dilWrVpm8prCwEIWFui5LlUpVnWYSEVmNi9IOMx7QVW61sVFg+QsxmPbXcYyIDsTQDv5IzSrA6Hl7ce12PoZ+s1Pv+pVHr+GPQ2qcuq7CKQDXswoQ5OlUx3dBVLuqXIFVrVZjypQp6Nu3L6Kiokyel5qaCj8//YxxPz8/pKammrwmNjYWHh4e2kdIiGG3JhFRQ+WqtMN3Y7piaAd/AIC/hyMWPdkTd7XxQflOj2Vx17Aq/rp2e+R3u3EjuxAHLt+CJEm4XsE0Y6KGoMo9IxMnTsTJkyexe7f5haaqYvr06Xq9KSqVigEJETVqEb6u+OWpnriVUwg7GxukZxdgyNc7Dc67mVOEHjM26+2bPKg1/D0cseLINfQKb4aw5i54qGsQh3OowahSMDJp0iSsXr0aO3fuRHBwsNlz/f39kZaWprcvLS0N/v7+Jq9RKpVQKllKmYiaHs3KwO5OdogO9sCxa1mY8UAUBrb1RUzsVqPXzNpyQfv80JXbAIDXlh9DsJcTAj2ccE+UP+7vHMgS9VRvVSqBVZIk/Pe//8XKlSuxfft2tG7dusJrHnnkEeTl5eHff//V7uvTpw86derEBFYiIjOy8oqRlV+M0GbOAICcwhJM/SMeG0+LP/B6hHlpg4+K2NsqsGfaQPi6OWr35RaWoEQtwcPJvuYbT4RaKnr20ksvYcmSJfj7778RGRmp3e/h4QEnJ5FQNW7cOAQFBSE2NhaAmNrbr18/zJw5E8OHD8fSpUvx6aef4siRI2ZzTapyM0RETUFSRh583JRwtLfFO6tO4Lf9Ykrw6G7BuLejP575XxzURv5njwpyx5N9wpGWXYD9lzOw8/wNeDnbY8cbA3D6ugr+7o4Ia+6CklI1CkvULMZG1VYrwYip8ceFCxdiwoQJAID+/fsjLCwMixYt0h5fvnw53nnnHVy5cgWtW7fGZ599hnvvvdfSt2UwQkRkQrqqAO//cwpPxLRAn1bNAQDLDiXhWmY+nuobhtXHU/Dz7gQk3DRdD2pU50Csir+O8OYuWPVSX0z6/QiOXL2Nxc/2RucQzzq6E2qMWA6eiIi0zqaq8MCcvcgvrlwRtT6tmqFnuDcSM/JwMjkLvz3TS2+oh8icWikHT0REDVNbf3f88nRPDIvSTR54Z3i7Cq/be+kWvtl8ASuOJON8Wg5eX368NptJTRR7RoiImpi/Dl9Di2bOaO3rhsFf79BbF2d8TAsMbOeH8QsOmn2NR7qHYOZDHaFQKFBQXIpjSZnoGe7N6cSkh8M0RERUoaSMPFxIz0afVs2hyi+Gr7sYgun28Sbcyi2q8Pp5j3fDzgs3sORAIl4fGomJAyJqu8nUgDAYISKiKrtyMxf7L99CsJczHv/5AACgZ5g3OgS5Y+GeKyave6hrMB7vHYoWzVygyi/Grgs34GBng6ggDwR5OsHT2QE3cwoRu/YsHu0Zgh5hppcToYaPwQgREdWIrPxi/LzrMoZ3CkSkvxtSswowZ9tFHEzIwLm07Eq91sC2vth6Nl27fWXm8JpuLtUjDEaIiKhWFRSX4n97ryDA0wnNXRww5qcDlX6NHmFeeKFfKwyI9IWNDfNNGhsGI0REVKcupudg0pIjuHwzF0Ulau3+rqGeOJKYafba3i298frQSDg72KGguBRezg7wdnXAp2vOYHS3YHQvN5wjSRJu5xVj46lUPNA1CEo729q4JaomBiNERGQVOYUlGPfzAUT4uuKz0dEAgGNJmVh84CrWnUxFdkGJRa8T6u2MxIw8AMAPT3TD3e39oFAokKYqwJCvdkBV9joPdgnCV490rpV7oephMEJERPVObmEJrt7KQ35xKSJ8XDF76wX8vDvBomtfHxqJIe398PmGc9h0Wn8B1h2v90eLZi4AgOJSNextWUarPmAwQkREDUKpWsLbK09g6aGkar3O3mkDse1cOt5eeRI/j++OQe38aqiFVFUMRoiIqMHIyivGJ2tOo2sLL2TlF2N0t2As3p+I4lI1SiUJc7dfqvRrfjIqCg90CcLZVBW6teAUYmtgMEJERI3CrZxC9P9iOwI8HHFPVABmb7lQpddZ+lxv9Ar3xoojyWgf6I52Afw8qW0MRoiIqNG4nVsEBzsbuCjt0OmDDdrk1eauDriZo6sUW9HMnSmDW+ObzSKYWTihB9KzC/Bg12DmmNQSBiNERNQopWTl41J6LvpGNNOuhZOmKkDCzVz0btkMYdPWVOr1nu/XEtOHiUUDz6aq8MofxzBxQCu0D3BHSx/XGm9/U8JghIiImqSVR6/hlT+OISrIHXPHdsPq4ylYeigRV2/lmbymta8rFArgfFqO3v4vH47Gg12DuABgFTEYISKiJuvKzVyEeDvDVlbVNSu/GH/HJ+OHHZeRnJlv8WvZ2SjQLsAdvzzVExKA49cycUdEc9iZGdopKC7Ftdv5iPBt2j0rDEaIiIhMuJieg42nU9EjzBvxiZmYsfZMlV7Hyd4WMx/qiF7hzeBkbwsPZ3sAwEf/nsaCPQn4bHQn3N3eD57ODjXZ/AaDwQgREZGFrtzMxYoj1xB39TYKS9T4aGQHODvY4Y9DSVi4JwGFsvL2prgp7TC2dwsEejrivb9Paff7uCkxY1QUeoR5w8ulaQUlDEaIiIhqwKErGXh43j4AwFv3toWnkwPe+Ot4pV+nR5gXlj0f06TyTxiMEBER1ZDNp9Nw7XYexvcJQ2GJGt0/2YycQsvW2JEL8nSCi9IW+cWleKBzEKBQoH2AG+5u74+EW7lYczwFzg62eLJvuF6+S0PFYISIiKiWJGXkoVQt4XZeEfZeuoU7Iprjl31XsfLoNaglwMHOBp1DPHE9Mx8Otja4fDO3Uq/f1t8NSntbPH1HOO6PDqylu6h9DEaIiIjq2I3sQqgKitGqXH2S65n5GDlnD25kF8LWRoFSteUfvdHBHpj1aBfkFZVi85k0PNG7hTb35GJ6DpR2Ngjxdq7R+6gpDEaIiIjqEbVaglqSYGdrg//+fhT/HrtucM5L/VvhewvW4RnZORAPdAnCM/+Lg5ujHba/PgClagnujnbaKce3cgrx3t+ncF+nAAzrGFDj92MJBiNERET1VHZBMWZtvoAe4d545Y94ONjZ4NDbg7H1bDqe//UwAOC7MV0wacnRSr3u8I4B+ObRzrC3tcHYn/Zjz8VbAIArM4cDENVrU7IK0DXUq2ZvyAQGI0RERA1A4q08KO1t4OfuiOJSNd5acQKdQz0xtlcLFJWoMWHhQey9dAvjY1rg8s1c7Lpws9Lv8fHIDhjUzg93f70TuUUlWPPfO5FfXIquoZ61OruHwQgREVEjcCunEJdv5qJHmLd235xtF/FP/HV8O6YLbucW4bttF6sUpHz5cDTOpWUjv6gU4/u0QISvW002ncEIERFRU9L9k824mVNYqWva+rshTVWA23nF+POFGHSXBTw1wdLPb7safVciIiKyiiXP9sKBhAyM7RkKGxsFJEnCubRs/HX4GrxdlIgO9sDx5CzMXHdWe83Z1GwAgNLOBtEhnlZqOYMRIiKiRqGNnxva+OmGWRQKBdr6u+Pt4e21+7qFeSE1qwCD2/nht/1Xsf5UKgBgYFtf2JtZ+K+2MRghIiJqIpR2tvjg/g4AgB7hXlh7IgUpWQV4sEuwVdvFYISIiKgJUtrZ4gErByEa1uuTISIiIgKDESIiIrIyBiNERERkVQxGiIiIyKoYjBAREZFVVToY2blzJ0aMGIHAwEAoFAqsWrXK7Pnbt2+HQqEweKSmpla1zURERNSIVDoYyc3NRXR0NObMmVOp686dO4eUlBTtw9fXt7JvTURERI1QpeuMDBs2DMOGDav0G/n6+sLT07PS1xEREVHjVmc5I507d0ZAQACGDBmCPXv2mD23sLAQKpVK70FERESNU60HIwEBAZg3bx7++usv/PXXXwgJCUH//v1x5MgRk9fExsbCw8ND+wgJCantZhIREZGVKCRJkqp8sUKBlStXYtSoUZW6rl+/fggNDcWvv/5q9HhhYSEKC3XLIKtUKoSEhFS4BDERERHVHyqVCh4eHhV+fltlbZqePXti9+7dJo8rlUoolco6bBERERFZi1XqjMTHxyMgIMAab01ERET1TKV7RnJycnDx4kXtdkJCAuLj4+Ht7Y3Q0FBMnz4dycnJ+OWXXwAA33zzDcLDw9GhQwcUFBTgp59+wtatW7Fx48aauwsiIiJqsCodjMTFxWHAgAHa7alTpwIAxo8fj0WLFiElJQWJiYna40VFRXj11VeRnJwMZ2dndOrUCZs3b9Z7DSIiImq6qpXAWlcsTYAhIiKi+sPSz2+uTUNERERW1bSDkd9GAzNbABe3WLslRERETVbTDkaKcoCCTKAw29otISIiarKadjDi4Cq+FuXo9qlLrdMWIiKiJqppByNKTTCSK77evCiGbbZ8ZL02ERERNTFNOxhxcBFfNcM0G98GirKBXV8aPz/9DLBsPJB+tm7aR0RE1AQ08WDETXzd+jGQdgpQJZs/f+kY4PQq4NdRYlutBpaOBf75L1BSaO5KIiIiMsEqa9PUG5phGgCY2wdw8jJ/fsZl8TU7RXy9eQ44u1o8P7ceeOUUYOdQ8+0kIiJqxJp2MOLgqr+df1v3XK0GbMo6jq7u0wUdcqXFuue56cDN84B/VM23k4iIqBFr4sGIi+ljhSrAyRPITAQW3mN4/OJmwL7c9bb2uufqUvFgTwkREZFZTTtnROlm+lhBpvh6dZ/x4+vfEgGLnDxv5MeBwOwuxnNJJAnYNwe4tK1SzSUiImqMmnbPiKQ2fWxWNODZAsi8avy4jZ1hsbSSgrKvRUBKvHiefgYI7Fz2fpJ4JOwANrwl9n2QVdXWExERNQpNu2ekOM/8cVOBCADY2AIF5QKJn4eImiWaXhVAf+hm8WiRKHv9iP51FzcD22eKPBUiIqImpmkHIwFdqn5t6nHg4HzD/UcX6yfCzu0jelDUpSLouHFGv6iauhT47SFgeyxwbk3V2yOnVot23DhXM69HRERUi5p2MBLcDXj8L+Dlo5Zf4+Kre37DSPGzvJtAfqb+vsOL9EvOy8mHejKTLG/H9XhRG8WYk38Cf78EzOlp+esRERFZSdMORgAgYjDg3VJ/36h5ps9/wMwxQAQX8mEaANj4DnDzgonzZUmwNra656kngOPLjF9ToALm9xO9LsbW0kk6YL6NRERE9UjTTmCVs3MCSvLFc/cA0+e5B5p/nUKVYc8IIIZhjMm9oXuukMWG8+4QX12aA60G6l+Td1P3PD8TcGmmf9xcYi4REVE9w54RjYhB4qujh2ExNLmKqrTmpOvnjGhc3Gz8fNV186+XctxwX3GB7nneLcPjDEaIiKgBYTCiMWI20Hsi8OR6oFmEbv+L+4CW/XXb9s6G17r66Z5f2Gg4W8acLNl6OMX5hsev7AYWPwxkJOj2yfNM8jMMr2EwQkREDQiHaTRcmgH3fKrbnnwcsHME3PyAyHuBy9vFfgcXoNcLwIGy3BEHN8A9CMhJ0117fr3l73tsie65sanGFzeJr/m3gWfKeleKZMGI0Z4RyfL3JyIisjIGI6Z4tTC+38YWGPC2Lhhx9THsiShff8SclGO655oZNyVFhuddOwTsmS3ySm6c0e3PM9YzIgtGSov1a50QERHVMwxGqkK+po2dI6AuqfxrKGwBqdxMmOQjwNo3gAsbjF+z6V3DfcZ6RiALRorzAFsP/cNXdosKsqG9RTBz/ahIklUoKnULZmkKuNlwJJCIiMzjJ4UlXHz0t+VTcO2UVQtG7v7EcN/VPcDBH4DbVyx/nZx0w33y9hxfpr8+Tn4msGg4sGCo6DX5dRTw24PA0V8tf8+KqNXAT4OAnwayqiwREVWIwYgl2o8Eej4HPPSz4TE7R+O1Piri5AmE96t203B+vW7NG0C05fTfuuNrXwM2f6jbPv6H7nlxnm6YKG5h9duikX9bJPFePwrkGgmWiIiIZBiMWMLGFrj3c6DjaMNjdo5AuxGVf01JAjxCqt+2jEvAP5OAL9oAty4B8Ut0C/Zp7J8jvqadAta9odsvn71jbFZOeXu/A86urfg8dbHuubFVi4mIiGQYjFSXnSPQ703ggfnA3TP0j/m2B0Yv1A3zdHlCd6y0CPCJrJk2HP1N9EBs+xQ4+Zfxc0qKRK6InHz2Tp6R2igapSXA1X3AxreBpY9V3B55kGOqDD4REVEZJrBWl50SsHcEoh8Bzsmm9D70s64nJSAaOLVCDPVocjNKi4AOo4wnpZrTd7IoB394oZhZI5/JI6kBjyDj1/35JHB2tf4+edBQWDYDSF0qAgilO/DX00DaabF6sZu/8dfNuAwc+QXo/RLgWrZuj7xnRl4ThYiIyAgGI1UV3g9I2AH0fFa3Tx4YyIdumrUC7npdPI8aDVzaIr66NAOGfS5mxPSfBuz6UvSmrJ6iX7dEY+T3QOcx4n36/Ffki2x4S3f81ArAp63x9pYPRAAxe6e8P58UQdX4f/V7WTIu656rS3VJvAuHA9nXgfSzwJilYp+8x6WQPSNERGQeg5GqGvsnoEoGvMN1+1r2A/w7AmF3ih4TYx76ScxisXMQ272e0x276zXxdd2bxq+NekhMv1XYigDHu5XhOcZWEjbln0n625KkS37d953p667FAaG9xPPssnL2V/fojpcvV//3RLEgYYcHLG9bbZEHUkREVC8wZ6Sq7Bz0AxFA1B95YTdwj4lF8QARTGgCEZOvbSSQmXpWDAfJeYVZ1FSL5coW4JMv4FfegruBP57QVaUFAFvZPZXIhn/2fy9yWpZPkL32LeCbjqKmSl3KvQV8GQn889+6fV8iIjKLwUh9dN9X4qu8vomxlYQ9Q02/hqnhGnPST+me3zxv/twz/wC/jNRty4MRec/IrUuG117YCGQmipoqxirIAsDtq8Cc3sDhRRU222KHF4og68gvNfeaRERUbQxG6qPwu4C3rouy8+Y4GFm0TyO4u+G+YZ+Zf700WTBitLKrGdnXgevx4rk8gVXeS5J+VgwFlcrK3V/cYvhamYnAsnGi7P2/kyvXDnM4zZiIqF5iMFJfObgAbe8Tz0N6mT5v+JeG+x7/C/AMM9wfMRh4xsiHv4Y8GKmK+f2AVS8Be2bp9smrwX7fC9g/V78Qmvx52mng2+5iCCcl3vD1b14AEnZWvX2lRtb8ISIiq2MCa33m6gNMSwLszfSA9HgGaBahGzIZ+K4IOm5e1J3TeyLQvLVIegUA52bGez6qG4wAQPxi88c3vw90e1K3rboukmadvIH/3Wf+2u/KenteOiBmMrkFAO3vNzxv7etA0gHgyXX66wjJgxFJqtm1eORuXwFO/wN0fwpQugLH/hCrM49eCDh71857EhE1YAxG6jtH94rPCe8H9HlZzOTp9B+xz8lLd/yeT/XPNzVckX5afG0eCdw8V/m2WkJho58cu+87YF8lX+PYEl3vywdGVkg+OF98Pf6HCAg05MHIovvE9lPra352zfz+oiT+7Ssi/2dl2YypnV/ofhYZCUDSQaDjw1xMsK7VZiBKRFXC/wUbA4UCuPtjXSACiB4D/45A96cNzy8fjIT2EV81H9aaKcZyVUmINdpWG/MzdYxRq/WTYuW9PitfELkqquuiFP5vspL9GQn6ryMPRq7uBq4dBLJTxHbODWDf9yKhNvkIUGAkyLFUflk1W/lsI/l+AJjdWQQpR5lMW6e2zgC+jgKyjdTxqaqiXFGhmItCElVZpYORnTt3YsSIEQgMDIRCocCqVasqvGb79u3o2rUrlEolIiIisGjRoio0lSrF3klMM9bMzJGT95rETDIcOvDrYHjNxAM1067iPODKrspdU6gSDw35OjrHfhe5Kj8PBVa9CFzcpDuWlVTudYwUYNv7regl+WUksGE68Fk48OMAYPVU4225cQ7ITDJ+rLzyf30rjPxzKx+wUO3a+Rmguqaf11Rdix8GFt4DHPqx5l6TqImpdDCSm5uL6OhozJkzx6LzExISMHz4cAwYMADx8fGYMmUKnnnmGWzYsKHSjaUa8ugSwKedyGEY8jGgdNMda9FXVIGtT/Z+C5xYrttONDKuk5VouE+Vor9dkGl4zoF5IjhKL5cvc/JPw3NzbwJzegLfRFXYZKFcMGJsOEae4FtdkiRqqVDFpCqstG2KpuBf3AIgP1O/WjERWaTSOSPDhg3DsGHDLD5/3rx5CA8Px5dfilkf7dq1w+7du/H1119j6NChlX17qgkhPYCJ+3Xbob1FDwMADHqv8uPprQaJEvdVFTFY9MaY+mt11xdVe11NdVhA9Ghc2lq568vnFmhyagCxeKCt7J9PcYH4gJMnzBr0jBjJTSmtwWBk5+fAthnAf341nthLtUuSgC/biunsL8cbFkUkIpNqPWdk3759GDx4sN6+oUOHYt++ymYtUq3p9IjoKQnoDAT3EPs6jzU870ET3dC29ob7Wt8NPL8TiLZglV+ftsCQj/T3dXiw4usqorouyr8Dokejss6uFtONNeR5K6prYmgn/nfxITSnB/B5hP7ig4B+HoGxYRp1se65JIleoMoGTRrbylaNXvYEMKeXmMVTFSWFwPrpwMXNVbu+KdPU1THWe0dEJtV6MJKamgo/Pz+9fX5+flCpVMjPzzd6TWFhIVQqld6DapG9E/DiXuD5HbqZJfd9I6YAA7pKr53+I1bzLU9dCgz+QH/fY3+I1YofmCdWMDZG6QE4uAHtyv0VH9wTeHihmGlSHeoSkaC6bFzVrv/jcRFwSJLYlie1Hv6fyPdY9YJYoygzUeTD6K0NpNAv+mYsGCmVBSPnNwAb3wF+rYE1fG6c1c3iKS/lGJB1zXD/v1OA9W8BWz4SZfx/e8jy90s/C/z6IJB0qErNNaoor+Jzqqw2ZtNIuqc2nKhIVBn1cjZNbGwsPDw8tI+QkBBrN6nxK5/PYOcAvLQfuPNVsYKvRr+y9WS6yj7g1SXAHa8AfacYf72WA4y/55TjwGvndYvuaWiqx/b5L2BjpNelMv58Wrf4X1XkpgNFZYmv8llAebJ1fLbJpk7Le08UCuMfqEd/0z1Xy3IXrh81PDc7FbhQjR4KlWyoKuU4sH0m8MNdwOyu4r0vbRNTjjOTRLn8/XNEoGXO0cXiHuKXAFnJYt8vI8VQ3ZL/mL/WUmfXAJ8GAPvn1czrlVcbU3vl+T/GAk8Skg4ByYet3QqqZ2o9fPf390damv40urS0NLi7u8PJycnoNdOnT8fUqbrZDCqVigGJNbj6ihwSud4TgbA7AN8OujVevFqYfx2XZmKhv6/KTQ928tTffmE3cGqlCGwA0bMy+ZiYpbD766rdQ9L+is+pyPaZYjaPfBZSpixhVl7oTT7TBwpdIAOIMvmpJ8UqxhryYRr5tTk3RA5I3AJxzh1TRUG7jW8DFzYBz2zSb48pNy8A7oHi+Q936vaXFoqhhF9Hie1sWbJvUbbp18vLAP5+SbftFgC8cgrISTW8B0AM+eTeEO1oZSIoNebPsinp698Eer9g+XXWJA882TNiXIEK+Lls2P7dm8aHeKlJqvV/MTExMVi7dq3evk2bNiEmJsbkNUqlEkqlkZVryfpsbIDALuL5E6vEB/Gg98W2m5HF/DTcA0RF1IIsMRzRychf0P4dxUPOIwjwsHIguu878dUtULfP1JRceU/EjTPA/2RDUEd/FQ85+V/T8qq4X0Ton7f7K/F93/+92P6/MBEEeASbb7tmSrR8OEjjmmxIJW6B8evVpfpF4eS1UgARxOydrdt2Kpsmvmc2cOuCyKnRBFxjlgFtKkhalyTg6l794a2aUtt1QIplwUhDLKqWlwE4uFa8qrgparX4nfdpa7qQYI7sD9PifAYjpFXpvsScnBzEx8cjPj4egJi6Gx8fj8RE8Zfi9OnTMW6crgv/hRdewOXLl/HGG2/g7Nmz+P7777Fs2TK88sorNXMHZD2tBgAP/aSrU9L9SZH4+vAi4+e36ANEDgNGfCOeW8qluWXnBZdLUh36qfHzwu4Ent8F9J+uP7RUEfnsHFO2fqK/bWzKsVzyYeBanHiek27+3PIzlg79pHtualaOJs/l9lXDY5s/0D2XTHxQly/+ZmyVZXmvlUewKEK36V3Rcybv+Tn0syg6ti3W+HsBwJYPgUX3mj5ekYRdwI+DdIs2AmLoTF0qeoOqo6RI5BGZGjqS18IxFvzVZ9lposbO3Er8uyxv1xfi+k3vmT6nfE+hJQqy9BPJa4JabTj1n6yq0sFIXFwcunTpgi5dxF/HU6dORZcuXfDee+IXMCUlRRuYAEB4eDjWrFmDTZs2ITo6Gl9++SV++uknTuttjOyUwKjvgQ41kIApp0mkBYA7XwPeTgVsZX+92TmKYYxnNgH3lk0D7joeiJkIoyasBgI6Af2nVfyXemUZq2VSnqb3QGPZePHVWMAgd3Kl/rYkS5g0VTG2oOwDMuNSxe0yen2m/nb5YRgA8G6le+7ood9DIHd5myg6tmOmqMcht+sr4K9nqz4cp/G/+4DkOOCPJ8R2US4wKxpYeK/lH36mXNwMnPlXDB1VlFxbX1aIlv+OmKPp6bt1oervpZnNpelJNEbes2bpz2P+AGBuDHCtBvNMVj4vho3Pl9W7Ksyx/HtFtaLSwzT9+/eHZOaHZqy6av/+/XH0qJHkPCJLOMt6RloNFLN/5KXdpyWKQAgQCweG3SkWD7REYNeaa6clwu4UbU+SVbRVXRPlxCvqRSksF3Ds+QZIPwOM+cN0EKQJUm5VMRgpHzQY6xmRByglBaZ7BeQ/s6JcMUy07zsg6iHRI1KTNDksaafF85xUXbItYPyDR5JENdWLm4BhnwO9ymYjFWSJXih7WQ2ZhJ1A5D2m37+6vTAVSTsFeLYQCzGaknVN9BJ1f1IE3ubYyYbF1eraWy9J/vtTbCQYUZeKlb1bxABB3cQ+TSB98k8guFvNtOPEMvF15xfi/5efBgI9nwfu/axmXp8qjSnfVP/Je0ZcfcVXTR2S6Mf0/yNVKADftvoFyeTC7tTftncUuRfDv6ybpEO3AOPvs9DMB5ujp+ljFzaI4YHU42LbyVuU+O9YlpNTWN2ekXIBkLGeEfnwUnG+6Z4RucJsYN8c8Rf5P/+tWtvMKS0CFtyjn4cjT2bWBAuH/yemM6vVIkDSLCew7nVdb8GaV8V05/Vv6q7XfL9NKSkyfzwvA1jyqEjYrkjifmDT+7relss7xHDIz0PMX7c9VgRh22NF4vTih/WHrwAxM2rXlyLA15APpQA122NQUc/IyRUiSfvHgYbH5MNgNUnTo3Pwh9p5fbIIgxGq/+Rr52iSWUd8I+qXDP/S/LV3l+Vw3P8dMPEgMNZImXePYNGj4mDmr8xQE2Ppj/xmfL8pbn6VXyW4eWv97T7lPrxnhgLLJ4jn3cYDQ2cAvu3Edvpp0bthbNqwJconrBrrGZEHH2knxQdKRQqzxaM2Je7TTxhOlPVGlRSJ78u/L4vpzJe3GX7YJZYFL/KlCDS2zQBO/mX6vcv3jJQUinyTg2WFAzd/AJxfp/u5yanVwKqXRAACAAuGil4wzfCH5n3TK8ijkH9/f3sQuLBRrMOkIUliZtSWj/Tr48i/D8eWimJ+8u9ddVQUjJgLmmvr94XTsOsF/hSo/rOxBV6/DLx2UfRkACI3oeNo/fLrxsRMAl49D3R9AvCJ1F1vjLEpyi36isq04/8RbZh8TP94uxEVt1/eG+MeXPkeGPlsoglrxXpCd71u/NzIsuRPx7LidJe3A//XQlfXYdB7wFAzCaTlqcol7RrrGSlP0wVuzvo3RW6HJTR/mR/5FTi8SP9YcQFwbr3oZTHm5nndc72ekSJdvgAgCsGV7wWqKIfnz6dMHyvfM3Jiucg3WVu2IvaV3aavvXZIzFLb843+66SU9cbI15LKSRf5NvJViItygVOr9Ncp0sxiKcoGZncRvSHyvJbsVN1z+fdh5fOips4fj5tub2WYC0Y2vC16cTTUav0ZUMYWuqwJ1ghGJKnuc1QSD4gep5oKLGsYgxFqGFyaAa4+lb9OoRC9EZZ46GfRA9Kyv27fk2tFZVpbe9EGrzDDfJSxfwEeoUB4P92++78TibWj5okqtAobUcCt42jTwchLsv8kQnrrnrvK2u/fUdzTwHf0cxg0AjqLr6aGdro/ZRh0dR2vvx3UXRdA3TwnvhbmiNofpqYAV1Zlil4V54n3/2cS8O9k4GNfYM1rIgD5NBD4/RFgw1vGq7/evqJ7Lq8NU1ooenE0rsXpkn01ji3R/5A2xlRBuvI9I7myInmJB/R7AOQzobZ+Aiy4W3adbAhMXSJ6RTTrSAEiINryoajdoenJWf0KsHw8cNVEwJNxWfSGyIdjinJ1z8t/H8q3oyK2ZsoyyHOQNMGQJImie+UTXwsyy/XY1EbPiKQfjPw4EFjxfC28j/wtJeB/I0Tdn5pcm6oii+4V/+6Wj6/4XCtgZR4ijeatgafWiQX7TNURAUTp+z8eB3qVFeNqPRh45YSYYbHlQ6DtfUD4nUDnMbohmRf3iqDC2dt0MOLbVtRu2TMLuO9rYHZnsd8jCBj8oegN0vR4AECx7AMECqDtcF2NCPnQFgC0uEMERU5egL2zbv8dU4G+L4sepIM/iOnOLs1F0HFll5iee3xZxTMf3AL0C6cBIqC5ssv8dZYozNGvx1JaKArhlXfrouG+UhO5GyVFQKas5+PWBeM5Cb+MMt+21VOM77+4WSQXRz8qllA4vFB3TB5sAKLXwiNIfEjt/Fz/2AHZNOKSQsPeGM33NzNRDOc8sxU4buGaRPIPd3lujancjG86Ane9IXoZy5P/lW/nqNv3x+Nie3TZkhDyAOjydhEsXtgkcp/K2/+9/vdDHjBVRVEuAAXg4Ky/Xx6MJB8Wjwfm1V6tmJJC3c8t7YSublNlaHpWKpNorPk3lJOmW+gzboH4/ew4uvJtqGEMRojKixwuaiV4hRk/3m4EMOWEGHKRc3AGhv2fblueG6LJ4Si//9HfRVf4yLJhhlYDDCuVhvQCQows9HfvF6Lbf8DbQPtRgJu/7piP7P182ophJs37yoe2OowSAYqTl37+jUeo7rmxQCR6jOg50PBsYRiMlA+IqqowW+Q7VKQyC/ulndJv3+0rhvkxgCjiZU5WkvH914+Kx9nVFbclO0UEI8byR/Z+q9/GipxbW/E5GvL8H3kxMmM9I4AIeP6ZZDwYkQcZdg7A+Y3iq+b+b10Entmin19kbgowYBiYVacQ3sEfxb8VtwDDoVZjQUdRrvmZSuVf+8SfwOgF4ucIiN9ZB1fjry3/HsiH0ipj1YtiRteLeyyrxCzvgZHUoqZM67vFDCVATAiorRlUFmIwQlRe8whgykn9WTzleYaaPlYRec9I23uBN68a/4/gv0eAjATjgQggkm5b9hd1PspfLw9M/DroB0DyYMRU0m5Ff62NmCUK2C0r+2AyNoQmrwVTGX5R+kMo31k4nVPzH6slshL1p1KXFlWcEFpblo0TPVOnV5k/z5IaIJUZRsu4rHsunxElnyZuY69fuM4Uea9F7g1gSblFLlPixXT26vRulF8RuzxJEgGpT6Tu3+f2mfp5KNkpIj9IzljOSEGm+WAkO1VM9+79ki4P6LcHgYkHgNQTojZK9yeBez83vFb+PbCkkKIxmqG6w4t0y2eYU36YrVCl/++lKEe/19UKmDNCZIxniGF3bk3R5HVomPqLpFkrMQRkikIhhpaMXa9QAK5lAUnrcsMC8mmcxlZhBkR+jHxdIq8w4P5vRT5AUHfxV688ebeiUuuVSRJsP9Lycy2hsBE9N6ZoljFIOii+2jqIROHqLtJoKVUysGF6zbyWJQnGGvJhLXlOi3y/uYCyMFtU000/a9l0boWN4bThytAEI1s+BpaONcy3uLIbWDxaDCdpbDeSrJ1ULoHT2O9m/m0xPHiz7HuhVosgRlND54/HRc/NZ+G6a26cFUO1msrDB+cbT1KVf68yZT1rhdkiGfm4kdlbppRPMDeloiCwtme2WYDBCFFd6/2SGFp5dlvtvs+zW4DRC4GO5f5KlffMmPvrTz5MFRAtVmqeckIM+QD6XdCOHobXy4d37nzN4mYj6qGKz3Fwq/gcDe9W5tc30gRrV/eIr92fBsL6Av3eNH5+57HA42am9TYUmuRkQD//KGGn+KouLZeXBP0P7i0fiWq68/sZL2BWnqSuuHKtOcX54rHrCzH8c+2g/nH57CxzM1X2zNLfNhqMZIo6Nd91E0FI3M9itetN74l7vWYkWRoQU8Tlw1/G8pjkAVnWNd3z48tEztmKZ0SAZ/DaO4DF/9G/5uB8MQupIhUFgQxGiJogOweg3xtAUC1Xf/UIBqIeNKxr4hEicky6PK7fS1KefJhKU/LdzU9/mGfEbDGsYqzCZ3EB8PQm4JHF+gFGxBDx3hot+wPuQYCLr5h9ZG54TGPAWxWfo+ERZHpKt52jyJuR06wmrVntuLzeL+ovmlhVj/5u+pidmZ9LTdHMvikv/bToJZIn+GpIZcXh8jPFByEggs6KhlAA0SNQnWGa0kJgvez3TDMFuShXrHsEWXCcZyYXI/eG7rmpoCX/tkguBYBz64B1ZYHp/u8Ngxm5pWP0hz+u7jU8Rx6QHVsielgkSX+ozNiQ3C/3i0TfuX319+/7zvysnN1fA/P7mz4O1ItghDkjRE2NQgH8538VnycPCgI6GT+n23jxAIDntovu/iu7xbTbQe+KHhVA/JWtUVokAg+NcX8bvm7fKaLOhjEj54jenutHxTCVb3vgj7G640M+EkmYu8rWKQrqLv5i1fBsIT5U8zOBR34RCcIKW0Aqa6Mm30YejDg3Ewm+Qd3FEJ58mrDc4A/0FyA0ZdD7Il/o6U3GK6ne9zWwqmy21v3f1k6V2vIJx3LbY4FLW40fy70J7Pg//X2qZOPnyqWe0JXpryp5nZnfHxW1f+IWANvKLVB5+4plQW1ynPF6NzdkvUZ7Zul+NwDjM39M+fdl/XpICTv1XxsQ07kDu+j3XmQmidlGDq5AcHf93zdjSz/cPCdyw8rb/KFY8bsi5ZeasAIGI0RknDxL399EMCKnSXptPUT0lMh7XWxsgYHvAls/FsNUIT3FX5ymhmSGfChK/294S6wdkleW0+Diq+tVeUg2vff5naIOSvuRQN/JYl/zNsDZf0WCn7yK6uRjhrMcfNvr/hLW9Hq4B+mOh8boLwCpNDFMJM+9KK/fm7oP8fC7xNeQnqI3SBN4aMhn+hgbAgOAftNEhVlLAgFL2buIoRlTgQgg8jLkxeQAXSKzOTW9/pDmNY8YCaxvnNNP0K2sxH265+XzYSq7gnBc2bRurzD9oFnun5f1V85O2g9sfEcEQU+ur3ito73fiqRy+dIYgGWBCMCeESKqx9wCxEKCdo6AV3jF58sZG/656zUxA0gzDDLRxDCBRs/nRIJty35i3H7dNP0ARC4gGvhvub9wox8RDwBo0UcMO7j4GJ9uGdxNF4y4lyW0esvuOafcbARTOSuhvUW3udIDeDNB1BrxCBZDCvZOwN7vRJAnT2Lu9B/g/Hr92TTylZ1NJRm7BwKTDonCbxp2TuanwCpsRL7S1o+NH/cM0S80Zkz5QMSUqNGVm+FkytSzYoVdY4wFIoAoc18dl7aYPlaSLxK51cX6AYQpGy3I6SgfUJ6W9Rae+RcI6WH++mO/i9+v+74WybY3zlY8dVqOwQgR1Vs2tsCzZX8h11QBKE0gYglbe11NC89Qy0rvm3L3DBEURD9m/Li/bAaGpmfEVjab5ma5rnVjCzH+94gI2sYsE8GRjS3gHyWOae77xd0iuJNfb2Mrhs1KS0RxtPC7yiUZmwhG7J1E97+Nna6g1RuXy4KTcrkQTl7AaxcAKHTFrjQfgAHRuumuHsGGwUhgF7H8wInlli3spzH6Z1Fyv8jIB93QWCDsDlGFtCKW1NGwlKu/+aGiFn3FbBu1mRwMQFRhzrhkvhhgq0HmgxpLFWQarp5tTNwCEYzs+VokF1eGJa9fy5jASkSmKRS1V4myLrk0EyX0m7UyfjxQlkwsHyLp/rT4etcbFb9Hs7J6L22G6td5kfNuaTox1tYO6PmsqJMhb0P5rvfy++XJrg7OwLREoPdEoPPj4p5d/YEn14ngShMEyRdflC89YGzWUbsRorqvZqVscxS2oj2Dy4Zkxiw1fl7MS/oBIAA0jzQ8787XjCcfV2Xada8XjedVyA142/w0cA2PoIqnq3u3NH88uCfwhAXBXf5t4wX5jMm9WflABAA2vQvsn1v562oQgxEioqCuoqLtf37VD77umSlKrPd+0fCaUfMM99UUpSxPxLkZMOxzUcb/vm90+zVl18sPiTm6A/d8CoyaI3o0Xj2rXwEYEDOgNEJ76Z57BOmfF9BZFGQDRFDy8CJgUpwo9d/mHsN1mvw7Am8lA3dMEdthd+iuL0/+fe72JPDMJsNz2t9v/Nq+LxvfD5jOQ7JTmp89NuwzMaXbVMAq5x5oPhixdxEBlzm5N4yvBu7bXn8747Ll+TZn/rXsPGPkM5WsgMEIEREgeiXKf/jZOYh8kvLTowGg82PAgHdqpy02NsD4f8W0aPcAoNdzwLQk/Q9azQylihaCNJojI8tBCOquey5J+msX3T9b1wOjUIgk3uatgQmrgTF/AN0m6L9uaZHh96r8ytpPrNI9HzUPCOom8omMDUfJc2fkPIKN7wdEsvEYI4XDCjLFbCdTNL1j3hYEI44e+sFI+V6QiQcq7hlxaW6816fvFPG90PwczOXwuJSrfGxqrSSNmhzyqmEMRoiIqqrH02IBxPIrH9eE8LuAdvfptm3t9Et2a6Z4PjBffIA++JPlry1fYkA+ayg7VX+mkG8Fwxq9J4qhIA1jORSaeiCAWLBRvvZS58dEXpJHsPGgydT6RhUt1dDmbsMhp+w0EUj994jx6zRBV0VDOYBYaFFenfb5nWIJia7jReK1Z9l7lx+KAsTPqc0wUaPHGL/2wCunRDBqSuRw4IU9ojevMupxMMIEViKiqnL2BqaeMd5zUls6/kfUTYkcJrb92gMvm/iANcU9UHzY2Sr1k2l924mAIn6xyMswlqgrZ2MjhoK2ltX5KDGySnK2LGH04UXmX6/z40D8b7pteS+NRmiMYaBxz/+JGVPJh0VRPc15J2Tl1jXF7Zq1EutB3TgrVjrW0PS2aKZdG+PiK9Z5aXO3yKNZ8h8xlKd0E4/7ywUYT6wSBdlaDRR1RAKixe9Mp4eNvjwAUVvE0V1MbTflwfmienL5NWfKe2A+sPI53XabYcD+Obrth/8HLJcF0pJktRwxBiNERNVRl4EIIKY3a5aArw75h+7Eg6LIVtfxYqqxvbN+lVxLGesZuWOKWHX5zleNL6goN/I74O6PgW+7itwJzQdjy/6ifS6+YokDV9nQlKs/0LusTou8ON+w/xN1OtrdL3oEwmQzd5w8xTTsl+NF8ma3CbpeGK8WonfFWGG7F3aJICa8n2jb9Gvmf/4uzYH+JpYV0AjpLeqKaNtW1nvh6Gn6Gs3Ql3xo67kdZetVtQGux4tz/DuKr66+Yl+XsSKPJ3EfEDNRJFrv7SaCOEBM8bXSgnkKSTJXxL9+UKlU8PDwQFZWFtzdrbuyIBERlbNvjihQN/Yv44s7VjZ4Ki6r5aFZBLK4QEyllQcocQuAA/NFcGZsOKQ6CrPFQnlOXsAMWeDzQS1UKs3PFGvd2NqL+4y8R+yXJODLSCAnTeTOvHwEOPGnCJo0uUPJR4Afy4a93k4zveyBOZIEfOInCqu9fLTiXJdKsvTzmz0jRERUPTETge5PmZ6tUtlenPKvY+9omMvR/SnxqA2aYRdA9IIk7BDDY7XByVNULS5PoQCe3igCFJ9Isd3zWf1z5IFDVQIRzfu4+YmeoNybNR6MWIrBCBERVZ+5abMN2cOLgHNrxVIDdU2+crYxTp7A5OPV/953fxooKdQf/qpjHKYhIiKiWmHp5zen9hIREZFVMRghIiIiq2IwQkRERFbFYISIiIisisEIERERWRWDESIiIrIqBiNERERkVQxGiIiIyKoYjBAREZFVMRghIiIiq2IwQkRERFbFYISIiIisisEIERERWZWdtRtgCc3CwiqVysotISIiIktpPrc1n+OmNIhgJDs7GwAQEhJi5ZYQERFRZWVnZ8PDw8PkcYVUUbhSD6jValy/fh1ubm5QKBQ19roqlQohISFISkqCu7t7jb1ufdGY768x3xvA+2vIGvO9AY37/hrzvQHWuT9JkpCdnY3AwEDY2JjODGkQPSM2NjYIDg6utdd3d3dvlL94Go35/hrzvQG8v4asMd8b0LjvrzHfG1D392euR0SDCaxERERkVQxGiIiIyKqadDCiVCrx/vvvQ6lUWrsptaIx319jvjeA99eQNeZ7Axr3/TXmewPq9/01iARWIiIiaryadM8IERERWR+DESIiIrIqBiNERERkVQxGiIiIyKqadDAyZ84chIWFwdHREb169cLBgwet3aQK7dy5EyNGjEBgYCAUCgVWrVqld1ySJLz33nsICAiAk5MTBg8ejAsXLuidk5GRgbFjx8Ld3R2enp54+umnkZOTU4d3YVxsbCx69OgBNzc3+Pr6YtSoUTh37pzeOQUFBZg4cSKaNWsGV1dXPPTQQ0hLS9M7JzExEcOHD4ezszN8fX3x+uuvo6SkpC5vxai5c+eiU6dO2oJDMTExWLdunfZ4Q7638mbOnAmFQoEpU6Zo9zXk+/vggw+gUCj0Hm3bttUeb8j3ppGcnIzHH38czZo1g5OTEzp27Ii4uDjt8Yb6f0tYWJjBz06hUGDixIkAGv7PrrS0FO+++y7Cw8Ph5OSEVq1a4eOPP9ZbC6ZB/OykJmrp0qWSg4ODtGDBAunUqVPSs88+K3l6ekppaWnWbppZa9euld5++21pxYoVEgBp5cqVesdnzpwpeXh4SKtWrZKOHTsm3X///VJ4eLiUn5+vPeeee+6RoqOjpf3790u7du2SIiIipMcee6yO78TQ0KFDpYULF0onT56U4uPjpXvvvVcKDQ2VcnJytOe88MILUkhIiLRlyxYpLi5O6t27t9SnTx/t8ZKSEikqKkoaPHiwdPToUWnt2rVS8+bNpenTp1vjlvT8888/0po1a6Tz589L586dk9566y3J3t5eOnnypCRJDfve5A4ePCiFhYVJnTp1kiZPnqzd35Dv7/3335c6dOggpaSkaB83btzQHm/I9yZJkpSRkSG1aNFCmjBhgnTgwAHp8uXL0oYNG6SLFy9qz2mo/7ekp6fr/dw2bdokAZC2bdsmSVLD/9nNmDFDatasmbR69WopISFBWr58ueTq6irNmjVLe05D+Nk12WCkZ8+e0sSJE7XbpaWlUmBgoBQbG2vFVlVO+WBErVZL/v7+0ueff67dl5mZKSmVSun333+XJEmSTp8+LQGQDh06pD1n3bp1kkKhkJKTk+us7ZZIT0+XAEg7duyQJEnci729vbR8+XLtOWfOnJEASPv27ZMkSQRrNjY2UmpqqvacuXPnSu7u7lJhYWHd3oAFvLy8pJ9++qnR3Ft2drbUunVradOmTVK/fv20wUhDv7/3339fio6ONnqsod+bJEnSm2++Kd1xxx0mjzem/1smT54stWrVSlKr1Y3iZzd8+HDpqaee0tv34IMPSmPHjpUkqeH87JrkME1RUREOHz6MwYMHa/fZ2Nhg8ODB2LdvnxVbVj0JCQlITU3Vuy8PDw/06tVLe1/79u2Dp6cnunfvrj1n8ODBsLGxwYEDB+q8zeZkZWUBALy9vQEAhw8fRnFxsd79tW3bFqGhoXr317FjR/j5+WnPGTp0KFQqFU6dOlWHrTevtLQUS5cuRW5uLmJiYhrNvU2cOBHDhw/Xuw+gcfzsLly4gMDAQLRs2RJjx45FYmIigMZxb//88w+6d++Ohx9+GL6+vujSpQt+/PFH7fHG8n9LUVERfvvtNzz11FNQKBSN4mfXp08fbNmyBefPnwcAHDt2DLt378awYcMANJyfXYNYKK+m3bx5E6WlpXq/XADg5+eHs2fPWqlV1ZeamgoARu9Lcyw1NRW+vr56x+3s7ODt7a09pz5Qq9WYMmUK+vbti6ioKACi7Q4ODvD09NQ7t/z9Gbt/zTFrO3HiBGJiYlBQUABXV1esXLkS7du3R3x8fIO/t6VLl+LIkSM4dOiQwbGG/rPr1asXFi1ahMjISKSkpODDDz/EnXfeiZMnTzb4ewOAy5cvY+7cuZg6dSreeustHDp0CC+//DIcHBwwfvz4RvN/y6pVq5CZmYkJEyYAaPi/lwAwbdo0qFQqtG3bFra2tigtLcWMGTMwduxYAA3nc6FJBiNU/02cOBEnT57E7t27rd2UGhUZGYn4+HhkZWXhzz//xPjx47Fjxw5rN6vakpKSMHnyZGzatAmOjo7Wbk6N0/yVCQCdOnVCr1690KJFCyxbtgxOTk5WbFnNUKvV6N69Oz799FMAQJcuXXDy5EnMmzcP48ePt3Lras7PP/+MYcOGITAw0NpNqTHLli3D4sWLsWTJEnTo0AHx8fGYMmUKAgMDG9TPrkkO0zRv3hy2trYGGdNpaWnw9/e3UquqT9N2c/fl7++P9PR0veMlJSXIyMioN/c+adIkrF69Gtu2bUNwcLB2v7+/P4qKipCZmal3fvn7M3b/mmPW5uDggIiICHTr1g2xsbGIjo7GrFmzGvy9HT58GOnp6ejatSvs7OxgZ2eHHTt2YPbs2bCzs4Ofn1+Dvr/yPD090aZNG1y8eLHB/+wAICAgAO3bt9fb165dO+1QVGP4v+Xq1avYvHkznnnmGe2+xvCze/311zFt2jQ8+uij6NixI5544gm88soriI2NBdBwfnZNMhhxcHBAt27dsGXLFu0+tVqNLVu2ICYmxootq57w8HD4+/vr3ZdKpcKBAwe09xUTE4PMzEwcPnxYe87WrVuhVqvRq1evOm+znCRJmDRpElauXImtW7ciPDxc73i3bt1gb2+vd3/nzp1DYmKi3v2dOHFC7x/Wpk2b4O7ubvCfbX2gVqtRWFjY4O9t0KBBOHHiBOLj47WP7t27Y+zYsdrnDfn+ysvJycGlS5cQEBDQ4H92ANC3b1+DafTnz59HixYtADT8/1sAYOHChfD19cXw4cO1+xrDzy4vLw82Nvof5ba2tlCr1QAa0M+uTtJk66GlS5dKSqVSWrRokXT69Gnpueeekzw9PfUypuuj7Oxs6ejRo9LRo0clANJXX30lHT16VLp69aokSWIKl6enp/T3339Lx48fl0aOHGl0CleXLl2kAwcOSLt375Zat25t9el3kiRJL774ouTh4SFt375dbypeXl6e9pwXXnhBCg0NlbZu3SrFxcVJMTExUkxMjPa4Zhre3XffLcXHx0vr16+XfHx86sU0vGnTpkk7duyQEhISpOPHj0vTpk2TFAqFtHHjRkmSGva9GSOfTSNJDfv+Xn31VWn79u1SQkKCtGfPHmnw4MFS8+bNpfT0dEmSGva9SZKYjm1nZyfNmDFDunDhgrR48WLJ2dlZ+u2337TnNOT/W0pLS6XQ0FDpzTffNDjW0H9248ePl4KCgrRTe1esWCE1b95ceuONN7TnNISfXZMNRiRJkr799lspNDRUcnBwkHr27Cnt37/f2k2q0LZt2yQABo/x48dLkiSmcb377ruSn5+fpFQqpUGDBknnzp3Te41bt25Jjz32mOTq6iq5u7tLTz75pJSdnW2Fu9Fn7L4ASAsXLtSek5+fL7300kuSl5eX5OzsLD3wwANSSkqK3utcuXJFGjZsmOTk5CQ1b95cevXVV6Xi4uI6vhtDTz31lNSiRQvJwcFB8vHxkQYNGqQNRCSpYd+bMeWDkYZ8f4888ogUEBAgOTg4SEFBQdIjjzyiV4OjId+bxr///itFRUVJSqVSatu2rTR//ny94w35/5YNGzZIAAzaK0kN/2enUqmkyZMnS6GhoZKjo6PUsmVL6e2339abdtwQfnYKSZKVaSMiIiKqY00yZ4SIiIjqDwYjREREZFUMRoiIiMiqGIwQERGRVTEYISIiIqtiMEJERERWxWCEiIiIrIrBCBEREVkVgxEiIiKyKgYjREREZFUMRoiIiMiqGIwQERGRVf0/INHyAL6VOAQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainHistory,valHistory,=[],[]\n",
    "previousLoss = float('inf')\n",
    "cnt =0 \n",
    "bestModel = model\n",
    "for epoch in range(10000):\n",
    "\n",
    "    trainLoss = trainAI(trainLoader, model, loss_fn, optimizer) \n",
    "    valLoss = valAI(valLoader, model, loss_fn)\n",
    "\n",
    "    trainHistory.append(trainLoss)\n",
    "    valHistory.append(valLoss)\n",
    "\n",
    "    print(f'epoch: {epoch} - train loss: {trainLoss} - val loss: {valLoss}')\n",
    "\n",
    "    if cnt >100:\n",
    "        break\n",
    "    elif previousLoss < valLoss:\n",
    "        cnt +=1\n",
    "    else:\n",
    "        previousLoss = valLoss\n",
    "        bestModel = model\n",
    "        cnt =0 \n",
    "\n",
    "plt.plot(trainHistory,label='train')\n",
    "plt.plot(valHistory, label='val')\n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2],\n",
       " [2, 0],\n",
       " [3, 9],\n",
       " [4, 9],\n",
       " [5, 3],\n",
       " [6, 7],\n",
       " [7, 0],\n",
       " [8, 3],\n",
       " [9, 0],\n",
       " [10, 3],\n",
       " [11, 5],\n",
       " [12, 7],\n",
       " [13, 4],\n",
       " [14, 0],\n",
       " [15, 4],\n",
       " [16, 3],\n",
       " [17, 3],\n",
       " [18, 1],\n",
       " [19, 9],\n",
       " [20, 0],\n",
       " [21, 9],\n",
       " [22, 1],\n",
       " [23, 1],\n",
       " [24, 5],\n",
       " [25, 7],\n",
       " [26, 4],\n",
       " [27, 2],\n",
       " [28, 7],\n",
       " [29, 4],\n",
       " [30, 7],\n",
       " [31, 7],\n",
       " [32, 5],\n",
       " [33, 4],\n",
       " [34, 2],\n",
       " [35, 6],\n",
       " [36, 2],\n",
       " [37, 5],\n",
       " [38, 5],\n",
       " [39, 1],\n",
       " [40, 6],\n",
       " [41, 7],\n",
       " [42, 7],\n",
       " [43, 4],\n",
       " [44, 9],\n",
       " [45, 8],\n",
       " [46, 7],\n",
       " [47, 8],\n",
       " [48, 8],\n",
       " [49, 6],\n",
       " [50, 7],\n",
       " [51, 6],\n",
       " [52, 8],\n",
       " [53, 8],\n",
       " [54, 3],\n",
       " [55, 8],\n",
       " [56, 2],\n",
       " [57, 1],\n",
       " [58, 2],\n",
       " [59, 2],\n",
       " [60, 0],\n",
       " [61, 4],\n",
       " [62, 1],\n",
       " [63, 7],\n",
       " [64, 0],\n",
       " [65, 0],\n",
       " [66, 0],\n",
       " [67, 1],\n",
       " [68, 9],\n",
       " [69, 0],\n",
       " [70, 1],\n",
       " [71, 6],\n",
       " [72, 5],\n",
       " [73, 8],\n",
       " [74, 8],\n",
       " [75, 2],\n",
       " [76, 8],\n",
       " [77, 8],\n",
       " [78, 9],\n",
       " [79, 2],\n",
       " [80, 3],\n",
       " [81, 5],\n",
       " [82, 4],\n",
       " [83, 1],\n",
       " [84, 0],\n",
       " [85, 9],\n",
       " [86, 2],\n",
       " [87, 4],\n",
       " [88, 3],\n",
       " [89, 6],\n",
       " [90, 7],\n",
       " [91, 2],\n",
       " [92, 0],\n",
       " [93, 6],\n",
       " [94, 6],\n",
       " [95, 1],\n",
       " [96, 4],\n",
       " [97, 3],\n",
       " [98, 9],\n",
       " [99, 7],\n",
       " [100, 4],\n",
       " [101, 0],\n",
       " [102, 9],\n",
       " [103, 2],\n",
       " [104, 0],\n",
       " [105, 7],\n",
       " [106, 3],\n",
       " [107, 0],\n",
       " [108, 5],\n",
       " [109, 0],\n",
       " [110, 9],\n",
       " [111, 0],\n",
       " [112, 0],\n",
       " [113, 4],\n",
       " [114, 7],\n",
       " [115, 1],\n",
       " [116, 7],\n",
       " [117, 1],\n",
       " [118, 1],\n",
       " [119, 5],\n",
       " [120, 3],\n",
       " [121, 3],\n",
       " [122, 7],\n",
       " [123, 2],\n",
       " [124, 8],\n",
       " [125, 6],\n",
       " [126, 3],\n",
       " [127, 8],\n",
       " [128, 7],\n",
       " [129, 8],\n",
       " [130, 4],\n",
       " [131, 3],\n",
       " [132, 5],\n",
       " [133, 6],\n",
       " [134, 0],\n",
       " [135, 0],\n",
       " [136, 0],\n",
       " [137, 3],\n",
       " [138, 1],\n",
       " [139, 5],\n",
       " [140, 0],\n",
       " [141, 5],\n",
       " [142, 3],\n",
       " [143, 4],\n",
       " [144, 5],\n",
       " [145, 5],\n",
       " [146, 8],\n",
       " [147, 7],\n",
       " [148, 7],\n",
       " [149, 2],\n",
       " [150, 8],\n",
       " [151, 4],\n",
       " [152, 3],\n",
       " [153, 5],\n",
       " [154, 6],\n",
       " [155, 5],\n",
       " [156, 3],\n",
       " [157, 7],\n",
       " [158, 3],\n",
       " [159, 7],\n",
       " [160, 8],\n",
       " [161, 3],\n",
       " [162, 0],\n",
       " [163, 4],\n",
       " [164, 5],\n",
       " [165, 1],\n",
       " [166, 2],\n",
       " [167, 7],\n",
       " [168, 6],\n",
       " [169, 3],\n",
       " [170, 0],\n",
       " [171, 2],\n",
       " [172, 7],\n",
       " [173, 8],\n",
       " [174, 6],\n",
       " [175, 1],\n",
       " [176, 3],\n",
       " [177, 7],\n",
       " [178, 4],\n",
       " [179, 1],\n",
       " [180, 2],\n",
       " [181, 4],\n",
       " [182, 3],\n",
       " [183, 5],\n",
       " [184, 2],\n",
       " [185, 4],\n",
       " [186, 9],\n",
       " [187, 2],\n",
       " [188, 1],\n",
       " [189, 6],\n",
       " [190, 0],\n",
       " [191, 6],\n",
       " [192, 1],\n",
       " [193, 4],\n",
       " [194, 9],\n",
       " [195, 6],\n",
       " [196, 0],\n",
       " [197, 9],\n",
       " [198, 7],\n",
       " [199, 6],\n",
       " [200, 9],\n",
       " [201, 1],\n",
       " [202, 9],\n",
       " [203, 0],\n",
       " [204, 9],\n",
       " [205, 9],\n",
       " [206, 0],\n",
       " [207, 8],\n",
       " [208, 4],\n",
       " [209, 6],\n",
       " [210, 2],\n",
       " [211, 0],\n",
       " [212, 9],\n",
       " [213, 3],\n",
       " [214, 6],\n",
       " [215, 3],\n",
       " [216, 2],\n",
       " [217, 1],\n",
       " [218, 6],\n",
       " [219, 3],\n",
       " [220, 4],\n",
       " [221, 2],\n",
       " [222, 3],\n",
       " [223, 1],\n",
       " [224, 2],\n",
       " [225, 2],\n",
       " [226, 0],\n",
       " [227, 4],\n",
       " [228, 6],\n",
       " [229, 1],\n",
       " [230, 0],\n",
       " [231, 0],\n",
       " [232, 4],\n",
       " [233, 9],\n",
       " [234, 1],\n",
       " [235, 7],\n",
       " [236, 3],\n",
       " [237, 2],\n",
       " [238, 3],\n",
       " [239, 8],\n",
       " [240, 6],\n",
       " [241, 8],\n",
       " [242, 6],\n",
       " [243, 2],\n",
       " [244, 8],\n",
       " [245, 5],\n",
       " [246, 5],\n",
       " [247, 4],\n",
       " [248, 8],\n",
       " [249, 3],\n",
       " [250, 2],\n",
       " [251, 9],\n",
       " [252, 7],\n",
       " [253, 1],\n",
       " [254, 3],\n",
       " [255, 8],\n",
       " [256, 4],\n",
       " [257, 5],\n",
       " [258, 1],\n",
       " [259, 4],\n",
       " [260, 5],\n",
       " [261, 6],\n",
       " [262, 3],\n",
       " [263, 3],\n",
       " [264, 5],\n",
       " [265, 7],\n",
       " [266, 0],\n",
       " [267, 6],\n",
       " [268, 8],\n",
       " [269, 3],\n",
       " [270, 1],\n",
       " [271, 6],\n",
       " [272, 0],\n",
       " [273, 6],\n",
       " [274, 3],\n",
       " [275, 9],\n",
       " [276, 8],\n",
       " [277, 1],\n",
       " [278, 5],\n",
       " [279, 8],\n",
       " [280, 4],\n",
       " [281, 0],\n",
       " [282, 9],\n",
       " [283, 2],\n",
       " [284, 0],\n",
       " [285, 5],\n",
       " [286, 3],\n",
       " [287, 7],\n",
       " [288, 1],\n",
       " [289, 9],\n",
       " [290, 9],\n",
       " [291, 5],\n",
       " [292, 7],\n",
       " [293, 7],\n",
       " [294, 9],\n",
       " [295, 9],\n",
       " [296, 6],\n",
       " [297, 3],\n",
       " [298, 0],\n",
       " [299, 3],\n",
       " [300, 3],\n",
       " [301, 6],\n",
       " [302, 9],\n",
       " [303, 8],\n",
       " [304, 2],\n",
       " [305, 6],\n",
       " [306, 3],\n",
       " [307, 7],\n",
       " [308, 1],\n",
       " [309, 4],\n",
       " [310, 5],\n",
       " [311, 8],\n",
       " [312, 5],\n",
       " [313, 9],\n",
       " [314, 0],\n",
       " [315, 0],\n",
       " [316, 3],\n",
       " [317, 8],\n",
       " [318, 4],\n",
       " [319, 1],\n",
       " [320, 8],\n",
       " [321, 4],\n",
       " [322, 1],\n",
       " [323, 1],\n",
       " [324, 9],\n",
       " [325, 8],\n",
       " [326, 4],\n",
       " [327, 5],\n",
       " [328, 1],\n",
       " [329, 5],\n",
       " [330, 3],\n",
       " [331, 6],\n",
       " [332, 3],\n",
       " [333, 1],\n",
       " [334, 3],\n",
       " [335, 0],\n",
       " [336, 9],\n",
       " [337, 0],\n",
       " [338, 0],\n",
       " [339, 6],\n",
       " [340, 0],\n",
       " [341, 6],\n",
       " [342, 3],\n",
       " [343, 1],\n",
       " [344, 8],\n",
       " [345, 6],\n",
       " [346, 0],\n",
       " [347, 6],\n",
       " [348, 5],\n",
       " [349, 2],\n",
       " [350, 2],\n",
       " [351, 6],\n",
       " [352, 7],\n",
       " [353, 7],\n",
       " [354, 2],\n",
       " [355, 3],\n",
       " [356, 8],\n",
       " [357, 3],\n",
       " [358, 9],\n",
       " [359, 2],\n",
       " [360, 7],\n",
       " [361, 8],\n",
       " [362, 6],\n",
       " [363, 3],\n",
       " [364, 8],\n",
       " [365, 4],\n",
       " [366, 2],\n",
       " [367, 3],\n",
       " [368, 8],\n",
       " [369, 1],\n",
       " [370, 6],\n",
       " [371, 4],\n",
       " [372, 8],\n",
       " [373, 7],\n",
       " [374, 9],\n",
       " [375, 7],\n",
       " [376, 6],\n",
       " [377, 9],\n",
       " [378, 5],\n",
       " [379, 3],\n",
       " [380, 7],\n",
       " [381, 6],\n",
       " [382, 5],\n",
       " [383, 5],\n",
       " [384, 4],\n",
       " [385, 2],\n",
       " [386, 6],\n",
       " [387, 2],\n",
       " [388, 1],\n",
       " [389, 3],\n",
       " [390, 7],\n",
       " [391, 1],\n",
       " [392, 7],\n",
       " [393, 9],\n",
       " [394, 9],\n",
       " [395, 6],\n",
       " [396, 1],\n",
       " [397, 1],\n",
       " [398, 1],\n",
       " [399, 7],\n",
       " [400, 3],\n",
       " [401, 9],\n",
       " [402, 7],\n",
       " [403, 6],\n",
       " [404, 1],\n",
       " [405, 1],\n",
       " [406, 1],\n",
       " [407, 9],\n",
       " [408, 3],\n",
       " [409, 5],\n",
       " [410, 5],\n",
       " [411, 5],\n",
       " [412, 0],\n",
       " [413, 4],\n",
       " [414, 1],\n",
       " [415, 2],\n",
       " [416, 3],\n",
       " [417, 1],\n",
       " [418, 1],\n",
       " [419, 3],\n",
       " [420, 5],\n",
       " [421, 9],\n",
       " [422, 6],\n",
       " [423, 6],\n",
       " [424, 5],\n",
       " [425, 3],\n",
       " [426, 1],\n",
       " [427, 4],\n",
       " [428, 7],\n",
       " [429, 4],\n",
       " [430, 7],\n",
       " [431, 4],\n",
       " [432, 8],\n",
       " [433, 5],\n",
       " [434, 2],\n",
       " [435, 6],\n",
       " [436, 1],\n",
       " [437, 3],\n",
       " [438, 9],\n",
       " [439, 5],\n",
       " [440, 0],\n",
       " [441, 8],\n",
       " [442, 4],\n",
       " [443, 7],\n",
       " [444, 4],\n",
       " [445, 4],\n",
       " [446, 4],\n",
       " [447, 1],\n",
       " [448, 5],\n",
       " [449, 3],\n",
       " [450, 9],\n",
       " [451, 5],\n",
       " [452, 7],\n",
       " [453, 6],\n",
       " [454, 9],\n",
       " [455, 5],\n",
       " [456, 9],\n",
       " [457, 2],\n",
       " [458, 3],\n",
       " [459, 5],\n",
       " [460, 6],\n",
       " [461, 6],\n",
       " [462, 7],\n",
       " [463, 5],\n",
       " [464, 0],\n",
       " [465, 5],\n",
       " [466, 1],\n",
       " [467, 7],\n",
       " [468, 4],\n",
       " [469, 4],\n",
       " [470, 1],\n",
       " [471, 1],\n",
       " [472, 4],\n",
       " [473, 9],\n",
       " [474, 5],\n",
       " [475, 6],\n",
       " [476, 0],\n",
       " [477, 1],\n",
       " [478, 3],\n",
       " [479, 1],\n",
       " [480, 0],\n",
       " [481, 4],\n",
       " [482, 8],\n",
       " [483, 1],\n",
       " [484, 2],\n",
       " [485, 7],\n",
       " [486, 9],\n",
       " [487, 4],\n",
       " [488, 8],\n",
       " [489, 3],\n",
       " [490, 7],\n",
       " [491, 7],\n",
       " [492, 4],\n",
       " [493, 2],\n",
       " [494, 4],\n",
       " [495, 6],\n",
       " [496, 7],\n",
       " [497, 6],\n",
       " [498, 3],\n",
       " [499, 2],\n",
       " [500, 0],\n",
       " [501, 6],\n",
       " [502, 5],\n",
       " [503, 9],\n",
       " [504, 4],\n",
       " [505, 1],\n",
       " [506, 8],\n",
       " [507, 3],\n",
       " [508, 3],\n",
       " [509, 0],\n",
       " [510, 6],\n",
       " [511, 7],\n",
       " [512, 5],\n",
       " [513, 8],\n",
       " [514, 7],\n",
       " [515, 5],\n",
       " [516, 3],\n",
       " [517, 5],\n",
       " [518, 7],\n",
       " [519, 4],\n",
       " [520, 3],\n",
       " [521, 6],\n",
       " [522, 9],\n",
       " [523, 0],\n",
       " [524, 7],\n",
       " [525, 7],\n",
       " [526, 1],\n",
       " [527, 0],\n",
       " [528, 1],\n",
       " [529, 1],\n",
       " [530, 7],\n",
       " [531, 0],\n",
       " [532, 5],\n",
       " [533, 3],\n",
       " [534, 8],\n",
       " [535, 3],\n",
       " [536, 5],\n",
       " [537, 6],\n",
       " [538, 5],\n",
       " [539, 7],\n",
       " [540, 3],\n",
       " [541, 0],\n",
       " [542, 2],\n",
       " [543, 8],\n",
       " [544, 2],\n",
       " [545, 0],\n",
       " [546, 3],\n",
       " [547, 0],\n",
       " [548, 9],\n",
       " [549, 2],\n",
       " [550, 1],\n",
       " [551, 1],\n",
       " [552, 3],\n",
       " [553, 0],\n",
       " [554, 5],\n",
       " [555, 0],\n",
       " [556, 0],\n",
       " [557, 7],\n",
       " [558, 5],\n",
       " [559, 6],\n",
       " [560, 2],\n",
       " [561, 0],\n",
       " [562, 3],\n",
       " [563, 8],\n",
       " [564, 1],\n",
       " [565, 6],\n",
       " [566, 5],\n",
       " [567, 4],\n",
       " [568, 1],\n",
       " [569, 1],\n",
       " [570, 9],\n",
       " [571, 6],\n",
       " [572, 5],\n",
       " [573, 3],\n",
       " [574, 6],\n",
       " [575, 0],\n",
       " [576, 4],\n",
       " [577, 8],\n",
       " [578, 2],\n",
       " [579, 4],\n",
       " [580, 2],\n",
       " [581, 5],\n",
       " [582, 1],\n",
       " [583, 7],\n",
       " [584, 6],\n",
       " [585, 9],\n",
       " [586, 1],\n",
       " [587, 7],\n",
       " [588, 3],\n",
       " [589, 8],\n",
       " [590, 0],\n",
       " [591, 8],\n",
       " [592, 8],\n",
       " [593, 4],\n",
       " [594, 5],\n",
       " [595, 3],\n",
       " [596, 6],\n",
       " [597, 6],\n",
       " [598, 6],\n",
       " [599, 0],\n",
       " [600, 3],\n",
       " [601, 5],\n",
       " [602, 1],\n",
       " [603, 7],\n",
       " [604, 1],\n",
       " [605, 6],\n",
       " [606, 2],\n",
       " [607, 8],\n",
       " [608, 5],\n",
       " [609, 6],\n",
       " [610, 4],\n",
       " [611, 7],\n",
       " [612, 4],\n",
       " [613, 3],\n",
       " [614, 3],\n",
       " [615, 2],\n",
       " [616, 4],\n",
       " [617, 7],\n",
       " [618, 0],\n",
       " [619, 0],\n",
       " [620, 9],\n",
       " [621, 8],\n",
       " [622, 5],\n",
       " [623, 9],\n",
       " [624, 4],\n",
       " [625, 0],\n",
       " [626, 8],\n",
       " [627, 1],\n",
       " [628, 3],\n",
       " [629, 6],\n",
       " [630, 2],\n",
       " [631, 6],\n",
       " [632, 1],\n",
       " [633, 8],\n",
       " [634, 6],\n",
       " [635, 1],\n",
       " [636, 4],\n",
       " [637, 7],\n",
       " [638, 7],\n",
       " [639, 8],\n",
       " [640, 3],\n",
       " [641, 0],\n",
       " [642, 9],\n",
       " [643, 9],\n",
       " [644, 6],\n",
       " [645, 7],\n",
       " [646, 7],\n",
       " [647, 4],\n",
       " [648, 4],\n",
       " [649, 1],\n",
       " [650, 8],\n",
       " [651, 4],\n",
       " [652, 8],\n",
       " [653, 0],\n",
       " [654, 2],\n",
       " [655, 8],\n",
       " [656, 2],\n",
       " [657, 4],\n",
       " [658, 3],\n",
       " [659, 3],\n",
       " [660, 7],\n",
       " [661, 2],\n",
       " [662, 3],\n",
       " [663, 4],\n",
       " [664, 0],\n",
       " [665, 4],\n",
       " [666, 8],\n",
       " [667, 1],\n",
       " [668, 3],\n",
       " [669, 3],\n",
       " [670, 6],\n",
       " [671, 3],\n",
       " [672, 9],\n",
       " [673, 4],\n",
       " [674, 3],\n",
       " [675, 8],\n",
       " [676, 7],\n",
       " [677, 7],\n",
       " [678, 2],\n",
       " [679, 6],\n",
       " [680, 0],\n",
       " [681, 6],\n",
       " [682, 9],\n",
       " [683, 8],\n",
       " [684, 8],\n",
       " [685, 1],\n",
       " [686, 3],\n",
       " [687, 4],\n",
       " [688, 6],\n",
       " [689, 9],\n",
       " [690, 9],\n",
       " [691, 2],\n",
       " [692, 6],\n",
       " [693, 0],\n",
       " [694, 1],\n",
       " [695, 8],\n",
       " [696, 4],\n",
       " [697, 3],\n",
       " [698, 9],\n",
       " [699, 8],\n",
       " [700, 8],\n",
       " [701, 4],\n",
       " [702, 0],\n",
       " [703, 5],\n",
       " [704, 0],\n",
       " [705, 6],\n",
       " [706, 0],\n",
       " [707, 9],\n",
       " [708, 4],\n",
       " [709, 6],\n",
       " [710, 5],\n",
       " [711, 1],\n",
       " [712, 8],\n",
       " [713, 1],\n",
       " [714, 5],\n",
       " [715, 3],\n",
       " [716, 6],\n",
       " [717, 2],\n",
       " [718, 3],\n",
       " [719, 7],\n",
       " [720, 8],\n",
       " [721, 9],\n",
       " [722, 3],\n",
       " [723, 1],\n",
       " [724, 0],\n",
       " [725, 1],\n",
       " [726, 0],\n",
       " [727, 6],\n",
       " [728, 4],\n",
       " [729, 3],\n",
       " [730, 5],\n",
       " [731, 7],\n",
       " [732, 1],\n",
       " [733, 3],\n",
       " [734, 2],\n",
       " [735, 7],\n",
       " [736, 7],\n",
       " [737, 1],\n",
       " [738, 5],\n",
       " [739, 1],\n",
       " [740, 5],\n",
       " [741, 4],\n",
       " [742, 4],\n",
       " [743, 3],\n",
       " [744, 4],\n",
       " [745, 3],\n",
       " [746, 9],\n",
       " [747, 0],\n",
       " [748, 7],\n",
       " [749, 8],\n",
       " [750, 6],\n",
       " [751, 4],\n",
       " [752, 9],\n",
       " [753, 4],\n",
       " [754, 4],\n",
       " [755, 1],\n",
       " [756, 4],\n",
       " [757, 7],\n",
       " [758, 1],\n",
       " [759, 1],\n",
       " [760, 8],\n",
       " [761, 9],\n",
       " [762, 0],\n",
       " [763, 4],\n",
       " [764, 0],\n",
       " [765, 4],\n",
       " [766, 0],\n",
       " [767, 0],\n",
       " [768, 8],\n",
       " [769, 1],\n",
       " [770, 8],\n",
       " [771, 6],\n",
       " [772, 5],\n",
       " [773, 0],\n",
       " [774, 1],\n",
       " [775, 5],\n",
       " [776, 3],\n",
       " [777, 4],\n",
       " [778, 6],\n",
       " [779, 3],\n",
       " [780, 1],\n",
       " [781, 1],\n",
       " [782, 6],\n",
       " [783, 9],\n",
       " [784, 8],\n",
       " [785, 3],\n",
       " [786, 5],\n",
       " [787, 5],\n",
       " [788, 4],\n",
       " [789, 8],\n",
       " [790, 8],\n",
       " [791, 5],\n",
       " [792, 0],\n",
       " [793, 4],\n",
       " [794, 0],\n",
       " [795, 4],\n",
       " [796, 3],\n",
       " [797, 1],\n",
       " [798, 6],\n",
       " [799, 9],\n",
       " [800, 9],\n",
       " [801, 1],\n",
       " [802, 1],\n",
       " [803, 3],\n",
       " [804, 3],\n",
       " [805, 1],\n",
       " [806, 4],\n",
       " [807, 9],\n",
       " [808, 6],\n",
       " [809, 9],\n",
       " [810, 1],\n",
       " [811, 5],\n",
       " [812, 4],\n",
       " [813, 2],\n",
       " [814, 3],\n",
       " [815, 2],\n",
       " [816, 4],\n",
       " [817, 0],\n",
       " [818, 9],\n",
       " [819, 7],\n",
       " [820, 4],\n",
       " [821, 3],\n",
       " [822, 0],\n",
       " [823, 5],\n",
       " [824, 0],\n",
       " [825, 1],\n",
       " [826, 9],\n",
       " [827, 0],\n",
       " [828, 4],\n",
       " [829, 5],\n",
       " [830, 2],\n",
       " [831, 8],\n",
       " [832, 0],\n",
       " [833, 5],\n",
       " [834, 9],\n",
       " [835, 3],\n",
       " [836, 9],\n",
       " [837, 6],\n",
       " [838, 1],\n",
       " [839, 5],\n",
       " [840, 5],\n",
       " [841, 1],\n",
       " [842, 9],\n",
       " [843, 0],\n",
       " [844, 8],\n",
       " [845, 4],\n",
       " [846, 6],\n",
       " [847, 7],\n",
       " [848, 2],\n",
       " [849, 8],\n",
       " [850, 5],\n",
       " [851, 8],\n",
       " [852, 9],\n",
       " [853, 7],\n",
       " [854, 7],\n",
       " [855, 2],\n",
       " [856, 8],\n",
       " [857, 1],\n",
       " [858, 3],\n",
       " [859, 4],\n",
       " [860, 5],\n",
       " [861, 0],\n",
       " [862, 4],\n",
       " [863, 1],\n",
       " [864, 4],\n",
       " [865, 2],\n",
       " [866, 3],\n",
       " [867, 6],\n",
       " [868, 9],\n",
       " [869, 2],\n",
       " [870, 3],\n",
       " [871, 4],\n",
       " [872, 5],\n",
       " [873, 4],\n",
       " [874, 2],\n",
       " [875, 3],\n",
       " [876, 3],\n",
       " [877, 1],\n",
       " [878, 1],\n",
       " [879, 0],\n",
       " [880, 1],\n",
       " [881, 4],\n",
       " [882, 9],\n",
       " [883, 1],\n",
       " [884, 1],\n",
       " [885, 2],\n",
       " [886, 7],\n",
       " [887, 1],\n",
       " [888, 5],\n",
       " [889, 4],\n",
       " [890, 9],\n",
       " [891, 1],\n",
       " [892, 7],\n",
       " [893, 6],\n",
       " [894, 0],\n",
       " [895, 4],\n",
       " [896, 2],\n",
       " [897, 9],\n",
       " [898, 4],\n",
       " [899, 1],\n",
       " [900, 1],\n",
       " [901, 5],\n",
       " [902, 3],\n",
       " [903, 5],\n",
       " [904, 7],\n",
       " [905, 9],\n",
       " [906, 7],\n",
       " [907, 8],\n",
       " [908, 3],\n",
       " [909, 2],\n",
       " [910, 7],\n",
       " [911, 2],\n",
       " [912, 0],\n",
       " [913, 4],\n",
       " [914, 7],\n",
       " [915, 1],\n",
       " [916, 6],\n",
       " [917, 4],\n",
       " [918, 6],\n",
       " [919, 1],\n",
       " [920, 8],\n",
       " [921, 7],\n",
       " [922, 3],\n",
       " [923, 5],\n",
       " [924, 9],\n",
       " [925, 4],\n",
       " [926, 7],\n",
       " [927, 9],\n",
       " [928, 2],\n",
       " [929, 6],\n",
       " [930, 3],\n",
       " [931, 3],\n",
       " [932, 2],\n",
       " [933, 1],\n",
       " [934, 4],\n",
       " [935, 5],\n",
       " [936, 3],\n",
       " [937, 7],\n",
       " [938, 7],\n",
       " [939, 9],\n",
       " [940, 5],\n",
       " [941, 6],\n",
       " [942, 3],\n",
       " [943, 6],\n",
       " [944, 1],\n",
       " [945, 0],\n",
       " [946, 9],\n",
       " [947, 3],\n",
       " [948, 2],\n",
       " [949, 9],\n",
       " [950, 2],\n",
       " [951, 6],\n",
       " [952, 7],\n",
       " [953, 5],\n",
       " [954, 2],\n",
       " [955, 3],\n",
       " [956, 2],\n",
       " [957, 8],\n",
       " [958, 3],\n",
       " [959, 0],\n",
       " [960, 2],\n",
       " [961, 7],\n",
       " [962, 9],\n",
       " [963, 4],\n",
       " [964, 0],\n",
       " [965, 9],\n",
       " [966, 5],\n",
       " [967, 1],\n",
       " [968, 8],\n",
       " [969, 8],\n",
       " [970, 5],\n",
       " [971, 3],\n",
       " [972, 2],\n",
       " [973, 9],\n",
       " [974, 6],\n",
       " [975, 7],\n",
       " [976, 0],\n",
       " [977, 8],\n",
       " [978, 0],\n",
       " [979, 7],\n",
       " [980, 4],\n",
       " [981, 5],\n",
       " [982, 8],\n",
       " [983, 7],\n",
       " [984, 9],\n",
       " [985, 7],\n",
       " [986, 7],\n",
       " [987, 0],\n",
       " [988, 5],\n",
       " [989, 3],\n",
       " [990, 2],\n",
       " [991, 1],\n",
       " [992, 9],\n",
       " [993, 5],\n",
       " [994, 6],\n",
       " [995, 8],\n",
       " [996, 3],\n",
       " [997, 6],\n",
       " [998, 2],\n",
       " [999, 2],\n",
       " [1000, 9],\n",
       " ...]"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testAI(dataloader, model):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    y=1\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            X  = X.to(device)\n",
    "            pred = model(X)\n",
    "            for i in pred:\n",
    "                out.append([y, torch.argmax(i).item()])\n",
    "                y+=1\n",
    "\n",
    "    \n",
    "    return out\n",
    "\n",
    "result = testAI(testLoader, bestModel) \n",
    "# result = testAI(testLoader[0], model[0])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      9\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outDF= pd.DataFrame(result)\n",
    "outDF= outDF.astype(int)\n",
    "outDF.columns=['ImageId','Label']\n",
    "outDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDF.to_csv('result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
