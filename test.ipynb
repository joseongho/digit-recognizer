{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n"
     ]
    }
   ],
   "source": [
    "trainDF = pd.read_csv('train.csv')\n",
    "trainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28000 entries, 0 to 27999\n",
      "Columns: 784 entries, pixel0 to pixel783\n",
      "dtypes: int64(784)\n",
      "memory usage: 167.5 MB\n"
     ]
    }
   ],
   "source": [
    "testDF = pd.read_csv('test.csv')\n",
    "testDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, DF):\n",
    "        if 'label' in DF.columns:\n",
    "            self.x = DF.drop(columns=['label']).values\n",
    "            self.y = pd.get_dummies(DF['label']).values\n",
    "        else:\n",
    "            self.x = DF.values\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x=torch.FloatTensor(self.x[idx].reshape(1,28,28))\n",
    "\n",
    "        if hasattr(self,'y'):\n",
    "            y=torch.FloatTensor(self.y[idx])\n",
    "            return x,y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "trainSet= MyDataset(DF=trainDF)\n",
    "testSet= MyDataset(DF=testDF)\n",
    "trainSet[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "\n",
    "dataset_size = len(trainSet)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(0.2* dataset_size)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "trainSampler= MySampler(train_indices)\n",
    "valSampler= MySampler(val_indices)\n",
    "# val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x30224b5b0>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader= torch.utils.data.DataLoader(trainSet,batch_size=2048,sampler=trainSampler)\n",
    "valLoader= torch.utils.data.DataLoader(trainSet,batch_size=2048,sampler=valSampler)\n",
    "testLoader = torch.utils.data.DataLoader(testSet)\n",
    "trainLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x30225f340>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainLoader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "plt.imshow(images[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1620da190>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa20lEQVR4nO3df3BU9f3v8deGHwtqsjGEZLMSMKBCFUlvqaT5ohQllxBnGBC+vf7qHXAcHDF4hdTqpKMibWfSYr/Wr94I/7Sk3hFQ7whcGUsHgwljDXSIMFxua76EpiWWJNTcIRuChEg+9w+u2y4k4Fl2eWeX52PmzJDd88l5e9zx6ckuJz7nnBMAAFdYmvUAAICrEwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmhlsPcL7+/n4dO3ZM6enp8vl81uMAADxyzqm7u1uhUEhpaYNf5wy5AB07dkz5+fnWYwAALlNra6vGjRs36PNDLkDp6emSpDt1r4ZrhPE0AACvvlSfPtL7kf+eDyZhAaqurtZLL72k9vZ2FRYW6rXXXtOMGTMuue6rH7sN1wgN9xEgAEg6//8Oo5d6GyUhH0J46623VFFRodWrV+uTTz5RYWGhSktLdfz48UQcDgCQhBISoJdfflnLli3TI488oltvvVXr16/XNddco1//+teJOBwAIAnFPUBnzpxRY2OjSkpK/nGQtDSVlJSooaHhgv17e3sVDoejNgBA6ot7gD7//HOdPXtWubm5UY/n5uaqvb39gv2rqqoUCAQiG5+AA4Crg/lfRK2srFRXV1dka21ttR4JAHAFxP1TcNnZ2Ro2bJg6OjqiHu/o6FAwGLxgf7/fL7/fH+8xAABDXNyvgEaOHKnp06ertrY28lh/f79qa2tVXFwc78MBAJJUQv4eUEVFhZYsWaJvf/vbmjFjhl555RX19PTokUceScThAABJKCEBuv/++/X3v/9dL7zwgtrb2/XNb35TO3bsuOCDCQCAq5fPOeesh/hn4XBYgUBAs7WAOyEAQBL60vWpTtvU1dWljIyMQfcz/xQcAODqRIAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwMtx4AALy4/vdZntdsLtgV07EKf/6E5zXBf/84pmNdjbgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDNSAGZyGzI8r3k9/33Pa/rcCM9rJMnnYlqGr4krIACACQIEADAR9wC9+OKL8vl8UduUKVPifRgAQJJLyHtAt912mz744IN/HGQ4bzUBAKIlpAzDhw9XMBhMxLcGAKSIhLwHdPjwYYVCIU2cOFEPP/ywjh49Oui+vb29CofDURsAIPXFPUBFRUWqqanRjh07tG7dOrW0tOiuu+5Sd3f3gPtXVVUpEAhEtvz8/HiPBAAYguIeoLKyMn3ve9/TtGnTVFpaqvfff18nTpzQ22+/PeD+lZWV6urqimytra3xHgkAMAQl/NMBmZmZuuWWW9Tc3Dzg836/X36/P9FjAACGmIT/PaCTJ0/qyJEjysvLS/ShAABJJO4Bevrpp1VfX6+//OUv+vjjj3Xfffdp2LBhevDBB+N9KABAEov7j+A+++wzPfjgg+rs7NTYsWN15513as+ePRo7dmy8DwUASGJxD9DmzZvj/S0BJIE/ry32vGbzuH/zvMbv8/6e8Xc+ie0nMKGaQ57XnI3pSFcn7gUHADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJhI+C+kA5B8/u8j3m8s2vDgLzyvuS5tlOc1L3Xe6nlN7tLPPa+RpLPhcEzr8PVwBQQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAAT3A0bSGHDJt8U07oFqz70vCYQw52tD54563nNtl/c43lNZmeD5zVIPK6AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAAT3IwUSBJ9c7/tec09/1Yf07Eqsj6NaZ1Xy9Y+5XnN2De4sWiq4AoIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBzUgBAx3/7V88r2l89r97XtMv53mNJP1H3xnPax7943/1vCZvy589r/nS8woMVVwBAQBMECAAgAnPAdq9e7fmz5+vUCgkn8+nrVu3Rj3vnNMLL7ygvLw8jR49WiUlJTp8+HC85gUApAjPAerp6VFhYaGqq6sHfH7t2rV69dVXtX79eu3du1fXXnutSktLdfr06cseFgCQOjx/CKGsrExlZWUDPuec0yuvvKLnnntOCxYskCS98cYbys3N1datW/XAAw9c3rQAgJQR1/eAWlpa1N7erpKSkshjgUBARUVFamgY+Nfo9vb2KhwOR20AgNQX1wC1t7dLknJzc6Mez83NjTx3vqqqKgUCgciWn58fz5EAAEOU+afgKisr1dXVFdlaW1utRwIAXAFxDVAwGJQkdXR0RD3e0dERee58fr9fGRkZURsAIPXFNUAFBQUKBoOqra2NPBYOh7V3714VFxfH81AAgCTn+VNwJ0+eVHNzc+TrlpYWHThwQFlZWRo/frxWrlypn/70p7r55ptVUFCg559/XqFQSAsXLozn3ACAJOc5QPv27dPdd98d+bqiokKStGTJEtXU1OiZZ55RT0+PHnvsMZ04cUJ33nmnduzYoVGjRsVvagBA0vM552K7W2GChMNhBQIBzdYCDfeNsB4HuKThN473vGb29v/jeU3F9d7vKBLrzUgLG5Z4XpP/r4diOhZSz5euT3Xapq6urou+r2/+KTgAwNWJAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJjz/OgYglQ3LzfG8ZtZ7f/K8ZuX1/+F5jeTzvKLly9MxHEe69v30mNYBXnAFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GakwD/LuM7zkoqsTxMwSHys/Nb8mNZldTbEeRLgQlwBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuBkpUtLwcTfEtG7G//R+Y9E0+WI6ller2oo8r3FfnE7AJEB8cAUEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjgZqRIScfXXxvTuh9l/2/Pa/pjOM5Tx2Z6XtPyXe//v9h/6pTnNcCVwhUQAMAEAQIAmPAcoN27d2v+/PkKhULy+XzaunVr1PNLly6Vz+eL2ubNmxeveQEAKcJzgHp6elRYWKjq6upB95k3b57a2toi26ZNmy5rSABA6vH8IYSysjKVlZVddB+/369gMBjzUACA1JeQ94Dq6uqUk5OjyZMna/ny5ers7Bx0397eXoXD4agNAJD64h6gefPm6Y033lBtba1+/vOfq76+XmVlZTp79uyA+1dVVSkQCES2/Pz8eI8EABiC4v73gB544IHIn2+//XZNmzZNkyZNUl1dnebMmXPB/pWVlaqoqIh8HQ6HiRAAXAUS/jHsiRMnKjs7W83NzQM+7/f7lZGREbUBAFJfwgP02WefqbOzU3l5eYk+FAAgiXj+EdzJkyejrmZaWlp04MABZWVlKSsrS2vWrNHixYsVDAZ15MgRPfPMM7rppptUWloa18EBAMnNc4D27dunu+++O/L1V+/fLFmyROvWrdPBgwf1m9/8RidOnFAoFNLcuXP1k5/8RH6/P35TAwCSnucAzZ49W865QZ//3e9+d1kDAecbPu4Gz2v+8w2fJmCSgZ3s7/W8pvHV/+R5TeapBs9rgKGMe8EBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADARNx/JTdwMcMneP916+kbezyvWZOz3/MaSfr87Bee15T94hnPa3L/x8ee1wCphisgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyPFFfXXB73fjHT/ja8lYJKBPfu3ez2vyX2VG4sCseAKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwc1IEbPjT/yL5zXvLn8phiON8rxixd/ujOE4UufDWTGsCsd0LOBqxxUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCm5FCw8aOjWnd00+95XlNwXDvNxaNxSfrvhnTuqw/N8R3EACD4goIAGCCAAEATHgKUFVVle644w6lp6crJydHCxcuVFNTU9Q+p0+fVnl5ucaMGaPrrrtOixcvVkdHR1yHBgAkP08Bqq+vV3l5ufbs2aOdO3eqr69Pc+fOVU9PT2SfVatW6b333tM777yj+vp6HTt2TIsWLYr74ACA5ObpQwg7duyI+rqmpkY5OTlqbGzUrFmz1NXVpV/96lfauHGj7rnnHknShg0b9I1vfEN79uzRd77znfhNDgBIapf1HlBXV5ckKSvr3K8xbmxsVF9fn0pKSiL7TJkyRePHj1dDw8CfLurt7VU4HI7aAACpL+YA9ff3a+XKlZo5c6amTp0qSWpvb9fIkSOVmZkZtW9ubq7a29sH/D5VVVUKBAKRLT8/P9aRAABJJOYAlZeX69ChQ9q8efNlDVBZWamurq7I1traelnfDwCQHGL6i6grVqzQ9u3btXv3bo0bNy7yeDAY1JkzZ3TixImoq6COjg4Fg8EBv5ff75ff749lDABAEvN0BeSc04oVK7Rlyxbt2rVLBQUFUc9Pnz5dI0aMUG1tbeSxpqYmHT16VMXFxfGZGACQEjxdAZWXl2vjxo3atm2b0tPTI+/rBAIBjR49WoFAQI8++qgqKiqUlZWljIwMPfnkkyouLuYTcACAKJ4CtG7dOknS7Nmzox7fsGGDli5dKkn65S9/qbS0NC1evFi9vb0qLS3V66+/HpdhAQCpw1OAnHOX3GfUqFGqrq5WdXV1zEPhyvrbQzfHtO6/XLfj0jsZOZPhsx4BwCVwLzgAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYiOk3oiK1pPXFtq7PnfW8ZoRvmOc1vc77gN2TvM8mSQP/3l4AicAVEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABggpuRQjmvfxzTug0rJnlec21ar+c1v1z/r57X3PxKbP9MAK4croAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABPcjBQx+1+3jrkixwmKG4sCqYgrIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGDCU4Cqqqp0xx13KD09XTk5OVq4cKGampqi9pk9e7Z8Pl/U9vjjj8d1aABA8vMUoPr6epWXl2vPnj3auXOn+vr6NHfuXPX09ETtt2zZMrW1tUW2tWvXxnVoAEDy8/QbUXfs2BH1dU1NjXJyctTY2KhZs2ZFHr/mmmsUDAbjMyEAICVd1ntAXV1dkqSsrKyox998801lZ2dr6tSpqqys1KlTpwb9Hr29vQqHw1EbACD1eboC+mf9/f1auXKlZs6cqalTp0Yef+ihhzRhwgSFQiEdPHhQzz77rJqamvTuu+8O+H2qqqq0Zs2aWMcAACQpn3POxbJw+fLl+u1vf6uPPvpI48aNG3S/Xbt2ac6cOWpubtakSZMueL63t1e9vb2Rr8PhsPLz8zVbCzTcNyKW0QAAhr50farTNnV1dSkjI2PQ/WK6AlqxYoW2b9+u3bt3XzQ+klRUVCRJgwbI7/fL7/fHMgYAIIl5CpBzTk8++aS2bNmiuro6FRQUXHLNgQMHJEl5eXkxDQgASE2eAlReXq6NGzdq27ZtSk9PV3t7uyQpEAho9OjROnLkiDZu3Kh7771XY8aM0cGDB7Vq1SrNmjVL06ZNS8g/AAAgOXl6D8jn8w34+IYNG7R06VK1trbq+9//vg4dOqSenh7l5+frvvvu03PPPXfRnwP+s3A4rEAgwHtAAJCkEvIe0KValZ+fr/r6ei/fEgBwleJecAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE8OtBzifc06S9KX6JGc8DADAsy/VJ+kf/z0fzJALUHd3tyTpI71vPAkA4HJ0d3crEAgM+rzPXSpRV1h/f7+OHTum9PR0+Xy+qOfC4bDy8/PV2tqqjIwMowntcR7O4Tycw3k4h/NwzlA4D845dXd3KxQKKS1t8Hd6htwVUFpamsaNG3fRfTIyMq7qF9hXOA/ncB7O4Tycw3k4x/o8XOzK5yt8CAEAYIIAAQBMJFWA/H6/Vq9eLb/fbz2KKc7DOZyHczgP53Aezkmm8zDkPoQAALg6JNUVEAAgdRAgAIAJAgQAMEGAAAAmkiZA1dXVuvHGGzVq1CgVFRXpD3/4g/VIV9yLL74on88XtU2ZMsV6rITbvXu35s+fr1AoJJ/Pp61bt0Y975zTCy+8oLy8PI0ePVolJSU6fPiwzbAJdKnzsHTp0gteH/PmzbMZNkGqqqp0xx13KD09XTk5OVq4cKGampqi9jl9+rTKy8s1ZswYXXfddVq8eLE6OjqMJk6Mr3MeZs+efcHr4fHHHzeaeGBJEaC33npLFRUVWr16tT755BMVFhaqtLRUx48ftx7tirvtttvU1tYW2T766CPrkRKup6dHhYWFqq6uHvD5tWvX6tVXX9X69eu1d+9eXXvttSotLdXp06ev8KSJdanzIEnz5s2Len1s2rTpCk6YePX19SovL9eePXu0c+dO9fX1ae7cuerp6Ynss2rVKr333nt65513VF9fr2PHjmnRokWGU8ff1zkPkrRs2bKo18PatWuNJh6ESwIzZsxw5eXlka/Pnj3rQqGQq6qqMpzqylu9erUrLCy0HsOUJLdly5bI1/39/S4YDLqXXnop8tiJEyec3+93mzZtMpjwyjj/PDjn3JIlS9yCBQtM5rFy/PhxJ8nV19c75879ux8xYoR75513Ivv86U9/cpJcQ0OD1ZgJd/55cM657373u+6pp56yG+prGPJXQGfOnFFjY6NKSkoij6WlpamkpEQNDQ2Gk9k4fPiwQqGQJk6cqIcfflhHjx61HslUS0uL2tvbo14fgUBARUVFV+Xro66uTjk5OZo8ebKWL1+uzs5O65ESqqurS5KUlZUlSWpsbFRfX1/U62HKlCkaP358Sr8ezj8PX3nzzTeVnZ2tqVOnqrKyUqdOnbIYb1BD7mak5/v888919uxZ5ebmRj2em5urTz/91GgqG0VFRaqpqdHkyZPV1tamNWvW6K677tKhQ4eUnp5uPZ6J9vZ2SRrw9fHVc1eLefPmadGiRSooKNCRI0f0ox/9SGVlZWpoaNCwYcOsx4u7/v5+rVy5UjNnztTUqVMlnXs9jBw5UpmZmVH7pvLrYaDzIEkPPfSQJkyYoFAopIMHD+rZZ59VU1OT3n33XcNpow35AOEfysrKIn+eNm2aioqKNGHCBL399tt69NFHDSfDUPDAAw9E/nz77bdr2rRpmjRpkurq6jRnzhzDyRKjvLxchw4duireB72Ywc7DY489Fvnz7bffrry8PM2ZM0dHjhzRpEmTrvSYAxryP4LLzs7WsGHDLvgUS0dHh4LBoNFUQ0NmZqZuueUWNTc3W49i5qvXAK+PC02cOFHZ2dkp+fpYsWKFtm/frg8//DDq17cEg0GdOXNGJ06ciNo/VV8Pg52HgRQVFUnSkHo9DPkAjRw5UtOnT1dtbW3ksf7+ftXW1qq4uNhwMnsnT57UkSNHlJeXZz2KmYKCAgWDwajXRzgc1t69e6/618dnn32mzs7OlHp9OOe0YsUKbdmyRbt27VJBQUHU89OnT9eIESOiXg9NTU06evRoSr0eLnUeBnLgwAFJGlqvB+tPQXwdmzdvdn6/39XU1Lg//vGP7rHHHnOZmZmuvb3derQr6gc/+IGrq6tzLS0t7ve//70rKSlx2dnZ7vjx49ajJVR3d7fbv3+/279/v5PkXn75Zbd//37317/+1Tnn3M9+9jOXmZnptm3b5g4ePOgWLFjgCgoK3BdffGE8eXxd7Dx0d3e7p59+2jU0NLiWlhb3wQcfuG9961vu5ptvdqdPn7YePW6WL1/uAoGAq6urc21tbZHt1KlTkX0ef/xxN378eLdr1y63b98+V1xc7IqLiw2njr9LnYfm5mb34x//2O3bt8+1tLS4bdu2uYkTJ7pZs2YZTx4tKQLknHOvvfaaGz9+vBs5cqSbMWOG27Nnj/VIV9z999/v8vLy3MiRI90NN9zg7r//ftfc3Gw9VsJ9+OGHTtIF25IlS5xz5z6K/fzzz7vc3Fzn9/vdnDlzXFNTk+3QCXCx83Dq1Ck3d+5cN3bsWDdixAg3YcIEt2zZspT7n7SB/vkluQ0bNkT2+eKLL9wTTzzhrr/+enfNNde4++67z7W1tdkNnQCXOg9Hjx51s2bNcllZWc7v97ubbrrJ/fCHP3RdXV22g5+HX8cAADAx5N8DAgCkJgIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAxP8DAfdsknhiFekAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(valLoader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "plt.imshow(images[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=25, out_features=25, bias=True)\n",
      "  (fc2): Linear(in_features=25, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 1, 3)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = torch.nn.Linear(25, 25)\n",
    "        self.fc2 = torch.nn.Linear(25, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "model = Net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAI(dataloader,valLoader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    trainLoss=0\n",
    "    for  X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        trainLoss +=loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    valLoss=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in valLoader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            valLoss+=loss.item()\n",
    "    \n",
    "    print(f'train loss: {trainLoss} - val Loss: {valLoss}')\n",
    "    return trainLoss, valLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 47.82567095756531 - val Loss: 11.852000713348389\n",
      "train loss: 39.98953175544739 - val Loss: 11.656530618667603\n",
      "train loss: 39.61702919006348 - val Loss: 11.60417652130127\n",
      "train loss: 39.481403827667236 - val Loss: 11.577975034713745\n",
      "train loss: 39.40501570701599 - val Loss: 11.56074857711792\n",
      "train loss: 39.351611375808716 - val Loss: 11.547809839248657\n",
      "train loss: 39.30980634689331 - val Loss: 11.537078857421875\n",
      "train loss: 39.27341365814209 - val Loss: 11.527392387390137\n",
      "train loss: 39.23808002471924 - val Loss: 11.517805576324463\n",
      "train loss: 39.20170259475708 - val Loss: 11.508115291595459\n",
      "train loss: 39.16263937950134 - val Loss: 11.497788667678833\n",
      "train loss: 39.121135234832764 - val Loss: 11.486840724945068\n",
      "train loss: 39.07527422904968 - val Loss: 11.474688529968262\n",
      "train loss: 39.02400326728821 - val Loss: 11.460606098175049\n",
      "train loss: 38.969637393951416 - val Loss: 11.445534706115723\n",
      "train loss: 38.91338014602661 - val Loss: 11.428425788879395\n",
      "train loss: 38.85186266899109 - val Loss: 11.409499645233154\n",
      "train loss: 38.78447127342224 - val Loss: 11.388900756835938\n",
      "train loss: 38.70997667312622 - val Loss: 11.366587400436401\n",
      "train loss: 38.62761950492859 - val Loss: 11.34138035774231\n",
      "train loss: 38.53663659095764 - val Loss: 11.312745094299316\n",
      "train loss: 38.4347505569458 - val Loss: 11.280241250991821\n",
      "train loss: 38.31967830657959 - val Loss: 11.24437427520752\n",
      "train loss: 38.19181847572327 - val Loss: 11.204661130905151\n",
      "train loss: 38.046940326690674 - val Loss: 11.159791231155396\n",
      "train loss: 37.882994174957275 - val Loss: 11.109485387802124\n",
      "train loss: 37.6979775428772 - val Loss: 11.052574396133423\n",
      "train loss: 37.49247908592224 - val Loss: 10.989365816116333\n",
      "train loss: 37.26352548599243 - val Loss: 10.918722867965698\n",
      "train loss: 37.010552406311035 - val Loss: 10.84041142463684\n",
      "train loss: 36.73282814025879 - val Loss: 10.755358457565308\n",
      "train loss: 36.43032717704773 - val Loss: 10.662781953811646\n",
      "train loss: 36.101306676864624 - val Loss: 10.563026666641235\n",
      "train loss: 35.74862766265869 - val Loss: 10.45711064338684\n",
      "train loss: 35.37270712852478 - val Loss: 10.344639539718628\n",
      "train loss: 34.97478222846985 - val Loss: 10.226263999938965\n",
      "train loss: 34.556358337402344 - val Loss: 10.101751327514648\n",
      "train loss: 34.117748379707336 - val Loss: 9.970741271972656\n",
      "train loss: 33.662479639053345 - val Loss: 9.83348834514618\n",
      "train loss: 33.19126379489899 - val Loss: 9.692800402641296\n",
      "train loss: 32.698851108551025 - val Loss: 9.545312881469727\n",
      "train loss: 32.18217647075653 - val Loss: 9.389945268630981\n",
      "train loss: 31.638126611709595 - val Loss: 9.230132818222046\n",
      "train loss: 31.090277552604675 - val Loss: 9.070343136787415\n",
      "train loss: 30.534448385238647 - val Loss: 8.906083703041077\n",
      "train loss: 29.964534163475037 - val Loss: 8.737150311470032\n",
      "train loss: 29.380568265914917 - val Loss: 8.562078595161438\n",
      "train loss: 28.765912294387817 - val Loss: 8.3813556432724\n",
      "train loss: 28.11104142665863 - val Loss: 8.191025376319885\n",
      "train loss: 27.414183020591736 - val Loss: 7.9850945472717285\n",
      "train loss: 26.67923665046692 - val Loss: 7.769428610801697\n",
      "train loss: 25.919485926628113 - val Loss: 7.553576350212097\n",
      "train loss: 25.17746591567993 - val Loss: 7.3459848165512085\n",
      "train loss: 24.477911353111267 - val Loss: 7.151453495025635\n",
      "train loss: 23.830313444137573 - val Loss: 6.974181890487671\n",
      "train loss: 23.233133554458618 - val Loss: 6.812052369117737\n",
      "train loss: 22.681591272354126 - val Loss: 6.6643208265304565\n",
      "train loss: 22.170673489570618 - val Loss: 6.528780460357666\n",
      "train loss: 21.69587254524231 - val Loss: 6.403059124946594\n",
      "train loss: 21.255486369132996 - val Loss: 6.287276029586792\n",
      "train loss: 20.847553372383118 - val Loss: 6.180604934692383\n",
      "train loss: 20.469877243041992 - val Loss: 6.081489562988281\n",
      "train loss: 20.120748281478882 - val Loss: 5.988877534866333\n",
      "train loss: 19.797189593315125 - val Loss: 5.9038331508636475\n",
      "train loss: 19.496811509132385 - val Loss: 5.824985384941101\n",
      "train loss: 19.217058539390564 - val Loss: 5.751428961753845\n",
      "train loss: 18.956135630607605 - val Loss: 5.681821823120117\n",
      "train loss: 18.711936950683594 - val Loss: 5.616586089134216\n",
      "train loss: 18.48263967037201 - val Loss: 5.555008053779602\n",
      "train loss: 18.266396164894104 - val Loss: 5.4963178634643555\n",
      "train loss: 18.06222414970398 - val Loss: 5.440729737281799\n",
      "train loss: 17.86889272928238 - val Loss: 5.387707948684692\n",
      "train loss: 17.685171842575073 - val Loss: 5.337308049201965\n",
      "train loss: 17.509479582309723 - val Loss: 5.289356231689453\n",
      "train loss: 17.340475916862488 - val Loss: 5.243270993232727\n",
      "train loss: 17.17846965789795 - val Loss: 5.1988303661346436\n",
      "train loss: 17.022342264652252 - val Loss: 5.1557435393333435\n",
      "train loss: 16.87108999490738 - val Loss: 5.113532543182373\n",
      "train loss: 16.724529504776 - val Loss: 5.072848737239838\n",
      "train loss: 16.582397878170013 - val Loss: 5.033259391784668\n",
      "train loss: 16.444825768470764 - val Loss: 4.995044529438019\n",
      "train loss: 16.310608208179474 - val Loss: 4.958053171634674\n",
      "train loss: 16.180319607257843 - val Loss: 4.921358823776245\n",
      "train loss: 16.053567707538605 - val Loss: 4.885639011859894\n",
      "train loss: 15.929989874362946 - val Loss: 4.850351870059967\n",
      "train loss: 15.809097349643707 - val Loss: 4.815829396247864\n",
      "train loss: 15.690318644046783 - val Loss: 4.781578600406647\n",
      "train loss: 15.573790192604065 - val Loss: 4.74771374464035\n",
      "train loss: 15.459103167057037 - val Loss: 4.714357554912567\n",
      "train loss: 15.346040427684784 - val Loss: 4.681130528450012\n",
      "train loss: 15.235175132751465 - val Loss: 4.648033082485199\n",
      "train loss: 15.125548303127289 - val Loss: 4.61551308631897\n",
      "train loss: 15.017439365386963 - val Loss: 4.583278298377991\n",
      "train loss: 14.91025048494339 - val Loss: 4.550966322422028\n",
      "train loss: 14.803735554218292 - val Loss: 4.518650531768799\n",
      "train loss: 14.69916021823883 - val Loss: 4.486673355102539\n",
      "train loss: 14.59581845998764 - val Loss: 4.4551045298576355\n",
      "train loss: 14.492886126041412 - val Loss: 4.424349665641785\n",
      "train loss: 14.391144931316376 - val Loss: 4.394091904163361\n",
      "train loss: 14.290965378284454 - val Loss: 4.3642080426216125\n",
      "train loss: 14.193036198616028 - val Loss: 4.334520637989044\n",
      "train loss: 14.095953524112701 - val Loss: 4.304924309253693\n",
      "train loss: 14.000198721885681 - val Loss: 4.276329398155212\n",
      "train loss: 13.905089557170868 - val Loss: 4.248300671577454\n",
      "train loss: 13.811055660247803 - val Loss: 4.220325767993927\n",
      "train loss: 13.718457639217377 - val Loss: 4.193209707736969\n",
      "train loss: 13.627169668674469 - val Loss: 4.1664950251579285\n",
      "train loss: 13.537688910961151 - val Loss: 4.140611469745636\n",
      "train loss: 13.449707627296448 - val Loss: 4.11524373292923\n",
      "train loss: 13.363659918308258 - val Loss: 4.090086340904236\n",
      "train loss: 13.278793692588806 - val Loss: 4.065156936645508\n",
      "train loss: 13.195461094379425 - val Loss: 4.04063755273819\n",
      "train loss: 13.113643527030945 - val Loss: 4.016798436641693\n",
      "train loss: 13.033202350139618 - val Loss: 3.9932122826576233\n",
      "train loss: 12.953946053981781 - val Loss: 3.970150053501129\n",
      "train loss: 12.875458002090454 - val Loss: 3.9473133087158203\n",
      "train loss: 12.798137545585632 - val Loss: 3.9247092604637146\n",
      "train loss: 12.721995651721954 - val Loss: 3.902419924736023\n",
      "train loss: 12.647567808628082 - val Loss: 3.880657434463501\n",
      "train loss: 12.574644982814789 - val Loss: 3.8592294454574585\n",
      "train loss: 12.502938747406006 - val Loss: 3.8381332755088806\n",
      "train loss: 12.432162165641785 - val Loss: 3.8173047304153442\n",
      "train loss: 12.362616121768951 - val Loss: 3.796757996082306\n",
      "train loss: 12.294532835483551 - val Loss: 3.776444137096405\n",
      "train loss: 12.227365732192993 - val Loss: 3.7565184831619263\n",
      "train loss: 12.161381483078003 - val Loss: 3.7367860674858093\n",
      "train loss: 12.096500813961029 - val Loss: 3.71706223487854\n",
      "train loss: 12.03262346982956 - val Loss: 3.697673976421356\n",
      "train loss: 11.970095932483673 - val Loss: 3.6785427927970886\n",
      "train loss: 11.90832394361496 - val Loss: 3.6601317524909973\n",
      "train loss: 11.847712278366089 - val Loss: 3.6418951749801636\n",
      "train loss: 11.787904858589172 - val Loss: 3.623813569545746\n",
      "train loss: 11.729238450527191 - val Loss: 3.605925977230072\n",
      "train loss: 11.671302616596222 - val Loss: 3.5883509516716003\n",
      "train loss: 11.614527881145477 - val Loss: 3.57099312543869\n",
      "train loss: 11.558553636074066 - val Loss: 3.55416738986969\n",
      "train loss: 11.503317296504974 - val Loss: 3.537239372730255\n",
      "train loss: 11.448497951030731 - val Loss: 3.5205734968185425\n",
      "train loss: 11.394553244113922 - val Loss: 3.5038042664527893\n",
      "train loss: 11.341211318969727 - val Loss: 3.4874123334884644\n",
      "train loss: 11.288828253746033 - val Loss: 3.471211612224579\n",
      "train loss: 11.237054646015167 - val Loss: 3.4552814960479736\n",
      "train loss: 11.185940444469452 - val Loss: 3.4393956661224365\n",
      "train loss: 11.135498583316803 - val Loss: 3.423637270927429\n",
      "train loss: 11.085817694664001 - val Loss: 3.4083414673805237\n",
      "train loss: 11.03667289018631 - val Loss: 3.39300400018692\n",
      "train loss: 10.988475143909454 - val Loss: 3.37817120552063\n",
      "train loss: 10.941006898880005 - val Loss: 3.363529145717621\n",
      "train loss: 10.894382357597351 - val Loss: 3.3490095734596252\n",
      "train loss: 10.848283886909485 - val Loss: 3.3350228667259216\n",
      "train loss: 10.802882671356201 - val Loss: 3.3210108876228333\n",
      "train loss: 10.757951259613037 - val Loss: 3.306945323944092\n",
      "train loss: 10.713612616062164 - val Loss: 3.292939603328705\n",
      "train loss: 10.669728338718414 - val Loss: 3.2791870832443237\n",
      "train loss: 10.626206636428833 - val Loss: 3.2657865285873413\n",
      "train loss: 10.583218157291412 - val Loss: 3.2525312900543213\n",
      "train loss: 10.540674209594727 - val Loss: 3.239384889602661\n",
      "train loss: 10.498670518398285 - val Loss: 3.2266026735305786\n",
      "train loss: 10.457278609275818 - val Loss: 3.2139318585395813\n",
      "train loss: 10.416444003582 - val Loss: 3.2016764283180237\n",
      "train loss: 10.376268029212952 - val Loss: 3.1894920468330383\n",
      "train loss: 10.336491644382477 - val Loss: 3.177124798297882\n",
      "train loss: 10.297491014003754 - val Loss: 3.164896607398987\n",
      "train loss: 10.259059131145477 - val Loss: 3.15300053358078\n",
      "train loss: 10.221164762973785 - val Loss: 3.1411516070365906\n",
      "train loss: 10.18389230966568 - val Loss: 3.12960284948349\n",
      "train loss: 10.146990776062012 - val Loss: 3.1184677481651306\n",
      "train loss: 10.110456109046936 - val Loss: 3.107559382915497\n",
      "train loss: 10.074309647083282 - val Loss: 3.096650719642639\n",
      "train loss: 10.038498401641846 - val Loss: 3.086004614830017\n",
      "train loss: 10.00336641073227 - val Loss: 3.0752036571502686\n",
      "train loss: 9.968555867671967 - val Loss: 3.0645203590393066\n",
      "train loss: 9.934143126010895 - val Loss: 3.0540902614593506\n",
      "train loss: 9.900009393692017 - val Loss: 3.043890953063965\n",
      "train loss: 9.866289556026459 - val Loss: 3.033728241920471\n",
      "train loss: 9.833205878734589 - val Loss: 3.023711681365967\n",
      "train loss: 9.800598442554474 - val Loss: 3.013869047164917\n",
      "train loss: 9.768367767333984 - val Loss: 3.0039320588111877\n",
      "train loss: 9.736603081226349 - val Loss: 2.9941583275794983\n",
      "train loss: 9.705240964889526 - val Loss: 2.984551250934601\n",
      "train loss: 9.674437463283539 - val Loss: 2.9751394987106323\n",
      "train loss: 9.643787920475006 - val Loss: 2.9656814336776733\n",
      "train loss: 9.613617420196533 - val Loss: 2.956207036972046\n",
      "train loss: 9.583812773227692 - val Loss: 2.9469019770622253\n",
      "train loss: 9.554234445095062 - val Loss: 2.937631070613861\n",
      "train loss: 9.525134265422821 - val Loss: 2.928519904613495\n",
      "train loss: 9.49649703502655 - val Loss: 2.9195730686187744\n",
      "train loss: 9.468194603919983 - val Loss: 2.910804867744446\n",
      "train loss: 9.440199971199036 - val Loss: 2.902043581008911\n",
      "train loss: 9.412506341934204 - val Loss: 2.893373191356659\n",
      "train loss: 9.38524979352951 - val Loss: 2.8846977949142456\n",
      "train loss: 9.358161628246307 - val Loss: 2.8761494755744934\n",
      "train loss: 9.331227540969849 - val Loss: 2.8677518367767334\n",
      "train loss: 9.30451637506485 - val Loss: 2.85927951335907\n",
      "train loss: 9.278045177459717 - val Loss: 2.8507014513015747\n",
      "train loss: 9.251691579818726 - val Loss: 2.842232644557953\n",
      "train loss: 9.225484013557434 - val Loss: 2.833918869495392\n",
      "train loss: 9.199717938899994 - val Loss: 2.82592636346817\n",
      "train loss: 9.174220979213715 - val Loss: 2.818015933036804\n",
      "train loss: 9.148989021778107 - val Loss: 2.810310125350952\n",
      "train loss: 9.124053359031677 - val Loss: 2.8026755452156067\n",
      "train loss: 9.099293529987335 - val Loss: 2.7951603531837463\n",
      "train loss: 9.074778497219086 - val Loss: 2.787738561630249\n",
      "train loss: 9.05055856704712 - val Loss: 2.7803589701652527\n",
      "train loss: 9.026607036590576 - val Loss: 2.7727800607681274\n",
      "train loss: 9.002803802490234 - val Loss: 2.765112280845642\n",
      "train loss: 8.979142665863037 - val Loss: 2.757541060447693\n",
      "train loss: 8.955706655979156 - val Loss: 2.7498276233673096\n",
      "train loss: 8.93244057893753 - val Loss: 2.742485523223877\n",
      "train loss: 8.909604251384735 - val Loss: 2.7351778149604797\n",
      "train loss: 8.886912286281586 - val Loss: 2.7280929684638977\n",
      "train loss: 8.864513397216797 - val Loss: 2.721237063407898\n",
      "train loss: 8.842388361692429 - val Loss: 2.714169204235077\n",
      "train loss: 8.82045915722847 - val Loss: 2.7073253989219666\n",
      "train loss: 8.79886394739151 - val Loss: 2.7005635499954224\n",
      "train loss: 8.77746120095253 - val Loss: 2.6938669681549072\n",
      "train loss: 8.756311982870102 - val Loss: 2.6871346533298492\n",
      "train loss: 8.735210478305817 - val Loss: 2.6803742945194244\n",
      "train loss: 8.714276671409607 - val Loss: 2.6738485991954803\n",
      "train loss: 8.693584829568863 - val Loss: 2.667427361011505\n",
      "train loss: 8.673081368207932 - val Loss: 2.660895824432373\n",
      "train loss: 8.652676045894623 - val Loss: 2.6544685661792755\n",
      "train loss: 8.632518827915192 - val Loss: 2.648062825202942\n",
      "train loss: 8.612517297267914 - val Loss: 2.641686201095581\n",
      "train loss: 8.592759758234024 - val Loss: 2.6352687776088715\n",
      "train loss: 8.573149889707565 - val Loss: 2.6287757754325867\n",
      "train loss: 8.553592324256897 - val Loss: 2.6224856674671173\n",
      "train loss: 8.53413799405098 - val Loss: 2.6161832213401794\n",
      "train loss: 8.515037506818771 - val Loss: 2.6101874709129333\n",
      "train loss: 8.49600899219513 - val Loss: 2.604293465614319\n",
      "train loss: 8.47716623544693 - val Loss: 2.598201036453247\n",
      "train loss: 8.45842656493187 - val Loss: 2.592164307832718\n",
      "train loss: 8.439700812101364 - val Loss: 2.5858736634254456\n",
      "train loss: 8.420846611261368 - val Loss: 2.579469919204712\n",
      "train loss: 8.40222218632698 - val Loss: 2.573430359363556\n",
      "train loss: 8.383706718683243 - val Loss: 2.567395359277725\n",
      "train loss: 8.365411072969437 - val Loss: 2.5614024102687836\n",
      "train loss: 8.347279161214828 - val Loss: 2.5554367899894714\n",
      "train loss: 8.32915523648262 - val Loss: 2.549444556236267\n",
      "train loss: 8.311269700527191 - val Loss: 2.5438363552093506\n",
      "train loss: 8.293554097414017 - val Loss: 2.5380951166152954\n",
      "train loss: 8.275810897350311 - val Loss: 2.532471001148224\n",
      "train loss: 8.258259505033493 - val Loss: 2.5270781815052032\n",
      "train loss: 8.240859478712082 - val Loss: 2.521691143512726\n",
      "train loss: 8.223656952381134 - val Loss: 2.5165625512599945\n",
      "train loss: 8.20662584900856 - val Loss: 2.5114752650260925\n",
      "train loss: 8.189749211072922 - val Loss: 2.5064005851745605\n",
      "train loss: 8.173026710748672 - val Loss: 2.5013962984085083\n",
      "train loss: 8.15642061829567 - val Loss: 2.4962298572063446\n",
      "train loss: 8.139834076166153 - val Loss: 2.4913398921489716\n",
      "train loss: 8.123306691646576 - val Loss: 2.4863586127758026\n",
      "train loss: 8.106901943683624 - val Loss: 2.4811721444129944\n",
      "train loss: 8.090359300374985 - val Loss: 2.4759248793125153\n",
      "train loss: 8.073927134275436 - val Loss: 2.4707425236701965\n",
      "train loss: 8.057620227336884 - val Loss: 2.4653855562210083\n",
      "train loss: 8.041551142930984 - val Loss: 2.4602476358413696\n",
      "train loss: 8.025636702775955 - val Loss: 2.4550371170043945\n",
      "train loss: 8.00983664393425 - val Loss: 2.450071841478348\n",
      "train loss: 7.994246333837509 - val Loss: 2.445236772298813\n",
      "train loss: 7.978805214166641 - val Loss: 2.440303236246109\n",
      "train loss: 7.963438868522644 - val Loss: 2.4356227219104767\n",
      "train loss: 7.948172926902771 - val Loss: 2.430811583995819\n",
      "train loss: 7.9330835044384 - val Loss: 2.4262035191059113\n",
      "train loss: 7.918089658021927 - val Loss: 2.421627104282379\n",
      "train loss: 7.903212130069733 - val Loss: 2.4169906675815582\n",
      "train loss: 7.88841587305069 - val Loss: 2.4123663902282715\n",
      "train loss: 7.873628377914429 - val Loss: 2.4079730808734894\n",
      "train loss: 7.858996599912643 - val Loss: 2.4037162959575653\n",
      "train loss: 7.844598829746246 - val Loss: 2.3991699814796448\n",
      "train loss: 7.830248445272446 - val Loss: 2.3947413563728333\n",
      "train loss: 7.815902978181839 - val Loss: 2.3903575241565704\n",
      "train loss: 7.801486164331436 - val Loss: 2.3855442702770233\n",
      "train loss: 7.787130743265152 - val Loss: 2.3810333609580994\n",
      "train loss: 7.772855907678604 - val Loss: 2.3768663108348846\n",
      "train loss: 7.758756309747696 - val Loss: 2.372700810432434\n",
      "train loss: 7.744820058345795 - val Loss: 2.368728518486023\n",
      "train loss: 7.7309450507164 - val Loss: 2.3645505607128143\n",
      "train loss: 7.717229962348938 - val Loss: 2.3606316447257996\n",
      "train loss: 7.7036562860012054 - val Loss: 2.356579542160034\n",
      "train loss: 7.690341740846634 - val Loss: 2.3526013791561127\n",
      "train loss: 7.6771988570690155 - val Loss: 2.348799556493759\n",
      "train loss: 7.664246618747711 - val Loss: 2.3450793623924255\n",
      "train loss: 7.651499092578888 - val Loss: 2.341296374797821\n",
      "train loss: 7.638857305049896 - val Loss: 2.337529867887497\n",
      "train loss: 7.626359313726425 - val Loss: 2.3336822390556335\n",
      "train loss: 7.613929837942123 - val Loss: 2.3300013840198517\n",
      "train loss: 7.601555198431015 - val Loss: 2.326199769973755\n",
      "train loss: 7.589261382818222 - val Loss: 2.322620779275894\n",
      "train loss: 7.57713320851326 - val Loss: 2.319252461194992\n",
      "train loss: 7.565048545598984 - val Loss: 2.315852791070938\n",
      "train loss: 7.553122013807297 - val Loss: 2.3124892115592957\n",
      "train loss: 7.54131355881691 - val Loss: 2.30908203125\n",
      "train loss: 7.529589086771011 - val Loss: 2.305737406015396\n",
      "train loss: 7.517962008714676 - val Loss: 2.302451103925705\n",
      "train loss: 7.506447911262512 - val Loss: 2.299180030822754\n",
      "train loss: 7.495066672563553 - val Loss: 2.2959109246730804\n",
      "train loss: 7.483799308538437 - val Loss: 2.2925126254558563\n",
      "train loss: 7.472572714090347 - val Loss: 2.2892861366271973\n",
      "train loss: 7.461422026157379 - val Loss: 2.2861309349536896\n",
      "train loss: 7.450346082448959 - val Loss: 2.2829275727272034\n",
      "train loss: 7.439341455698013 - val Loss: 2.2797346711158752\n",
      "train loss: 7.42851322889328 - val Loss: 2.2765660285949707\n",
      "train loss: 7.417775511741638 - val Loss: 2.2733535766601562\n",
      "train loss: 7.407059907913208 - val Loss: 2.2702472507953644\n",
      "train loss: 7.396337032318115 - val Loss: 2.2671920359134674\n",
      "train loss: 7.3857594430446625 - val Loss: 2.2641675770282745\n",
      "train loss: 7.375227063894272 - val Loss: 2.261107712984085\n",
      "train loss: 7.364756256341934 - val Loss: 2.258110076189041\n",
      "train loss: 7.354359447956085 - val Loss: 2.255130559206009\n",
      "train loss: 7.344043403863907 - val Loss: 2.2521751821041107\n",
      "train loss: 7.33378067612648 - val Loss: 2.249307632446289\n",
      "train loss: 7.323582649230957 - val Loss: 2.246348559856415\n",
      "train loss: 7.313403010368347 - val Loss: 2.2434182465076447\n",
      "train loss: 7.303303897380829 - val Loss: 2.2405359148979187\n",
      "train loss: 7.293225347995758 - val Loss: 2.237740159034729\n",
      "train loss: 7.283223330974579 - val Loss: 2.2349062860012054\n",
      "train loss: 7.273254781961441 - val Loss: 2.232043445110321\n",
      "train loss: 7.2633746564388275 - val Loss: 2.22914519906044\n",
      "train loss: 7.253541082143784 - val Loss: 2.2262981235980988\n",
      "train loss: 7.243754833936691 - val Loss: 2.2234359681606293\n",
      "train loss: 7.2340215146541595 - val Loss: 2.220607340335846\n",
      "train loss: 7.224320948123932 - val Loss: 2.217978149652481\n",
      "train loss: 7.214689940214157 - val Loss: 2.2152372300624847\n",
      "train loss: 7.205113798379898 - val Loss: 2.21249258518219\n",
      "train loss: 7.195608288049698 - val Loss: 2.2097646296024323\n",
      "train loss: 7.1861632168293 - val Loss: 2.2070964574813843\n",
      "train loss: 7.176758229732513 - val Loss: 2.2044799625873566\n",
      "train loss: 7.1674244701862335 - val Loss: 2.2017528414726257\n",
      "train loss: 7.158056080341339 - val Loss: 2.1990843415260315\n",
      "train loss: 7.148739665746689 - val Loss: 2.196431338787079\n",
      "train loss: 7.139483213424683 - val Loss: 2.193749040365219\n",
      "train loss: 7.130252957344055 - val Loss: 2.1911149621009827\n",
      "train loss: 7.121035248041153 - val Loss: 2.188502252101898\n",
      "train loss: 7.1119332909584045 - val Loss: 2.1857719123363495\n",
      "train loss: 7.10284897685051 - val Loss: 2.1831249594688416\n",
      "train loss: 7.093771934509277 - val Loss: 2.18062561750412\n",
      "train loss: 7.084730535745621 - val Loss: 2.177924692630768\n",
      "train loss: 7.075647592544556 - val Loss: 2.1752211451530457\n",
      "train loss: 7.066639870405197 - val Loss: 2.172573894262314\n",
      "train loss: 7.057695060968399 - val Loss: 2.1700367629528046\n",
      "train loss: 7.048829078674316 - val Loss: 2.1675206124782562\n",
      "train loss: 7.039952367544174 - val Loss: 2.164952039718628\n",
      "train loss: 7.031102478504181 - val Loss: 2.162365674972534\n",
      "train loss: 7.022334098815918 - val Loss: 2.15966734290123\n",
      "train loss: 7.013588011264801 - val Loss: 2.15701225399971\n",
      "train loss: 7.0048536360263824 - val Loss: 2.1543578505516052\n",
      "train loss: 6.996166557073593 - val Loss: 2.151703715324402\n",
      "train loss: 6.9874787628650665 - val Loss: 2.1490543484687805\n",
      "train loss: 6.978844791650772 - val Loss: 2.14643856883049\n",
      "train loss: 6.970183908939362 - val Loss: 2.1438259184360504\n",
      "train loss: 6.961598604917526 - val Loss: 2.141293525695801\n",
      "train loss: 6.953055649995804 - val Loss: 2.138816088438034\n",
      "train loss: 6.944495916366577 - val Loss: 2.1363461315631866\n",
      "train loss: 6.935945093631744 - val Loss: 2.133781284093857\n",
      "train loss: 6.927545249462128 - val Loss: 2.1313180029392242\n",
      "train loss: 6.919142067432404 - val Loss: 2.1288000345230103\n",
      "train loss: 6.91081103682518 - val Loss: 2.126337558031082\n",
      "train loss: 6.90248367190361 - val Loss: 2.123829334974289\n",
      "train loss: 6.8942753076553345 - val Loss: 2.1212885677814484\n",
      "train loss: 6.886068224906921 - val Loss: 2.118803858757019\n",
      "train loss: 6.877939343452454 - val Loss: 2.1164093613624573\n",
      "train loss: 6.86974424123764 - val Loss: 2.114082992076874\n",
      "train loss: 6.861596375703812 - val Loss: 2.1116380989551544\n",
      "train loss: 6.853458672761917 - val Loss: 2.1091728806495667\n",
      "train loss: 6.845381140708923 - val Loss: 2.1066858768463135\n",
      "train loss: 6.837358087301254 - val Loss: 2.1043813228607178\n",
      "train loss: 6.829375267028809 - val Loss: 2.1019192337989807\n",
      "train loss: 6.821406126022339 - val Loss: 2.0994516015052795\n",
      "train loss: 6.813402503728867 - val Loss: 2.0970795154571533\n",
      "train loss: 6.8055309653282166 - val Loss: 2.094715893268585\n",
      "train loss: 6.797650039196014 - val Loss: 2.0924054384231567\n",
      "train loss: 6.789745539426804 - val Loss: 2.09000563621521\n",
      "train loss: 6.7818527817726135 - val Loss: 2.0876494348049164\n",
      "train loss: 6.77398282289505 - val Loss: 2.08521631360054\n",
      "train loss: 6.766156584024429 - val Loss: 2.0828218162059784\n",
      "train loss: 6.758459985256195 - val Loss: 2.080457866191864\n",
      "train loss: 6.75078684091568 - val Loss: 2.078261524438858\n",
      "train loss: 6.743106126785278 - val Loss: 2.0759901106357574\n",
      "train loss: 6.735481351613998 - val Loss: 2.073671281337738\n",
      "train loss: 6.727888882160187 - val Loss: 2.0713181495666504\n",
      "train loss: 6.720270484685898 - val Loss: 2.068989545106888\n",
      "train loss: 6.712644815444946 - val Loss: 2.0667985677719116\n",
      "train loss: 6.705052554607391 - val Loss: 2.0646435916423798\n",
      "train loss: 6.697482883930206 - val Loss: 2.06243234872818\n",
      "train loss: 6.689941942691803 - val Loss: 2.0602774024009705\n",
      "train loss: 6.682404339313507 - val Loss: 2.058083027601242\n",
      "train loss: 6.674944341182709 - val Loss: 2.05595126748085\n",
      "train loss: 6.667496681213379 - val Loss: 2.053904950618744\n",
      "train loss: 6.660073846578598 - val Loss: 2.051748663187027\n",
      "train loss: 6.652720004320145 - val Loss: 2.049588292837143\n",
      "train loss: 6.645385354757309 - val Loss: 2.047519028186798\n",
      "train loss: 6.638075560331345 - val Loss: 2.045375943183899\n",
      "train loss: 6.6308340430259705 - val Loss: 2.0431919395923615\n",
      "train loss: 6.623601496219635 - val Loss: 2.0410172045230865\n",
      "train loss: 6.616438269615173 - val Loss: 2.038961112499237\n",
      "train loss: 6.609292536973953 - val Loss: 2.0367840230464935\n",
      "train loss: 6.602179020643234 - val Loss: 2.0345985293388367\n",
      "train loss: 6.595156967639923 - val Loss: 2.0325277149677277\n",
      "train loss: 6.588092178106308 - val Loss: 2.030403256416321\n",
      "train loss: 6.581050932407379 - val Loss: 2.028243362903595\n",
      "train loss: 6.573990643024445 - val Loss: 2.0259624123573303\n",
      "train loss: 6.567012667655945 - val Loss: 2.023880660533905\n",
      "train loss: 6.5600462555885315 - val Loss: 2.0216943323612213\n",
      "train loss: 6.553126245737076 - val Loss: 2.019562691450119\n",
      "train loss: 6.54620698094368 - val Loss: 2.0174827873706818\n",
      "train loss: 6.539421200752258 - val Loss: 2.0154071748256683\n",
      "train loss: 6.532582104206085 - val Loss: 2.0133370459079742\n",
      "train loss: 6.525786429643631 - val Loss: 2.0112136900424957\n",
      "train loss: 6.519125998020172 - val Loss: 2.009033501148224\n",
      "train loss: 6.512452781200409 - val Loss: 2.006919205188751\n",
      "train loss: 6.505752742290497 - val Loss: 2.00491464138031\n",
      "train loss: 6.499155968427658 - val Loss: 2.0027306377887726\n",
      "train loss: 6.49253037571907 - val Loss: 2.0007871985435486\n",
      "train loss: 6.48595467209816 - val Loss: 1.9986867010593414\n",
      "train loss: 6.47945499420166 - val Loss: 1.9967536628246307\n",
      "train loss: 6.472826838493347 - val Loss: 1.9947976171970367\n",
      "train loss: 6.466294825077057 - val Loss: 1.9928323328495026\n",
      "train loss: 6.459741950035095 - val Loss: 1.9908665418624878\n",
      "train loss: 6.453298032283783 - val Loss: 1.9888848066329956\n",
      "train loss: 6.446887820959091 - val Loss: 1.9868604242801666\n",
      "train loss: 6.440459489822388 - val Loss: 1.9848682582378387\n",
      "train loss: 6.434082269668579 - val Loss: 1.9829275012016296\n",
      "train loss: 6.427748650312424 - val Loss: 1.981081783771515\n",
      "train loss: 6.421440988779068 - val Loss: 1.9791236519813538\n",
      "train loss: 6.415173023939133 - val Loss: 1.977158635854721\n",
      "train loss: 6.40894079208374 - val Loss: 1.9753670990467072\n",
      "train loss: 6.402634918689728 - val Loss: 1.973457008600235\n",
      "train loss: 6.396417677402496 - val Loss: 1.9714956879615784\n",
      "train loss: 6.390169829130173 - val Loss: 1.9695515930652618\n",
      "train loss: 6.3840272128582 - val Loss: 1.9675947725772858\n",
      "train loss: 6.3778893649578094 - val Loss: 1.9657748639583588\n",
      "train loss: 6.371796667575836 - val Loss: 1.9639858305454254\n",
      "train loss: 6.365733116865158 - val Loss: 1.962070256471634\n",
      "train loss: 6.359669804573059 - val Loss: 1.9603069722652435\n",
      "train loss: 6.353664219379425 - val Loss: 1.9584711790084839\n",
      "train loss: 6.3476729691028595 - val Loss: 1.9565694034099579\n",
      "train loss: 6.341687440872192 - val Loss: 1.954787939786911\n",
      "train loss: 6.335697591304779 - val Loss: 1.9530421793460846\n",
      "train loss: 6.329788833856583 - val Loss: 1.9512217342853546\n",
      "train loss: 6.323892533779144 - val Loss: 1.949523240327835\n",
      "train loss: 6.318064719438553 - val Loss: 1.9478888511657715\n",
      "train loss: 6.3122143149375916 - val Loss: 1.9461245238780975\n",
      "train loss: 6.306378066539764 - val Loss: 1.9443560242652893\n",
      "train loss: 6.300555258989334 - val Loss: 1.9425742328166962\n",
      "train loss: 6.294766843318939 - val Loss: 1.9408546686172485\n",
      "train loss: 6.289061009883881 - val Loss: 1.9390768706798553\n",
      "train loss: 6.283369362354279 - val Loss: 1.9372261762619019\n",
      "train loss: 6.277730733156204 - val Loss: 1.935470849275589\n",
      "train loss: 6.272108256816864 - val Loss: 1.9336715340614319\n",
      "train loss: 6.266451001167297 - val Loss: 1.931877464056015\n",
      "train loss: 6.260843634605408 - val Loss: 1.9301754832267761\n",
      "train loss: 6.2552530169487 - val Loss: 1.928333580493927\n",
      "train loss: 6.249688297510147 - val Loss: 1.9265795648097992\n",
      "train loss: 6.2441950142383575 - val Loss: 1.9247609972953796\n",
      "train loss: 6.238655537366867 - val Loss: 1.923000454902649\n",
      "train loss: 6.233086943626404 - val Loss: 1.921309232711792\n",
      "train loss: 6.227570950984955 - val Loss: 1.9196237623691559\n",
      "train loss: 6.222116142511368 - val Loss: 1.9178539216518402\n",
      "train loss: 6.216652870178223 - val Loss: 1.9161564409732819\n",
      "train loss: 6.211185187101364 - val Loss: 1.9144010245800018\n",
      "train loss: 6.20578008890152 - val Loss: 1.9127788245677948\n",
      "train loss: 6.200369507074356 - val Loss: 1.911123514175415\n",
      "train loss: 6.195024073123932 - val Loss: 1.9093764126300812\n",
      "train loss: 6.189736694097519 - val Loss: 1.9076768159866333\n",
      "train loss: 6.1844532787799835 - val Loss: 1.9061309695243835\n",
      "train loss: 6.179223895072937 - val Loss: 1.9047237634658813\n",
      "train loss: 6.17396467924118 - val Loss: 1.9030874073505402\n",
      "train loss: 6.168774574995041 - val Loss: 1.9014174342155457\n",
      "train loss: 6.163613587617874 - val Loss: 1.8996935486793518\n",
      "train loss: 6.158462554216385 - val Loss: 1.8979486227035522\n",
      "train loss: 6.153369694948196 - val Loss: 1.896294891834259\n",
      "train loss: 6.1482610404491425 - val Loss: 1.8946094810962677\n",
      "train loss: 6.1431286334991455 - val Loss: 1.8928900063037872\n",
      "train loss: 6.1380621790885925 - val Loss: 1.8912010788917542\n",
      "train loss: 6.132995933294296 - val Loss: 1.8895968794822693\n",
      "train loss: 6.127953141927719 - val Loss: 1.8880426287651062\n",
      "train loss: 6.1229554414749146 - val Loss: 1.8866139352321625\n",
      "train loss: 6.117980718612671 - val Loss: 1.885146588087082\n",
      "train loss: 6.113004148006439 - val Loss: 1.883626401424408\n",
      "train loss: 6.108048737049103 - val Loss: 1.882042944431305\n",
      "train loss: 6.103116691112518 - val Loss: 1.8804935812950134\n",
      "train loss: 6.09823751449585 - val Loss: 1.8790075480937958\n",
      "train loss: 6.093326568603516 - val Loss: 1.8774245083332062\n",
      "train loss: 6.08840075135231 - val Loss: 1.8759341537952423\n",
      "train loss: 6.083538830280304 - val Loss: 1.8743891417980194\n",
      "train loss: 6.078663527965546 - val Loss: 1.8729062676429749\n",
      "train loss: 6.0738126039505005 - val Loss: 1.8713502883911133\n",
      "train loss: 6.069001853466034 - val Loss: 1.869781732559204\n",
      "train loss: 6.0642459988594055 - val Loss: 1.8682959079742432\n",
      "train loss: 6.05952000617981 - val Loss: 1.866738110780716\n",
      "train loss: 6.0548228323459625 - val Loss: 1.8651686906814575\n",
      "train loss: 6.050110042095184 - val Loss: 1.863686203956604\n",
      "train loss: 6.045449137687683 - val Loss: 1.8620907664299011\n",
      "train loss: 6.040748685598373 - val Loss: 1.8606833219528198\n",
      "train loss: 6.036101281642914 - val Loss: 1.8591701984405518\n",
      "train loss: 6.03153470158577 - val Loss: 1.8576775193214417\n",
      "train loss: 6.02689853310585 - val Loss: 1.856259971857071\n",
      "train loss: 6.022271692752838 - val Loss: 1.8550797998905182\n",
      "train loss: 6.017645388841629 - val Loss: 1.8535739183425903\n",
      "train loss: 6.0130539536476135 - val Loss: 1.8521302938461304\n",
      "train loss: 6.00847515463829 - val Loss: 1.8506694734096527\n",
      "train loss: 6.003883004188538 - val Loss: 1.8494709134101868\n",
      "train loss: 5.999319970607758 - val Loss: 1.848090261220932\n",
      "train loss: 5.9947729408741 - val Loss: 1.846837729215622\n",
      "train loss: 5.990210235118866 - val Loss: 1.8455113768577576\n",
      "train loss: 5.985713005065918 - val Loss: 1.8441350758075714\n",
      "train loss: 5.981303304433823 - val Loss: 1.8428255021572113\n",
      "train loss: 5.9768229722976685 - val Loss: 1.8414709270000458\n",
      "train loss: 5.972420930862427 - val Loss: 1.8401436805725098\n",
      "train loss: 5.968070715665817 - val Loss: 1.8389495611190796\n",
      "train loss: 5.963654309511185 - val Loss: 1.8373834192752838\n",
      "train loss: 5.959269493818283 - val Loss: 1.8359100222587585\n",
      "train loss: 5.9549223482608795 - val Loss: 1.8345636427402496\n",
      "train loss: 5.950530469417572 - val Loss: 1.8330133259296417\n",
      "train loss: 5.946207523345947 - val Loss: 1.8315869569778442\n",
      "train loss: 5.941868543624878 - val Loss: 1.830226868391037\n",
      "train loss: 5.937634170055389 - val Loss: 1.8290618062019348\n",
      "train loss: 5.933315426111221 - val Loss: 1.8276875615119934\n",
      "train loss: 5.929051846265793 - val Loss: 1.8262304067611694\n",
      "train loss: 5.924800306558609 - val Loss: 1.8249309957027435\n",
      "train loss: 5.9205776154994965 - val Loss: 1.82351616024971\n",
      "train loss: 5.916410446166992 - val Loss: 1.822149932384491\n",
      "train loss: 5.912218123674393 - val Loss: 1.8207703828811646\n",
      "train loss: 5.907998442649841 - val Loss: 1.8194350898265839\n",
      "train loss: 5.903890073299408 - val Loss: 1.8180164396762848\n",
      "train loss: 5.899807959794998 - val Loss: 1.8166851699352264\n",
      "train loss: 5.895679384469986 - val Loss: 1.815426081418991\n",
      "train loss: 5.891590058803558 - val Loss: 1.814102590084076\n",
      "train loss: 5.887547880411148 - val Loss: 1.8128116130828857\n",
      "train loss: 5.883409708738327 - val Loss: 1.811301350593567\n",
      "train loss: 5.879337459802628 - val Loss: 1.8099204301834106\n",
      "train loss: 5.875284403562546 - val Loss: 1.808525711297989\n",
      "train loss: 5.871248751878738 - val Loss: 1.8072088360786438\n",
      "train loss: 5.867185264825821 - val Loss: 1.8059073686599731\n",
      "train loss: 5.863204509019852 - val Loss: 1.8046072125434875\n",
      "train loss: 5.859166175127029 - val Loss: 1.803366869688034\n",
      "train loss: 5.855171054601669 - val Loss: 1.8020744919776917\n",
      "train loss: 5.851218819618225 - val Loss: 1.8008528053760529\n",
      "train loss: 5.847281962633133 - val Loss: 1.799503892660141\n",
      "train loss: 5.843349516391754 - val Loss: 1.7983723282814026\n",
      "train loss: 5.839392721652985 - val Loss: 1.7971296608448029\n",
      "train loss: 5.83551487326622 - val Loss: 1.7958447337150574\n",
      "train loss: 5.831606239080429 - val Loss: 1.7946412563323975\n",
      "train loss: 5.827666848897934 - val Loss: 1.7933982014656067\n",
      "train loss: 5.823742210865021 - val Loss: 1.7920271754264832\n",
      "train loss: 5.819869697093964 - val Loss: 1.7908945381641388\n",
      "train loss: 5.8160096406936646 - val Loss: 1.7896499931812286\n",
      "train loss: 5.812143176794052 - val Loss: 1.7885383069515228\n",
      "train loss: 5.80831453204155 - val Loss: 1.787370502948761\n",
      "train loss: 5.8044489324092865 - val Loss: 1.7861620485782623\n",
      "train loss: 5.800657391548157 - val Loss: 1.7849749326705933\n",
      "train loss: 5.796807408332825 - val Loss: 1.7838331162929535\n",
      "train loss: 5.793113023042679 - val Loss: 1.7827995419502258\n",
      "train loss: 5.789320707321167 - val Loss: 1.781500369310379\n",
      "train loss: 5.785554051399231 - val Loss: 1.7801062762737274\n",
      "train loss: 5.7818379402160645 - val Loss: 1.7788400948047638\n",
      "train loss: 5.778086334466934 - val Loss: 1.777489423751831\n",
      "train loss: 5.774305939674377 - val Loss: 1.7763429284095764\n",
      "train loss: 5.770559370517731 - val Loss: 1.775214821100235\n",
      "train loss: 5.7668052315711975 - val Loss: 1.7740588188171387\n",
      "train loss: 5.76313254237175 - val Loss: 1.7729021310806274\n",
      "train loss: 5.759512633085251 - val Loss: 1.771756798028946\n",
      "train loss: 5.755855351686478 - val Loss: 1.7706084549427032\n",
      "train loss: 5.752214014530182 - val Loss: 1.769507259130478\n",
      "train loss: 5.748546659946442 - val Loss: 1.768332064151764\n",
      "train loss: 5.744896441698074 - val Loss: 1.7671214640140533\n",
      "train loss: 5.7412630915641785 - val Loss: 1.7660100162029266\n",
      "train loss: 5.737687110900879 - val Loss: 1.764960378408432\n",
      "train loss: 5.7341345846652985 - val Loss: 1.7639105916023254\n",
      "train loss: 5.730536133050919 - val Loss: 1.762883484363556\n",
      "train loss: 5.726965993642807 - val Loss: 1.7617912888526917\n",
      "train loss: 5.723414599895477 - val Loss: 1.7607135474681854\n",
      "train loss: 5.719899445772171 - val Loss: 1.7596137821674347\n",
      "train loss: 5.7163874208927155 - val Loss: 1.7584855258464813\n",
      "train loss: 5.712896227836609 - val Loss: 1.7573745846748352\n",
      "train loss: 5.709354192018509 - val Loss: 1.7562769055366516\n",
      "train loss: 5.705923020839691 - val Loss: 1.755186527967453\n",
      "train loss: 5.70245173573494 - val Loss: 1.7541317641735077\n",
      "train loss: 5.699026852846146 - val Loss: 1.7530975341796875\n",
      "train loss: 5.69561505317688 - val Loss: 1.7519367933273315\n",
      "train loss: 5.692236572504044 - val Loss: 1.7508236169815063\n",
      "train loss: 5.688856929540634 - val Loss: 1.7497850060462952\n",
      "train loss: 5.685468256473541 - val Loss: 1.7487258613109589\n",
      "train loss: 5.682064950466156 - val Loss: 1.7476447522640228\n",
      "train loss: 5.6786304116249084 - val Loss: 1.7466281950473785\n",
      "train loss: 5.675223618745804 - val Loss: 1.7456194460391998\n",
      "train loss: 5.671789139509201 - val Loss: 1.7445692121982574\n",
      "train loss: 5.668459236621857 - val Loss: 1.7434534132480621\n",
      "train loss: 5.6650736927986145 - val Loss: 1.7424097955226898\n",
      "train loss: 5.661747694015503 - val Loss: 1.741041660308838\n",
      "train loss: 5.65841007232666 - val Loss: 1.7399544417858124\n",
      "train loss: 5.655109107494354 - val Loss: 1.738924264907837\n",
      "train loss: 5.651819169521332 - val Loss: 1.7378864586353302\n",
      "train loss: 5.648561537265778 - val Loss: 1.7367663979530334\n",
      "train loss: 5.645309865474701 - val Loss: 1.7357092797756195\n",
      "train loss: 5.642064452171326 - val Loss: 1.7347563207149506\n",
      "train loss: 5.638878613710403 - val Loss: 1.7337350845336914\n",
      "train loss: 5.635668605566025 - val Loss: 1.7326859533786774\n",
      "train loss: 5.6324233412742615 - val Loss: 1.7316830158233643\n",
      "train loss: 5.629237055778503 - val Loss: 1.7306554913520813\n",
      "train loss: 5.626075953245163 - val Loss: 1.7295650243759155\n",
      "train loss: 5.62291744351387 - val Loss: 1.728559285402298\n",
      "train loss: 5.619765818119049 - val Loss: 1.7275701761245728\n",
      "train loss: 5.616644412279129 - val Loss: 1.7266505360603333\n",
      "train loss: 5.613510638475418 - val Loss: 1.725691556930542\n",
      "train loss: 5.610385924577713 - val Loss: 1.7247190177440643\n",
      "train loss: 5.60730254650116 - val Loss: 1.7237312197685242\n",
      "train loss: 5.604141652584076 - val Loss: 1.7226036190986633\n",
      "train loss: 5.601035624742508 - val Loss: 1.7215169966220856\n",
      "train loss: 5.5979799926280975 - val Loss: 1.7204478979110718\n",
      "train loss: 5.594904839992523 - val Loss: 1.7193767428398132\n",
      "train loss: 5.591787099838257 - val Loss: 1.7183595299720764\n",
      "train loss: 5.588743984699249 - val Loss: 1.7173053622245789\n",
      "train loss: 5.5856920182704926 - val Loss: 1.7162436544895172\n",
      "train loss: 5.58263772726059 - val Loss: 1.7152569591999054\n",
      "train loss: 5.579655766487122 - val Loss: 1.714383065700531\n",
      "train loss: 5.576612681150436 - val Loss: 1.713427573442459\n",
      "train loss: 5.5736390352249146 - val Loss: 1.7124410271644592\n",
      "train loss: 5.570613384246826 - val Loss: 1.7114135921001434\n",
      "train loss: 5.567601412534714 - val Loss: 1.7104151546955109\n",
      "train loss: 5.56465819478035 - val Loss: 1.7094526290893555\n",
      "train loss: 5.561704516410828 - val Loss: 1.7085278928279877\n",
      "train loss: 5.558702260255814 - val Loss: 1.7075720131397247\n",
      "train loss: 5.555792063474655 - val Loss: 1.706653356552124\n",
      "train loss: 5.5528484582901 - val Loss: 1.7062047123908997\n",
      "train loss: 5.549832701683044 - val Loss: 1.7052068412303925\n",
      "train loss: 5.5468882620334625 - val Loss: 1.7040704488754272\n",
      "train loss: 5.5439421236515045 - val Loss: 1.7030882835388184\n",
      "train loss: 5.541029214859009 - val Loss: 1.7021221220493317\n",
      "train loss: 5.538119047880173 - val Loss: 1.701173722743988\n",
      "train loss: 5.535220593214035 - val Loss: 1.7002900838851929\n",
      "train loss: 5.5323158502578735 - val Loss: 1.699321299791336\n",
      "train loss: 5.529426872730255 - val Loss: 1.6984711289405823\n",
      "train loss: 5.5265394151210785 - val Loss: 1.6975761353969574\n",
      "train loss: 5.523696690797806 - val Loss: 1.6966670453548431\n",
      "train loss: 5.520849019289017 - val Loss: 1.6957561373710632\n",
      "train loss: 5.518007308244705 - val Loss: 1.6949549317359924\n",
      "train loss: 5.515136301517487 - val Loss: 1.6939685642719269\n",
      "train loss: 5.512259095907211 - val Loss: 1.6929334998130798\n",
      "train loss: 5.509422659873962 - val Loss: 1.6919198334217072\n",
      "train loss: 5.506542444229126 - val Loss: 1.6910829842090607\n",
      "train loss: 5.503692388534546 - val Loss: 1.6900935769081116\n",
      "train loss: 5.500886142253876 - val Loss: 1.6892109811306\n",
      "train loss: 5.498039662837982 - val Loss: 1.6883389353752136\n",
      "train loss: 5.495183527469635 - val Loss: 1.6872705817222595\n",
      "train loss: 5.492407143115997 - val Loss: 1.6862936913967133\n",
      "train loss: 5.4895491898059845 - val Loss: 1.685474455356598\n",
      "train loss: 5.486748397350311 - val Loss: 1.6845387518405914\n",
      "train loss: 5.48389145731926 - val Loss: 1.6836217045783997\n",
      "train loss: 5.481045871973038 - val Loss: 1.6827328503131866\n",
      "train loss: 5.478167533874512 - val Loss: 1.6818699836730957\n",
      "train loss: 5.475382953882217 - val Loss: 1.681013435125351\n",
      "train loss: 5.472529083490372 - val Loss: 1.6802650392055511\n",
      "train loss: 5.469735026359558 - val Loss: 1.67930468916893\n",
      "train loss: 5.466945022344589 - val Loss: 1.6784177422523499\n",
      "train loss: 5.464209675788879 - val Loss: 1.6775000989437103\n",
      "train loss: 5.461429625749588 - val Loss: 1.676696389913559\n",
      "train loss: 5.458655893802643 - val Loss: 1.6758542358875275\n",
      "train loss: 5.4559215903282166 - val Loss: 1.675030767917633\n",
      "train loss: 5.4531824588775635 - val Loss: 1.6741995513439178\n",
      "train loss: 5.450455516576767 - val Loss: 1.6733627021312714\n",
      "train loss: 5.44774603843689 - val Loss: 1.6725296676158905\n",
      "train loss: 5.44506561756134 - val Loss: 1.6716643571853638\n",
      "train loss: 5.442411154508591 - val Loss: 1.6708357334136963\n",
      "train loss: 5.439703404903412 - val Loss: 1.670063465833664\n",
      "train loss: 5.437067151069641 - val Loss: 1.6692787408828735\n",
      "train loss: 5.434433430433273 - val Loss: 1.6684991121292114\n",
      "train loss: 5.431799054145813 - val Loss: 1.6677722334861755\n",
      "train loss: 5.4291286170482635 - val Loss: 1.666962206363678\n",
      "train loss: 5.426520049571991 - val Loss: 1.6660358607769012\n",
      "train loss: 5.423894852399826 - val Loss: 1.6653371155261993\n",
      "train loss: 5.421239048242569 - val Loss: 1.6642980873584747\n",
      "train loss: 5.4186274111270905 - val Loss: 1.6635636985301971\n",
      "train loss: 5.416049778461456 - val Loss: 1.662748247385025\n",
      "train loss: 5.413459837436676 - val Loss: 1.6619738638401031\n",
      "train loss: 5.410872101783752 - val Loss: 1.661005824804306\n",
      "train loss: 5.408300697803497 - val Loss: 1.6601772904396057\n",
      "train loss: 5.405726850032806 - val Loss: 1.659383863210678\n",
      "train loss: 5.403198301792145 - val Loss: 1.6587353348731995\n",
      "train loss: 5.400664299726486 - val Loss: 1.6579578518867493\n",
      "train loss: 5.398155897855759 - val Loss: 1.6571733355522156\n",
      "train loss: 5.395640134811401 - val Loss: 1.6564298570156097\n",
      "train loss: 5.3930849730968475 - val Loss: 1.6556638181209564\n",
      "train loss: 5.39056459069252 - val Loss: 1.6548644304275513\n",
      "train loss: 5.3881078362464905 - val Loss: 1.6540921926498413\n",
      "train loss: 5.385632932186127 - val Loss: 1.653384029865265\n",
      "train loss: 5.383140981197357 - val Loss: 1.6526302993297577\n",
      "train loss: 5.380648076534271 - val Loss: 1.651752084493637\n",
      "train loss: 5.378303647041321 - val Loss: 1.6510547697544098\n",
      "train loss: 5.375863879919052 - val Loss: 1.6502346992492676\n",
      "train loss: 5.373465865850449 - val Loss: 1.6496549546718597\n",
      "train loss: 5.371069014072418 - val Loss: 1.6489518582820892\n",
      "train loss: 5.368652790784836 - val Loss: 1.6481671631336212\n",
      "train loss: 5.366247206926346 - val Loss: 1.6474492847919464\n",
      "train loss: 5.363867402076721 - val Loss: 1.646718442440033\n",
      "train loss: 5.361500829458237 - val Loss: 1.6460030376911163\n",
      "train loss: 5.359173268079758 - val Loss: 1.6452257931232452\n",
      "train loss: 5.356809884309769 - val Loss: 1.6444623172283173\n",
      "train loss: 5.35444912314415 - val Loss: 1.6437269151210785\n",
      "train loss: 5.352180451154709 - val Loss: 1.6429956555366516\n",
      "train loss: 5.349847167730331 - val Loss: 1.6421939432621002\n",
      "train loss: 5.347494751214981 - val Loss: 1.6414330899715424\n",
      "train loss: 5.34515979886055 - val Loss: 1.6409375071525574\n",
      "train loss: 5.342780888080597 - val Loss: 1.6402309238910675\n",
      "train loss: 5.340459197759628 - val Loss: 1.6394785642623901\n",
      "train loss: 5.338142454624176 - val Loss: 1.6387308835983276\n",
      "train loss: 5.335858702659607 - val Loss: 1.6379714608192444\n",
      "train loss: 5.333612740039825 - val Loss: 1.6371357440948486\n",
      "train loss: 5.331284880638123 - val Loss: 1.636485368013382\n",
      "train loss: 5.328989386558533 - val Loss: 1.6356251537799835\n",
      "train loss: 5.326717555522919 - val Loss: 1.6348850429058075\n",
      "train loss: 5.324435591697693 - val Loss: 1.6341451406478882\n",
      "train loss: 5.322155475616455 - val Loss: 1.633388489484787\n",
      "train loss: 5.319852203130722 - val Loss: 1.6330212950706482\n",
      "train loss: 5.317605942487717 - val Loss: 1.6323004066944122\n",
      "train loss: 5.315301090478897 - val Loss: 1.6313940286636353\n",
      "train loss: 5.313062757253647 - val Loss: 1.6306977570056915\n",
      "train loss: 5.3108115792274475 - val Loss: 1.6300344169139862\n",
      "train loss: 5.30857041478157 - val Loss: 1.629388839006424\n",
      "train loss: 5.306326180696487 - val Loss: 1.6287316679954529\n",
      "train loss: 5.304119497537613 - val Loss: 1.6281654238700867\n",
      "train loss: 5.301933586597443 - val Loss: 1.6276856362819672\n",
      "train loss: 5.299728602170944 - val Loss: 1.627188801765442\n",
      "train loss: 5.297483891248703 - val Loss: 1.626403123140335\n",
      "train loss: 5.295290768146515 - val Loss: 1.6259680092334747\n",
      "train loss: 5.293096661567688 - val Loss: 1.625301569700241\n",
      "train loss: 5.290984570980072 - val Loss: 1.6246351301670074\n",
      "train loss: 5.288828045129776 - val Loss: 1.624046415090561\n",
      "train loss: 5.286616325378418 - val Loss: 1.6233490109443665\n",
      "train loss: 5.284497916698456 - val Loss: 1.6227361857891083\n",
      "train loss: 5.282316356897354 - val Loss: 1.6220802962779999\n",
      "train loss: 5.280186027288437 - val Loss: 1.6214726865291595\n",
      "train loss: 5.2780482172966 - val Loss: 1.6208753883838654\n",
      "train loss: 5.275872379541397 - val Loss: 1.6202056109905243\n",
      "train loss: 5.273740142583847 - val Loss: 1.6196443438529968\n",
      "train loss: 5.271724075078964 - val Loss: 1.6190565824508667\n",
      "train loss: 5.269561171531677 - val Loss: 1.6183322072029114\n",
      "train loss: 5.26745069026947 - val Loss: 1.6177379488945007\n",
      "train loss: 5.2653248608112335 - val Loss: 1.6169506311416626\n",
      "train loss: 5.263227075338364 - val Loss: 1.6164056956768036\n",
      "train loss: 5.26111713051796 - val Loss: 1.6157743632793427\n",
      "train loss: 5.259022235870361 - val Loss: 1.6151576936244965\n",
      "train loss: 5.25694665312767 - val Loss: 1.614544540643692\n",
      "train loss: 5.2548549473285675 - val Loss: 1.6137829422950745\n",
      "train loss: 5.2527733743190765 - val Loss: 1.6130622923374176\n",
      "train loss: 5.250670999288559 - val Loss: 1.6124016344547272\n",
      "train loss: 5.248590022325516 - val Loss: 1.611617386341095\n",
      "train loss: 5.246518611907959 - val Loss: 1.6109232902526855\n",
      "train loss: 5.244441658258438 - val Loss: 1.610141783952713\n",
      "train loss: 5.242292612791061 - val Loss: 1.609432339668274\n",
      "train loss: 5.240192919969559 - val Loss: 1.6088516116142273\n",
      "train loss: 5.238045036792755 - val Loss: 1.6082406342029572\n",
      "train loss: 5.235963076353073 - val Loss: 1.6075960993766785\n",
      "train loss: 5.233921557664871 - val Loss: 1.6069528758525848\n",
      "train loss: 5.231807619333267 - val Loss: 1.6063014268875122\n",
      "train loss: 5.229666978120804 - val Loss: 1.6056687831878662\n",
      "train loss: 5.227653503417969 - val Loss: 1.6050492227077484\n",
      "train loss: 5.225602746009827 - val Loss: 1.6044481098651886\n",
      "train loss: 5.223469257354736 - val Loss: 1.6038178205490112\n",
      "train loss: 5.221447825431824 - val Loss: 1.6031691431999207\n",
      "train loss: 5.21942213177681 - val Loss: 1.6024605333805084\n",
      "train loss: 5.21735942363739 - val Loss: 1.6016782522201538\n",
      "train loss: 5.215332478284836 - val Loss: 1.6011863052845001\n",
      "train loss: 5.213260680437088 - val Loss: 1.6005004346370697\n",
      "train loss: 5.2111636698246 - val Loss: 1.5998414158821106\n",
      "train loss: 5.2091600596904755 - val Loss: 1.5992065966129303\n",
      "train loss: 5.207138806581497 - val Loss: 1.5986110866069794\n",
      "train loss: 5.2051345109939575 - val Loss: 1.5981123745441437\n",
      "train loss: 5.203098475933075 - val Loss: 1.5974572896957397\n",
      "train loss: 5.201098263263702 - val Loss: 1.596871256828308\n",
      "train loss: 5.199140846729279 - val Loss: 1.5963333249092102\n",
      "train loss: 5.197153180837631 - val Loss: 1.5956871509552002\n",
      "train loss: 5.195157885551453 - val Loss: 1.595060557126999\n",
      "train loss: 5.193177133798599 - val Loss: 1.5944626033306122\n",
      "train loss: 5.191222101449966 - val Loss: 1.5938639044761658\n",
      "train loss: 5.189171612262726 - val Loss: 1.593307375907898\n",
      "train loss: 5.187204271554947 - val Loss: 1.5925308465957642\n",
      "train loss: 5.1851886212825775 - val Loss: 1.591810703277588\n",
      "train loss: 5.183202922344208 - val Loss: 1.5911668241024017\n",
      "train loss: 5.181252658367157 - val Loss: 1.5903973281383514\n",
      "train loss: 5.179307222366333 - val Loss: 1.5897396802902222\n",
      "train loss: 5.177327185869217 - val Loss: 1.5891067385673523\n",
      "train loss: 5.17532417178154 - val Loss: 1.5885246396064758\n",
      "train loss: 5.173322319984436 - val Loss: 1.5877854526042938\n",
      "train loss: 5.171344429254532 - val Loss: 1.5872162878513336\n",
      "train loss: 5.169328927993774 - val Loss: 1.5865365862846375\n",
      "train loss: 5.167366445064545 - val Loss: 1.5857839584350586\n",
      "train loss: 5.165400952100754 - val Loss: 1.5851468443870544\n",
      "train loss: 5.163393497467041 - val Loss: 1.5845061242580414\n",
      "train loss: 5.161459118127823 - val Loss: 1.5838567614555359\n",
      "train loss: 5.159471392631531 - val Loss: 1.5832932591438293\n",
      "train loss: 5.1575475335121155 - val Loss: 1.5827151834964752\n",
      "train loss: 5.155566543340683 - val Loss: 1.58215993642807\n",
      "train loss: 5.153620392084122 - val Loss: 1.5815946757793427\n",
      "train loss: 5.151756197214127 - val Loss: 1.5809905230998993\n",
      "train loss: 5.149829596281052 - val Loss: 1.5804099440574646\n",
      "train loss: 5.147941559553146 - val Loss: 1.5798077285289764\n",
      "train loss: 5.146039694547653 - val Loss: 1.5791689157485962\n",
      "train loss: 5.1441231071949005 - val Loss: 1.5785636901855469\n",
      "train loss: 5.142266184091568 - val Loss: 1.5780040621757507\n",
      "train loss: 5.14040219783783 - val Loss: 1.5774167776107788\n",
      "train loss: 5.1386111080646515 - val Loss: 1.576736032962799\n",
      "train loss: 5.136773258447647 - val Loss: 1.5761743485927582\n",
      "train loss: 5.134919822216034 - val Loss: 1.5756593346595764\n",
      "train loss: 5.133024841547012 - val Loss: 1.5751546323299408\n",
      "train loss: 5.131215482950211 - val Loss: 1.574627935886383\n",
      "train loss: 5.129386335611343 - val Loss: 1.574127346277237\n",
      "train loss: 5.127582550048828 - val Loss: 1.5736006498336792\n",
      "train loss: 5.125779330730438 - val Loss: 1.573021560907364\n",
      "train loss: 5.123988747596741 - val Loss: 1.5724321007728577\n",
      "train loss: 5.122138530015945 - val Loss: 1.571878582239151\n",
      "train loss: 5.120322614908218 - val Loss: 1.571370780467987\n",
      "train loss: 5.118547469377518 - val Loss: 1.5707356631755829\n",
      "train loss: 5.1167338490486145 - val Loss: 1.5702470541000366\n",
      "train loss: 5.114908427000046 - val Loss: 1.5697836875915527\n",
      "train loss: 5.113109767436981 - val Loss: 1.5694178342819214\n",
      "train loss: 5.111330062150955 - val Loss: 1.568860650062561\n",
      "train loss: 5.109528660774231 - val Loss: 1.5683155059814453\n",
      "train loss: 5.107726603746414 - val Loss: 1.567721962928772\n",
      "train loss: 5.105939835309982 - val Loss: 1.5671944320201874\n",
      "train loss: 5.10413458943367 - val Loss: 1.5666594803333282\n",
      "train loss: 5.102356731891632 - val Loss: 1.5661273002624512\n",
      "train loss: 5.100522875785828 - val Loss: 1.565605342388153\n",
      "train loss: 5.098760962486267 - val Loss: 1.5651487112045288\n",
      "train loss: 5.096998810768127 - val Loss: 1.5646461844444275\n",
      "train loss: 5.095234006643295 - val Loss: 1.5641210675239563\n",
      "train loss: 5.093458890914917 - val Loss: 1.5635794699192047\n",
      "train loss: 5.0916769206523895 - val Loss: 1.5628909170627594\n",
      "train loss: 5.089928328990936 - val Loss: 1.5623258352279663\n",
      "train loss: 5.088144361972809 - val Loss: 1.5616921484470367\n",
      "train loss: 5.086343884468079 - val Loss: 1.5611035823822021\n",
      "train loss: 5.084599614143372 - val Loss: 1.560557782649994\n",
      "train loss: 5.082874536514282 - val Loss: 1.5600351989269257\n",
      "train loss: 5.0811329782009125 - val Loss: 1.5595169365406036\n",
      "train loss: 5.079397618770599 - val Loss: 1.5589688420295715\n",
      "train loss: 5.077662706375122 - val Loss: 1.5584552884101868\n",
      "train loss: 5.075942128896713 - val Loss: 1.5579403638839722\n",
      "train loss: 5.0742440819740295 - val Loss: 1.5573563277721405\n",
      "train loss: 5.072546601295471 - val Loss: 1.556822806596756\n",
      "train loss: 5.070848226547241 - val Loss: 1.5561170876026154\n",
      "train loss: 5.069163411855698 - val Loss: 1.5555432438850403\n",
      "train loss: 5.067455768585205 - val Loss: 1.5549698770046234\n",
      "train loss: 5.065705209970474 - val Loss: 1.554353505373001\n",
      "train loss: 5.064045071601868 - val Loss: 1.5538650751113892\n",
      "train loss: 5.062349796295166 - val Loss: 1.5533040761947632\n",
      "train loss: 5.0605897307395935 - val Loss: 1.5527595579624176\n",
      "train loss: 5.058899462223053 - val Loss: 1.5522500872612\n",
      "train loss: 5.057196199893951 - val Loss: 1.5517192482948303\n",
      "train loss: 5.05557182431221 - val Loss: 1.5511881113052368\n",
      "train loss: 5.053892731666565 - val Loss: 1.5506640672683716\n",
      "train loss: 5.052238464355469 - val Loss: 1.5501908659934998\n",
      "train loss: 5.050608813762665 - val Loss: 1.5497432351112366\n",
      "train loss: 5.048928797245026 - val Loss: 1.5492252707481384\n",
      "train loss: 5.047241449356079 - val Loss: 1.5487427413463593\n",
      "train loss: 5.04557865858078 - val Loss: 1.5482051372528076\n",
      "train loss: 5.043881058692932 - val Loss: 1.5477176308631897\n",
      "train loss: 5.042239993810654 - val Loss: 1.5473534762859344\n",
      "train loss: 5.040579468011856 - val Loss: 1.5468473732471466\n",
      "train loss: 5.038911581039429 - val Loss: 1.5463241040706635\n",
      "train loss: 5.03726664185524 - val Loss: 1.5457592010498047\n",
      "train loss: 5.035611867904663 - val Loss: 1.5452655851840973\n",
      "train loss: 5.033962070941925 - val Loss: 1.5447422862052917\n",
      "train loss: 5.032321959733963 - val Loss: 1.5442273020744324\n",
      "train loss: 5.030700176954269 - val Loss: 1.5436786711215973\n",
      "train loss: 5.029074132442474 - val Loss: 1.5431039035320282\n",
      "train loss: 5.027545362710953 - val Loss: 1.542541265487671\n",
      "train loss: 5.025884598493576 - val Loss: 1.5420338809490204\n",
      "train loss: 5.024277210235596 - val Loss: 1.5414641797542572\n",
      "train loss: 5.022675484418869 - val Loss: 1.5409068167209625\n",
      "train loss: 5.021068096160889 - val Loss: 1.5403397679328918\n",
      "train loss: 5.019464612007141 - val Loss: 1.539770096540451\n",
      "train loss: 5.017841666936874 - val Loss: 1.5392534732818604\n",
      "train loss: 5.016205966472626 - val Loss: 1.5387223660945892\n",
      "train loss: 5.01464718580246 - val Loss: 1.5382266640663147\n",
      "train loss: 5.013011604547501 - val Loss: 1.5376721024513245\n",
      "train loss: 5.011398166418076 - val Loss: 1.5371264815330505\n",
      "train loss: 5.009788751602173 - val Loss: 1.536491185426712\n",
      "train loss: 5.008167505264282 - val Loss: 1.5359570682048798\n",
      "train loss: 5.006590634584427 - val Loss: 1.5354462265968323\n",
      "train loss: 5.005011826753616 - val Loss: 1.5350050032138824\n",
      "train loss: 5.003411680459976 - val Loss: 1.534476786851883\n",
      "train loss: 5.001823753118515 - val Loss: 1.533911645412445\n",
      "train loss: 5.000224262475967 - val Loss: 1.533337652683258\n",
      "train loss: 4.9986512660980225 - val Loss: 1.5328292846679688\n",
      "train loss: 4.997041642665863 - val Loss: 1.5324888229370117\n",
      "train loss: 4.995482087135315 - val Loss: 1.5320442020893097\n",
      "train loss: 4.993900716304779 - val Loss: 1.531587690114975\n",
      "train loss: 4.992335081100464 - val Loss: 1.531112402677536\n",
      "train loss: 4.990759253501892 - val Loss: 1.5305981934070587\n",
      "train loss: 4.989187657833099 - val Loss: 1.530100017786026\n",
      "train loss: 4.987627565860748 - val Loss: 1.5296080112457275\n",
      "train loss: 4.986038506031036 - val Loss: 1.5291276574134827\n",
      "train loss: 4.984428912401199 - val Loss: 1.5286018252372742\n",
      "train loss: 4.982906490564346 - val Loss: 1.5280413329601288\n",
      "train loss: 4.981355935335159 - val Loss: 1.5275598168373108\n",
      "train loss: 4.9798040091991425 - val Loss: 1.5270781219005585\n",
      "train loss: 4.978239059448242 - val Loss: 1.5266368985176086\n",
      "train loss: 4.976683676242828 - val Loss: 1.526108741760254\n",
      "train loss: 4.975109428167343 - val Loss: 1.5255920886993408\n",
      "train loss: 4.973564863204956 - val Loss: 1.5250967144966125\n",
      "train loss: 4.972009241580963 - val Loss: 1.524601548910141\n",
      "train loss: 4.9704508781433105 - val Loss: 1.5241644978523254\n",
      "train loss: 4.968865752220154 - val Loss: 1.523771345615387\n",
      "train loss: 4.967312425374985 - val Loss: 1.5232813954353333\n",
      "train loss: 4.9657644629478455 - val Loss: 1.5227804481983185\n",
      "train loss: 4.964233577251434 - val Loss: 1.5222564041614532\n",
      "train loss: 4.962701112031937 - val Loss: 1.5218401551246643\n",
      "train loss: 4.96117240190506 - val Loss: 1.5213486552238464\n",
      "train loss: 4.959660768508911 - val Loss: 1.520863026380539\n",
      "train loss: 4.958108156919479 - val Loss: 1.5203127264976501\n",
      "train loss: 4.956565231084824 - val Loss: 1.5197806060314178\n",
      "train loss: 4.9550619423389435 - val Loss: 1.5192395448684692\n",
      "train loss: 4.9535022377967834 - val Loss: 1.5188228785991669\n",
      "train loss: 4.952007740736008 - val Loss: 1.5183491706848145\n",
      "train loss: 4.950480967760086 - val Loss: 1.5178233087062836\n",
      "train loss: 4.948957055807114 - val Loss: 1.5173794329166412\n",
      "train loss: 4.947384268045425 - val Loss: 1.5168646275997162\n",
      "train loss: 4.945823043584824 - val Loss: 1.5162735879421234\n",
      "train loss: 4.944302797317505 - val Loss: 1.5156604945659637\n",
      "train loss: 4.942799210548401 - val Loss: 1.5151137113571167\n",
      "train loss: 4.941295951604843 - val Loss: 1.5145525634288788\n",
      "train loss: 4.939787328243256 - val Loss: 1.5138046145439148\n",
      "train loss: 4.938284069299698 - val Loss: 1.5133349299430847\n",
      "train loss: 4.936828821897507 - val Loss: 1.5128262042999268\n",
      "train loss: 4.935329377651215 - val Loss: 1.5122445225715637\n",
      "train loss: 4.933853834867477 - val Loss: 1.511725276708603\n",
      "train loss: 4.932379573583603 - val Loss: 1.5112012922763824\n",
      "train loss: 4.930910438299179 - val Loss: 1.5107506811618805\n",
      "train loss: 4.929476767778397 - val Loss: 1.5102809965610504\n",
      "train loss: 4.927957475185394 - val Loss: 1.5097786486148834\n",
      "train loss: 4.926470190286636 - val Loss: 1.5093328356742859\n",
      "train loss: 4.925017178058624 - val Loss: 1.5087769031524658\n",
      "train loss: 4.923558592796326 - val Loss: 1.508398860692978\n",
      "train loss: 4.922092318534851 - val Loss: 1.508030652999878\n",
      "train loss: 4.920648962259293 - val Loss: 1.507557213306427\n",
      "train loss: 4.919214606285095 - val Loss: 1.5071738064289093\n",
      "train loss: 4.917698413133621 - val Loss: 1.506697028875351\n",
      "train loss: 4.916158139705658 - val Loss: 1.5061819553375244\n",
      "train loss: 4.914678394794464 - val Loss: 1.5056863129138947\n",
      "train loss: 4.913090139627457 - val Loss: 1.50520458817482\n",
      "train loss: 4.911598205566406 - val Loss: 1.5047141313552856\n",
      "train loss: 4.9101027846336365 - val Loss: 1.5042364001274109\n",
      "train loss: 4.908647447824478 - val Loss: 1.503720909357071\n",
      "train loss: 4.907199651002884 - val Loss: 1.5032186806201935\n",
      "train loss: 4.905749529600143 - val Loss: 1.5027552545070648\n",
      "train loss: 4.904264599084854 - val Loss: 1.5023875534534454\n",
      "train loss: 4.902829468250275 - val Loss: 1.5019490718841553\n",
      "train loss: 4.901329159736633 - val Loss: 1.50149467587471\n",
      "train loss: 4.899896860122681 - val Loss: 1.5009960234165192\n",
      "train loss: 4.898456871509552 - val Loss: 1.500433623790741\n",
      "train loss: 4.897033125162125 - val Loss: 1.4999540746212006\n",
      "train loss: 4.895570814609528 - val Loss: 1.499528706073761\n",
      "train loss: 4.894152671098709 - val Loss: 1.4989734888076782\n",
      "train loss: 4.892665475606918 - val Loss: 1.498570293188095\n",
      "train loss: 4.8912292420864105 - val Loss: 1.498087763786316\n",
      "train loss: 4.889789402484894 - val Loss: 1.4976133704185486\n",
      "train loss: 4.888376235961914 - val Loss: 1.49709153175354\n",
      "train loss: 4.8868937492370605 - val Loss: 1.496673196554184\n",
      "train loss: 4.885434210300446 - val Loss: 1.4961994290351868\n",
      "train loss: 4.884011268615723 - val Loss: 1.4957258999347687\n",
      "train loss: 4.88251331448555 - val Loss: 1.4952388107776642\n",
      "train loss: 4.881054103374481 - val Loss: 1.494766354560852\n",
      "train loss: 4.879597306251526 - val Loss: 1.4943170547485352\n",
      "train loss: 4.878152072429657 - val Loss: 1.493840366601944\n",
      "train loss: 4.8767116367816925 - val Loss: 1.4933292269706726\n",
      "train loss: 4.875245630741119 - val Loss: 1.4928297698497772\n",
      "train loss: 4.873823523521423 - val Loss: 1.4923256039619446\n",
      "train loss: 4.872348487377167 - val Loss: 1.4917906522750854\n",
      "train loss: 4.870895624160767 - val Loss: 1.49125936627388\n",
      "train loss: 4.869444102048874 - val Loss: 1.4907684624195099\n",
      "train loss: 4.868025332689285 - val Loss: 1.4903148710727692\n",
      "train loss: 4.8666059374809265 - val Loss: 1.489799439907074\n",
      "train loss: 4.865180879831314 - val Loss: 1.489433765411377\n",
      "train loss: 4.863761842250824 - val Loss: 1.4889605641365051\n",
      "train loss: 4.8623214066028595 - val Loss: 1.4883893430233002\n",
      "train loss: 4.860893726348877 - val Loss: 1.4877939820289612\n",
      "train loss: 4.859458923339844 - val Loss: 1.4872865676879883\n",
      "train loss: 4.858007609844208 - val Loss: 1.4867979884147644\n",
      "train loss: 4.856591671705246 - val Loss: 1.4863972961902618\n",
      "train loss: 4.855144798755646 - val Loss: 1.4859257936477661\n",
      "train loss: 4.853718012571335 - val Loss: 1.4854689240455627\n",
      "train loss: 4.852267742156982 - val Loss: 1.4849923253059387\n",
      "train loss: 4.850833296775818 - val Loss: 1.4844982028007507\n",
      "train loss: 4.849397271871567 - val Loss: 1.4840292632579803\n",
      "train loss: 4.847966969013214 - val Loss: 1.4834941625595093\n",
      "train loss: 4.846554785966873 - val Loss: 1.482991248369217\n",
      "train loss: 4.845143675804138 - val Loss: 1.482516586780548\n",
      "train loss: 4.843762516975403 - val Loss: 1.4820315837860107\n",
      "train loss: 4.842379570007324 - val Loss: 1.4816317558288574\n",
      "train loss: 4.8409793972969055 - val Loss: 1.4811060726642609\n",
      "train loss: 4.839618355035782 - val Loss: 1.4806189835071564\n",
      "train loss: 4.838259428739548 - val Loss: 1.4801096022129059\n",
      "train loss: 4.836878210306168 - val Loss: 1.4796878695487976\n",
      "train loss: 4.835553973913193 - val Loss: 1.479165494441986\n",
      "train loss: 4.834173291921616 - val Loss: 1.478665977716446\n",
      "train loss: 4.832831233739853 - val Loss: 1.4781882166862488\n",
      "train loss: 4.83149591088295 - val Loss: 1.4777230620384216\n",
      "train loss: 4.830135077238083 - val Loss: 1.4772512316703796\n",
      "train loss: 4.828782945871353 - val Loss: 1.4767565429210663\n",
      "train loss: 4.827465057373047 - val Loss: 1.4763303101062775\n",
      "train loss: 4.826100826263428 - val Loss: 1.476001799106598\n",
      "train loss: 4.82477006316185 - val Loss: 1.47556734085083\n",
      "train loss: 4.823452144861221 - val Loss: 1.4751476049423218\n",
      "train loss: 4.822119176387787 - val Loss: 1.4746978282928467\n",
      "train loss: 4.820790499448776 - val Loss: 1.4742365181446075\n",
      "train loss: 4.81944414973259 - val Loss: 1.4737842082977295\n",
      "train loss: 4.818140298128128 - val Loss: 1.473364531993866\n",
      "train loss: 4.816770315170288 - val Loss: 1.472941666841507\n",
      "train loss: 4.815411746501923 - val Loss: 1.4724942743778229\n",
      "train loss: 4.8141089379787445 - val Loss: 1.4716988503932953\n",
      "train loss: 4.812648266553879 - val Loss: 1.471319317817688\n",
      "train loss: 4.811297595500946 - val Loss: 1.4709137082099915\n",
      "train loss: 4.809975743293762 - val Loss: 1.4705206453800201\n",
      "train loss: 4.80864754319191 - val Loss: 1.4701050221920013\n",
      "train loss: 4.807318359613419 - val Loss: 1.469662219285965\n",
      "train loss: 4.805963426828384 - val Loss: 1.4691691994667053\n",
      "train loss: 4.804643213748932 - val Loss: 1.4686266481876373\n",
      "train loss: 4.803330779075623 - val Loss: 1.468321442604065\n",
      "train loss: 4.801978379487991 - val Loss: 1.4679826200008392\n",
      "train loss: 4.800664722919464 - val Loss: 1.4674588739871979\n",
      "train loss: 4.799348413944244 - val Loss: 1.4673486948013306\n",
      "train loss: 4.798080205917358 - val Loss: 1.4669078886508942\n",
      "train loss: 4.796754449605942 - val Loss: 1.4665012657642365\n",
      "train loss: 4.79545733332634 - val Loss: 1.4665671288967133\n",
      "train loss: 4.794067144393921 - val Loss: 1.4660224616527557\n",
      "train loss: 4.7927425801754 - val Loss: 1.465543419122696\n",
      "train loss: 4.791388273239136 - val Loss: 1.4651051759719849\n",
      "train loss: 4.790105104446411 - val Loss: 1.4646501243114471\n",
      "train loss: 4.788740009069443 - val Loss: 1.4640968143939972\n",
      "train loss: 4.787325471639633 - val Loss: 1.4637619256973267\n",
      "train loss: 4.786028414964676 - val Loss: 1.4632703959941864\n",
      "train loss: 4.784687131643295 - val Loss: 1.4629643559455872\n",
      "train loss: 4.783389180898666 - val Loss: 1.4624755382537842\n",
      "train loss: 4.7820218205451965 - val Loss: 1.4619191884994507\n",
      "train loss: 4.780686378479004 - val Loss: 1.46152862906456\n",
      "train loss: 4.7793906927108765 - val Loss: 1.4611220955848694\n",
      "train loss: 4.7780285477638245 - val Loss: 1.4606971144676208\n",
      "train loss: 4.776710152626038 - val Loss: 1.4602845907211304\n",
      "train loss: 4.775366306304932 - val Loss: 1.4598768651485443\n",
      "train loss: 4.774016380310059 - val Loss: 1.459389865398407\n",
      "train loss: 4.772713989019394 - val Loss: 1.45889014005661\n",
      "train loss: 4.771369099617004 - val Loss: 1.4585471153259277\n",
      "train loss: 4.770020067691803 - val Loss: 1.4580998420715332\n",
      "train loss: 4.768693774938583 - val Loss: 1.4577995836734772\n",
      "train loss: 4.767380446195602 - val Loss: 1.457268387079239\n",
      "train loss: 4.766030967235565 - val Loss: 1.4569240808486938\n",
      "train loss: 4.764708936214447 - val Loss: 1.4564910531044006\n",
      "train loss: 4.76339864730835 - val Loss: 1.4559711515903473\n",
      "train loss: 4.76205986738205 - val Loss: 1.4558138251304626\n",
      "train loss: 4.760755032300949 - val Loss: 1.455290973186493\n",
      "train loss: 4.759429603815079 - val Loss: 1.4549134075641632\n",
      "train loss: 4.758107960224152 - val Loss: 1.4544359147548676\n",
      "train loss: 4.756816983222961 - val Loss: 1.453930139541626\n",
      "train loss: 4.755436688661575 - val Loss: 1.453347384929657\n",
      "train loss: 4.754105478525162 - val Loss: 1.4530071020126343\n",
      "train loss: 4.752768784761429 - val Loss: 1.4526349604129791\n",
      "train loss: 4.7513618767261505 - val Loss: 1.45219486951828\n",
      "train loss: 4.75009298324585 - val Loss: 1.4517691731452942\n",
      "train loss: 4.74880388379097 - val Loss: 1.4514631032943726\n",
      "train loss: 4.7475780844688416 - val Loss: 1.4510266184806824\n",
      "train loss: 4.7462469935417175 - val Loss: 1.4507185518741608\n",
      "train loss: 4.744969695806503 - val Loss: 1.4502573311328888\n",
      "train loss: 4.74371138215065 - val Loss: 1.449842482805252\n",
      "train loss: 4.742441803216934 - val Loss: 1.4492674469947815\n",
      "train loss: 4.7410978972911835 - val Loss: 1.4488606750965118\n",
      "train loss: 4.739820569753647 - val Loss: 1.448377102613449\n",
      "train loss: 4.738531917333603 - val Loss: 1.448068767786026\n",
      "train loss: 4.737218499183655 - val Loss: 1.4476871192455292\n",
      "train loss: 4.7359797060489655 - val Loss: 1.4473715126514435\n",
      "train loss: 4.7346402406692505 - val Loss: 1.447030782699585\n",
      "train loss: 4.733418941497803 - val Loss: 1.4466059803962708\n",
      "train loss: 4.732145845890045 - val Loss: 1.4462100565433502\n",
      "train loss: 4.730869382619858 - val Loss: 1.4455168545246124\n",
      "train loss: 4.729625344276428 - val Loss: 1.4454049170017242\n",
      "train loss: 4.728406071662903 - val Loss: 1.4447867572307587\n",
      "train loss: 4.7271289229393005 - val Loss: 1.4443989992141724\n",
      "train loss: 4.72587063908577 - val Loss: 1.4439658224582672\n",
      "train loss: 4.724610656499863 - val Loss: 1.4435975849628448\n",
      "train loss: 4.723354697227478 - val Loss: 1.443250060081482\n",
      "train loss: 4.72210231423378 - val Loss: 1.4428525865077972\n",
      "train loss: 4.720858544111252 - val Loss: 1.4424978196620941\n",
      "train loss: 4.719682186841965 - val Loss: 1.4421261847019196\n",
      "train loss: 4.718385338783264 - val Loss: 1.4417474865913391\n",
      "train loss: 4.717141807079315 - val Loss: 1.4414585828781128\n",
      "train loss: 4.715893238782883 - val Loss: 1.4410717487335205\n",
      "train loss: 4.7146651446819305 - val Loss: 1.4406520426273346\n",
      "train loss: 4.713372021913528 - val Loss: 1.4402303993701935\n",
      "train loss: 4.71213173866272 - val Loss: 1.439777672290802\n",
      "train loss: 4.710893511772156 - val Loss: 1.439376711845398\n",
      "train loss: 4.709592550992966 - val Loss: 1.4389296770095825\n",
      "train loss: 4.70833945274353 - val Loss: 1.4384896159172058\n",
      "train loss: 4.707114040851593 - val Loss: 1.438044011592865\n",
      "train loss: 4.7058989107608795 - val Loss: 1.4376646280288696\n",
      "train loss: 4.704629957675934 - val Loss: 1.4372787475585938\n",
      "train loss: 4.703451156616211 - val Loss: 1.436833143234253\n",
      "train loss: 4.702173352241516 - val Loss: 1.4364040791988373\n",
      "train loss: 4.700951486825943 - val Loss: 1.4359627664089203\n",
      "train loss: 4.699797540903091 - val Loss: 1.4354775846004486\n",
      "train loss: 4.698520720005035 - val Loss: 1.435093104839325\n",
      "train loss: 4.6973603665828705 - val Loss: 1.434662103652954\n",
      "train loss: 4.696143567562103 - val Loss: 1.4342219829559326\n",
      "train loss: 4.6949315667152405 - val Loss: 1.433847725391388\n",
      "train loss: 4.693752110004425 - val Loss: 1.4334504306316376\n",
      "train loss: 4.692556738853455 - val Loss: 1.4330112040042877\n",
      "train loss: 4.691391468048096 - val Loss: 1.4323542416095734\n",
      "train loss: 4.690171092748642 - val Loss: 1.4319234788417816\n",
      "train loss: 4.689043462276459 - val Loss: 1.4313000440597534\n",
      "train loss: 4.68785548210144 - val Loss: 1.4309325516223907\n",
      "train loss: 4.686643004417419 - val Loss: 1.4307473599910736\n",
      "train loss: 4.685503005981445 - val Loss: 1.4301237165927887\n",
      "train loss: 4.684286117553711 - val Loss: 1.4298822581768036\n",
      "train loss: 4.683129221200943 - val Loss: 1.4292739033699036\n",
      "train loss: 4.681933015584946 - val Loss: 1.428856074810028\n",
      "train loss: 4.680716931819916 - val Loss: 1.428596556186676\n",
      "train loss: 4.679540008306503 - val Loss: 1.4280474781990051\n",
      "train loss: 4.678325533866882 - val Loss: 1.427798330783844\n",
      "train loss: 4.677176088094711 - val Loss: 1.4271773397922516\n",
      "train loss: 4.675996541976929 - val Loss: 1.4269217252731323\n",
      "train loss: 4.674817442893982 - val Loss: 1.4268597066402435\n",
      "train loss: 4.673695087432861 - val Loss: 1.4264056086540222\n",
      "train loss: 4.672483295202255 - val Loss: 1.4259477853775024\n",
      "train loss: 4.671299457550049 - val Loss: 1.4255177676677704\n",
      "train loss: 4.670167416334152 - val Loss: 1.424983412027359\n",
      "train loss: 4.66898974776268 - val Loss: 1.424722582101822\n",
      "train loss: 4.667796820402145 - val Loss: 1.4241872727870941\n",
      "train loss: 4.666538715362549 - val Loss: 1.4235861897468567\n",
      "train loss: 4.665353745222092 - val Loss: 1.4231672883033752\n",
      "train loss: 4.664210319519043 - val Loss: 1.4226275086402893\n",
      "train loss: 4.663070440292358 - val Loss: 1.4221847355365753\n",
      "train loss: 4.661920726299286 - val Loss: 1.4220269918441772\n",
      "train loss: 4.66081041097641 - val Loss: 1.4214595258235931\n",
      "train loss: 4.659624487161636 - val Loss: 1.421271800994873\n",
      "train loss: 4.658470898866653 - val Loss: 1.4206748008728027\n",
      "train loss: 4.657270640134811 - val Loss: 1.4204289019107819\n",
      "train loss: 4.6561377346515656 - val Loss: 1.4199846684932709\n",
      "train loss: 4.654983073472977 - val Loss: 1.4195570647716522\n",
      "train loss: 4.653843134641647 - val Loss: 1.4191351532936096\n",
      "train loss: 4.652704983949661 - val Loss: 1.4186950027942657\n",
      "train loss: 4.651535555720329 - val Loss: 1.4183180630207062\n",
      "train loss: 4.650385543704033 - val Loss: 1.4178803861141205\n",
      "train loss: 4.649194151163101 - val Loss: 1.4173810184001923\n",
      "train loss: 4.648007303476334 - val Loss: 1.4170107245445251\n",
      "train loss: 4.646835505962372 - val Loss: 1.4166267216205597\n",
      "train loss: 4.645744815468788 - val Loss: 1.4161889255046844\n",
      "train loss: 4.644488096237183 - val Loss: 1.4157466292381287\n",
      "train loss: 4.643362671136856 - val Loss: 1.4154129028320312\n",
      "train loss: 4.642208367586136 - val Loss: 1.4151378571987152\n",
      "train loss: 4.6411130875349045 - val Loss: 1.4148035645484924\n",
      "train loss: 4.639949947595596 - val Loss: 1.4144395589828491\n",
      "train loss: 4.63876910507679 - val Loss: 1.4140066504478455\n",
      "train loss: 4.6376083344221115 - val Loss: 1.4136324524879456\n",
      "train loss: 4.636467695236206 - val Loss: 1.4132616817951202\n",
      "train loss: 4.63532355427742 - val Loss: 1.4129399061203003\n",
      "train loss: 4.63417312502861 - val Loss: 1.4126072824001312\n",
      "train loss: 4.632987231016159 - val Loss: 1.4122350811958313\n",
      "train loss: 4.6318700313568115 - val Loss: 1.4117360413074493\n",
      "train loss: 4.630658745765686 - val Loss: 1.4113318622112274\n",
      "train loss: 4.629462689161301 - val Loss: 1.4110097289085388\n",
      "train loss: 4.628324091434479 - val Loss: 1.410737007856369\n",
      "train loss: 4.627114772796631 - val Loss: 1.4103152751922607\n",
      "train loss: 4.6260212659835815 - val Loss: 1.4098695516586304\n",
      "train loss: 4.624875247478485 - val Loss: 1.409458875656128\n",
      "train loss: 4.62370540201664 - val Loss: 1.4090587496757507\n",
      "train loss: 4.622455820441246 - val Loss: 1.408575713634491\n",
      "train loss: 4.621312320232391 - val Loss: 1.4081538319587708\n",
      "train loss: 4.62017048895359 - val Loss: 1.4077872037887573\n",
      "train loss: 4.618967622518539 - val Loss: 1.4074283242225647\n",
      "train loss: 4.617830127477646 - val Loss: 1.4070497453212738\n",
      "train loss: 4.616643339395523 - val Loss: 1.406484216451645\n",
      "train loss: 4.615479439496994 - val Loss: 1.4060599505901337\n",
      "train loss: 4.614290460944176 - val Loss: 1.4056501686573029\n",
      "train loss: 4.61315430700779 - val Loss: 1.4052770137786865\n",
      "train loss: 4.612001538276672 - val Loss: 1.4049064218997955\n",
      "train loss: 4.6107963770627975 - val Loss: 1.4045590460300446\n",
      "train loss: 4.609641894698143 - val Loss: 1.4042178690433502\n",
      "train loss: 4.608439072966576 - val Loss: 1.4038216471672058\n",
      "train loss: 4.6072803139686584 - val Loss: 1.4034294188022614\n",
      "train loss: 4.606120765209198 - val Loss: 1.4030414819717407\n",
      "train loss: 4.604828044772148 - val Loss: 1.4024516642093658\n",
      "train loss: 4.603705614805222 - val Loss: 1.4020510017871857\n",
      "train loss: 4.602511838078499 - val Loss: 1.4015504121780396\n",
      "train loss: 4.601315408945084 - val Loss: 1.4012051820755005\n",
      "train loss: 4.6000780165195465 - val Loss: 1.4007481634616852\n",
      "train loss: 4.598926886916161 - val Loss: 1.4003944396972656\n",
      "train loss: 4.597689568996429 - val Loss: 1.3999621868133545\n",
      "train loss: 4.596482440829277 - val Loss: 1.3995280861854553\n",
      "train loss: 4.59534215927124 - val Loss: 1.3993678390979767\n",
      "train loss: 4.594077453017235 - val Loss: 1.3990368843078613\n",
      "train loss: 4.592874318361282 - val Loss: 1.3986734449863434\n",
      "train loss: 4.591668263077736 - val Loss: 1.398252934217453\n",
      "train loss: 4.59055869281292 - val Loss: 1.3979220390319824\n",
      "train loss: 4.5893906354904175 - val Loss: 1.397547334432602\n",
      "train loss: 4.588181644678116 - val Loss: 1.397100955247879\n",
      "train loss: 4.587005227804184 - val Loss: 1.396822214126587\n",
      "train loss: 4.585918962955475 - val Loss: 1.3963857889175415\n",
      "train loss: 4.584772676229477 - val Loss: 1.3959618210792542\n",
      "train loss: 4.583638921380043 - val Loss: 1.3955328166484833\n",
      "train loss: 4.58247272670269 - val Loss: 1.3950757086277008\n",
      "train loss: 4.581286922097206 - val Loss: 1.394721657037735\n",
      "train loss: 4.5802063047885895 - val Loss: 1.3943698406219482\n",
      "train loss: 4.579009830951691 - val Loss: 1.394056499004364\n",
      "train loss: 4.577883496880531 - val Loss: 1.3938175439834595\n",
      "train loss: 4.576868876814842 - val Loss: 1.3932785987854004\n",
      "train loss: 4.575732260942459 - val Loss: 1.3930364549160004\n",
      "train loss: 4.574718862771988 - val Loss: 1.3925896883010864\n",
      "train loss: 4.573599681258202 - val Loss: 1.3922020196914673\n",
      "train loss: 4.572462439537048 - val Loss: 1.3918499946594238\n",
      "train loss: 4.571360632777214 - val Loss: 1.3914850354194641\n",
      "train loss: 4.570240616798401 - val Loss: 1.3912189304828644\n",
      "train loss: 4.5691734701395035 - val Loss: 1.390817642211914\n",
      "train loss: 4.568053916096687 - val Loss: 1.3902766406536102\n",
      "train loss: 4.566849425435066 - val Loss: 1.3900459706783295\n",
      "train loss: 4.565789952874184 - val Loss: 1.3897041082382202\n",
      "train loss: 4.564658358693123 - val Loss: 1.389288067817688\n",
      "train loss: 4.563574776053429 - val Loss: 1.3891070783138275\n",
      "train loss: 4.56247915327549 - val Loss: 1.3887306153774261\n",
      "train loss: 4.561380967497826 - val Loss: 1.388394683599472\n",
      "train loss: 4.5602970868349075 - val Loss: 1.3880157768726349\n",
      "train loss: 4.5592038333415985 - val Loss: 1.3877053558826447\n",
      "train loss: 4.558094561100006 - val Loss: 1.387304961681366\n",
      "train loss: 4.556946203112602 - val Loss: 1.386977106332779\n",
      "train loss: 4.555832117795944 - val Loss: 1.3865853250026703\n",
      "train loss: 4.5547585636377335 - val Loss: 1.3859237730503082\n",
      "train loss: 4.553658574819565 - val Loss: 1.3858155608177185\n",
      "train loss: 4.552646040916443 - val Loss: 1.385300874710083\n",
      "train loss: 4.551539912819862 - val Loss: 1.3849450051784515\n",
      "train loss: 4.55043563246727 - val Loss: 1.3846032619476318\n",
      "train loss: 4.5493333786726 - val Loss: 1.384180873632431\n",
      "train loss: 4.54827219247818 - val Loss: 1.3840272426605225\n",
      "train loss: 4.5471151471138 - val Loss: 1.383553296327591\n",
      "train loss: 4.5460479110479355 - val Loss: 1.3833200633525848\n",
      "train loss: 4.544965833425522 - val Loss: 1.3828750848770142\n",
      "train loss: 4.543896824121475 - val Loss: 1.3825145065784454\n",
      "train loss: 4.542724817991257 - val Loss: 1.3822182416915894\n",
      "train loss: 4.541769325733185 - val Loss: 1.3819531202316284\n",
      "train loss: 4.540667951107025 - val Loss: 1.3815875947475433\n",
      "train loss: 4.539562597870827 - val Loss: 1.3813230693340302\n",
      "train loss: 4.538505434989929 - val Loss: 1.3807793855667114\n",
      "train loss: 4.53729435801506 - val Loss: 1.3804220259189606\n",
      "train loss: 4.536294311285019 - val Loss: 1.3800359964370728\n",
      "train loss: 4.535187974572182 - val Loss: 1.3796302378177643\n",
      "train loss: 4.534091740846634 - val Loss: 1.3792021870613098\n",
      "train loss: 4.532897397875786 - val Loss: 1.3787639141082764\n",
      "train loss: 4.531781107187271 - val Loss: 1.3784998953342438\n",
      "train loss: 4.530760928988457 - val Loss: 1.3780770599842072\n",
      "train loss: 4.529612898826599 - val Loss: 1.3776171505451202\n",
      "train loss: 4.528548762202263 - val Loss: 1.3773070573806763\n",
      "train loss: 4.527449682354927 - val Loss: 1.3770130574703217\n",
      "train loss: 4.526411101222038 - val Loss: 1.3767008483409882\n",
      "train loss: 4.525243923068047 - val Loss: 1.3763483464717865\n",
      "train loss: 4.524168610572815 - val Loss: 1.3760012090206146\n",
      "train loss: 4.523158594965935 - val Loss: 1.3756591975688934\n",
      "train loss: 4.522040009498596 - val Loss: 1.3753189742565155\n",
      "train loss: 4.520972788333893 - val Loss: 1.3749886453151703\n",
      "train loss: 4.519964724779129 - val Loss: 1.3746695220470428\n",
      "train loss: 4.518937513232231 - val Loss: 1.3742913007736206\n",
      "train loss: 4.517887771129608 - val Loss: 1.3739843666553497\n",
      "train loss: 4.516810789704323 - val Loss: 1.373616486787796\n",
      "train loss: 4.515743479132652 - val Loss: 1.3733482956886292\n",
      "train loss: 4.514698997139931 - val Loss: 1.372982531785965\n",
      "train loss: 4.513603791594505 - val Loss: 1.3725843131542206\n",
      "train loss: 4.512629047036171 - val Loss: 1.37228924036026\n",
      "train loss: 4.511575236916542 - val Loss: 1.371909648180008\n",
      "train loss: 4.5104604214429855 - val Loss: 1.371575653553009\n",
      "train loss: 4.5093991458415985 - val Loss: 1.3712563216686249\n",
      "train loss: 4.508361250162125 - val Loss: 1.3709284365177155\n",
      "train loss: 4.507283553481102 - val Loss: 1.3705878853797913\n",
      "train loss: 4.506298869848251 - val Loss: 1.370273470878601\n",
      "train loss: 4.505203261971474 - val Loss: 1.3699329793453217\n",
      "train loss: 4.504180431365967 - val Loss: 1.3696043491363525\n",
      "train loss: 4.503117009997368 - val Loss: 1.3692676424980164\n",
      "train loss: 4.502065598964691 - val Loss: 1.3689437508583069\n",
      "train loss: 4.501091793179512 - val Loss: 1.368634045124054\n",
      "train loss: 4.5000742971897125 - val Loss: 1.3684016168117523\n",
      "train loss: 4.499016776680946 - val Loss: 1.3681054711341858\n",
      "train loss: 4.498030975461006 - val Loss: 1.3678122162818909\n",
      "train loss: 4.4969726502895355 - val Loss: 1.3674845397472382\n",
      "train loss: 4.495987296104431 - val Loss: 1.3671761453151703\n",
      "train loss: 4.495002835988998 - val Loss: 1.366919994354248\n",
      "train loss: 4.493982225656509 - val Loss: 1.3666476905345917\n",
      "train loss: 4.492858901619911 - val Loss: 1.36624214053154\n",
      "train loss: 4.49196982383728 - val Loss: 1.365948736667633\n",
      "train loss: 4.49086207151413 - val Loss: 1.3655714094638824\n",
      "train loss: 4.489897444844246 - val Loss: 1.3652458488941193\n",
      "train loss: 4.488836452364922 - val Loss: 1.3649007678031921\n",
      "train loss: 4.487834706902504 - val Loss: 1.364572435617447\n",
      "train loss: 4.486839488148689 - val Loss: 1.364171177148819\n",
      "train loss: 4.485768437385559 - val Loss: 1.3638053238391876\n",
      "train loss: 4.484800398349762 - val Loss: 1.3635829985141754\n",
      "train loss: 4.48375141620636 - val Loss: 1.363142192363739\n",
      "train loss: 4.482736766338348 - val Loss: 1.362784206867218\n",
      "train loss: 4.481714844703674 - val Loss: 1.3626350462436676\n",
      "train loss: 4.4807190001010895 - val Loss: 1.3622347712516785\n",
      "train loss: 4.4797138422727585 - val Loss: 1.3619135618209839\n",
      "train loss: 4.4786441177129745 - val Loss: 1.3616152703762054\n",
      "train loss: 4.477673262357712 - val Loss: 1.3612863421440125\n",
      "train loss: 4.476657599210739 - val Loss: 1.361038714647293\n",
      "train loss: 4.475665181875229 - val Loss: 1.3607307374477386\n",
      "train loss: 4.474587127566338 - val Loss: 1.3603120148181915\n",
      "train loss: 4.473589509725571 - val Loss: 1.3600219190120697\n",
      "train loss: 4.472593694925308 - val Loss: 1.3596740365028381\n",
      "train loss: 4.471588313579559 - val Loss: 1.359300822019577\n",
      "train loss: 4.470579594373703 - val Loss: 1.3589430749416351\n",
      "train loss: 4.46959188580513 - val Loss: 1.3586058914661407\n",
      "train loss: 4.468599498271942 - val Loss: 1.3581860661506653\n",
      "train loss: 4.467642113566399 - val Loss: 1.3578689396381378\n",
      "train loss: 4.466643139719963 - val Loss: 1.3575305044651031\n",
      "train loss: 4.465607210993767 - val Loss: 1.357125699520111\n",
      "train loss: 4.464629903435707 - val Loss: 1.3567935526371002\n",
      "train loss: 4.4636188596487045 - val Loss: 1.3564752340316772\n",
      "train loss: 4.462593257427216 - val Loss: 1.3559924364089966\n",
      "train loss: 4.461642846465111 - val Loss: 1.3556499183177948\n",
      "train loss: 4.460621252655983 - val Loss: 1.3553571999073029\n",
      "train loss: 4.4596627950668335 - val Loss: 1.354979932308197\n",
      "train loss: 4.458725184202194 - val Loss: 1.3547723591327667\n",
      "train loss: 4.457712009549141 - val Loss: 1.3544126152992249\n",
      "train loss: 4.456756189465523 - val Loss: 1.3540974259376526\n",
      "train loss: 4.455801397562027 - val Loss: 1.3537770807743073\n",
      "train loss: 4.454815492033958 - val Loss: 1.3534612655639648\n",
      "train loss: 4.4538556188344955 - val Loss: 1.3535602688789368\n",
      "train loss: 4.452822506427765 - val Loss: 1.353270798921585\n",
      "train loss: 4.45183153450489 - val Loss: 1.3529361188411713\n",
      "train loss: 4.450840026140213 - val Loss: 1.352619230747223\n",
      "train loss: 4.449935466051102 - val Loss: 1.3523167073726654\n",
      "train loss: 4.448907256126404 - val Loss: 1.35198974609375\n",
      "train loss: 4.447991341352463 - val Loss: 1.3516749143600464\n",
      "train loss: 4.447077468037605 - val Loss: 1.3513234555721283\n",
      "train loss: 4.446102097630501 - val Loss: 1.3509571850299835\n",
      "train loss: 4.445144802331924 - val Loss: 1.3506576418876648\n",
      "train loss: 4.444211483001709 - val Loss: 1.3503462374210358\n",
      "train loss: 4.44318488240242 - val Loss: 1.3500649333000183\n",
      "train loss: 4.442222386598587 - val Loss: 1.3497835993766785\n",
      "train loss: 4.441301912069321 - val Loss: 1.3494710624217987\n",
      "train loss: 4.440331414341927 - val Loss: 1.349331110715866\n",
      "train loss: 4.439342483878136 - val Loss: 1.3489681482315063\n",
      "train loss: 4.438414245843887 - val Loss: 1.3486104011535645\n",
      "train loss: 4.437464147806168 - val Loss: 1.348316252231598\n",
      "train loss: 4.436541140079498 - val Loss: 1.347833663225174\n",
      "train loss: 4.435509353876114 - val Loss: 1.347478985786438\n",
      "train loss: 4.434548810124397 - val Loss: 1.3471410274505615\n",
      "train loss: 4.433534264564514 - val Loss: 1.3468193709850311\n",
      "train loss: 4.432575613260269 - val Loss: 1.3465010225772858\n",
      "train loss: 4.431578978896141 - val Loss: 1.3461858332157135\n",
      "train loss: 4.430602505803108 - val Loss: 1.345877230167389\n",
      "train loss: 4.4295957535505295 - val Loss: 1.3452207148075104\n",
      "train loss: 4.428595557808876 - val Loss: 1.3449462950229645\n",
      "train loss: 4.427703827619553 - val Loss: 1.3446116745471954\n",
      "train loss: 4.4267608523368835 - val Loss: 1.3442942798137665\n",
      "train loss: 4.4258459359407425 - val Loss: 1.3442834615707397\n",
      "train loss: 4.424968659877777 - val Loss: 1.3438624441623688\n",
      "train loss: 4.424075022339821 - val Loss: 1.3435538411140442\n",
      "train loss: 4.423057600855827 - val Loss: 1.3432353138923645\n",
      "train loss: 4.422164887189865 - val Loss: 1.3429205119609833\n",
      "train loss: 4.421151250600815 - val Loss: 1.3426240384578705\n",
      "train loss: 4.420219212770462 - val Loss: 1.3423795104026794\n",
      "train loss: 4.419275149703026 - val Loss: 1.34203639626503\n",
      "train loss: 4.41832759976387 - val Loss: 1.3417734801769257\n",
      "train loss: 4.417423099279404 - val Loss: 1.3414269387722015\n",
      "train loss: 4.416526198387146 - val Loss: 1.341053456068039\n",
      "train loss: 4.415502279996872 - val Loss: 1.3408116698265076\n",
      "train loss: 4.414600819349289 - val Loss: 1.3404443860054016\n",
      "train loss: 4.413654237985611 - val Loss: 1.3401480913162231\n",
      "train loss: 4.412665247917175 - val Loss: 1.3398130238056183\n",
      "train loss: 4.411782056093216 - val Loss: 1.3394884765148163\n",
      "train loss: 4.410884991288185 - val Loss: 1.339187353849411\n",
      "train loss: 4.409931406378746 - val Loss: 1.3388743102550507\n",
      "train loss: 4.409038752317429 - val Loss: 1.3385626673698425\n",
      "train loss: 4.4080876260995865 - val Loss: 1.3381594717502594\n",
      "train loss: 4.407164618372917 - val Loss: 1.3378105163574219\n",
      "train loss: 4.40619769692421 - val Loss: 1.3375028371810913\n",
      "train loss: 4.405305698513985 - val Loss: 1.3372457921504974\n",
      "train loss: 4.404325231909752 - val Loss: 1.3369053304195404\n",
      "train loss: 4.403405591845512 - val Loss: 1.336601585149765\n",
      "train loss: 4.402506723999977 - val Loss: 1.3362513780593872\n",
      "train loss: 4.401510044932365 - val Loss: 1.3359023928642273\n",
      "train loss: 4.4005613178014755 - val Loss: 1.3355374038219452\n",
      "train loss: 4.399558946490288 - val Loss: 1.335071176290512\n",
      "train loss: 4.398643538355827 - val Loss: 1.3346878588199615\n",
      "train loss: 4.397666856646538 - val Loss: 1.3343413770198822\n",
      "train loss: 4.396711051464081 - val Loss: 1.334021419286728\n",
      "train loss: 4.395807832479477 - val Loss: 1.3337014615535736\n",
      "train loss: 4.394844233989716 - val Loss: 1.333536058664322\n",
      "train loss: 4.393917009234428 - val Loss: 1.3329341113567352\n",
      "train loss: 4.392826974391937 - val Loss: 1.332906186580658\n",
      "train loss: 4.391895070672035 - val Loss: 1.332348108291626\n",
      "train loss: 4.390863373875618 - val Loss: 1.332372009754181\n",
      "train loss: 4.390010163187981 - val Loss: 1.3318229615688324\n",
      "train loss: 4.389029994606972 - val Loss: 1.3317856192588806\n",
      "train loss: 4.388178393244743 - val Loss: 1.3312873244285583\n",
      "train loss: 4.387233093380928 - val Loss: 1.3313101530075073\n",
      "train loss: 4.386364847421646 - val Loss: 1.3307211995124817\n",
      "train loss: 4.3852899968624115 - val Loss: 1.3307412266731262\n",
      "train loss: 4.384428143501282 - val Loss: 1.3304760158061981\n",
      "train loss: 4.383598744869232 - val Loss: 1.3300172686576843\n",
      "train loss: 4.38257160782814 - val Loss: 1.330076813697815\n",
      "train loss: 4.381681352853775 - val Loss: 1.3298049569129944\n",
      "train loss: 4.380759030580521 - val Loss: 1.3295820951461792\n",
      "train loss: 4.379826337099075 - val Loss: 1.3288795351982117\n",
      "train loss: 4.3788284212350845 - val Loss: 1.3289850950241089\n",
      "train loss: 4.377911880612373 - val Loss: 1.3284136652946472\n",
      "train loss: 4.376914963126183 - val Loss: 1.328517496585846\n",
      "train loss: 4.376078709959984 - val Loss: 1.3282416760921478\n",
      "train loss: 4.375147953629494 - val Loss: 1.327704280614853\n",
      "train loss: 4.374155521392822 - val Loss: 1.3277300894260406\n",
      "train loss: 4.373275592923164 - val Loss: 1.3274168074131012\n",
      "train loss: 4.372376412153244 - val Loss: 1.326899915933609\n",
      "train loss: 4.3713668435812 - val Loss: 1.3269215822219849\n",
      "train loss: 4.3704656809568405 - val Loss: 1.3266509473323822\n",
      "train loss: 4.369532987475395 - val Loss: 1.3260811567306519\n",
      "train loss: 4.368551954627037 - val Loss: 1.3260729610919952\n",
      "train loss: 4.367609813809395 - val Loss: 1.3257379531860352\n",
      "train loss: 4.366697147488594 - val Loss: 1.3254929780960083\n",
      "train loss: 4.365803226828575 - val Loss: 1.3248977661132812\n",
      "train loss: 4.36482360959053 - val Loss: 1.3252296447753906\n",
      "train loss: 4.36406297981739 - val Loss: 1.3245636820793152\n",
      "train loss: 4.363138943910599 - val Loss: 1.3241329491138458\n",
      "train loss: 4.362177193164825 - val Loss: 1.3245410323143005\n",
      "train loss: 4.361354053020477 - val Loss: 1.324141025543213\n",
      "train loss: 4.3604912757873535 - val Loss: 1.3238838016986847\n",
      "train loss: 4.359529376029968 - val Loss: 1.3235266506671906\n",
      "train loss: 4.358724519610405 - val Loss: 1.3233224749565125\n",
      "train loss: 4.357844308018684 - val Loss: 1.3230770528316498\n",
      "train loss: 4.356905072927475 - val Loss: 1.3228212893009186\n",
      "train loss: 4.355981960892677 - val Loss: 1.3224823474884033\n",
      "train loss: 4.355050981044769 - val Loss: 1.3222321569919586\n",
      "train loss: 4.354128539562225 - val Loss: 1.3219758570194244\n",
      "train loss: 4.353217795491219 - val Loss: 1.3217260241508484\n",
      "train loss: 4.352324888110161 - val Loss: 1.321433961391449\n",
      "train loss: 4.3514139503240585 - val Loss: 1.3212459087371826\n",
      "train loss: 4.350517585873604 - val Loss: 1.3208796977996826\n",
      "train loss: 4.349625498056412 - val Loss: 1.320628672838211\n",
      "train loss: 4.348715603351593 - val Loss: 1.320371687412262\n",
      "train loss: 4.347779333591461 - val Loss: 1.3201328217983246\n",
      "train loss: 4.346893608570099 - val Loss: 1.319896548986435\n",
      "train loss: 4.346044525504112 - val Loss: 1.3196454346179962\n",
      "train loss: 4.345144018530846 - val Loss: 1.3193859457969666\n",
      "train loss: 4.344202324748039 - val Loss: 1.3191571235656738\n",
      "train loss: 4.343362703919411 - val Loss: 1.3189222514629364\n",
      "train loss: 4.3424635380506516 - val Loss: 1.3186832070350647\n",
      "train loss: 4.341560572385788 - val Loss: 1.3184252679347992\n",
      "train loss: 4.34067839384079 - val Loss: 1.3182381093502045\n",
      "train loss: 4.339832007884979 - val Loss: 1.3180364072322845\n",
      "train loss: 4.338946059346199 - val Loss: 1.3175542950630188\n",
      "train loss: 4.33800333738327 - val Loss: 1.3176260888576508\n",
      "train loss: 4.33717754483223 - val Loss: 1.3170957565307617\n",
      "train loss: 4.336204618215561 - val Loss: 1.3171420991420746\n",
      "train loss: 4.335378527641296 - val Loss: 1.3166191577911377\n",
      "train loss: 4.334430158138275 - val Loss: 1.316615268588066\n",
      "train loss: 4.333652853965759 - val Loss: 1.3163757175207138\n",
      "train loss: 4.332721963524818 - val Loss: 1.3161072880029678\n",
      "train loss: 4.3318911492824554 - val Loss: 1.315616488456726\n",
      "train loss: 4.330948919057846 - val Loss: 1.3156318962574005\n",
      "train loss: 4.330169305205345 - val Loss: 1.315409243106842\n",
      "train loss: 4.329279854893684 - val Loss: 1.3148464113473892\n",
      "train loss: 4.328350752592087 - val Loss: 1.3148916065692902\n",
      "train loss: 4.327583000063896 - val Loss: 1.3146235942840576\n",
      "train loss: 4.3267499804496765 - val Loss: 1.3140579164028168\n",
      "train loss: 4.325822085142136 - val Loss: 1.314141184091568\n",
      "train loss: 4.325024038553238 - val Loss: 1.3134730160236359\n",
      "train loss: 4.324135109782219 - val Loss: 1.31354421377182\n",
      "train loss: 4.323308676481247 - val Loss: 1.3129957914352417\n",
      "train loss: 4.3224759846925735 - val Loss: 1.3129867613315582\n",
      "train loss: 4.3216777592897415 - val Loss: 1.31244096159935\n",
      "train loss: 4.320735529065132 - val Loss: 1.3121767491102219\n",
      "train loss: 4.319975346326828 - val Loss: 1.3119422942399979\n",
      "train loss: 4.319090202450752 - val Loss: 1.3116237670183182\n",
      "train loss: 4.318333879113197 - val Loss: 1.3117629289627075\n",
      "train loss: 4.317531377077103 - val Loss: 1.3111799210309982\n",
      "train loss: 4.31668995320797 - val Loss: 1.310948207974434\n",
      "train loss: 4.315874233841896 - val Loss: 1.3107417970895767\n",
      "train loss: 4.314993754029274 - val Loss: 1.3107982873916626\n",
      "train loss: 4.314190283417702 - val Loss: 1.3102906793355942\n",
      "train loss: 4.313390836119652 - val Loss: 1.3100195527076721\n",
      "train loss: 4.312512740492821 - val Loss: 1.309809848666191\n",
      "train loss: 4.311718061566353 - val Loss: 1.3099265545606613\n",
      "train loss: 4.310917437076569 - val Loss: 1.3093710243701935\n",
      "train loss: 4.310018688440323 - val Loss: 1.3090893775224686\n",
      "train loss: 4.309139266610146 - val Loss: 1.3091744482517242\n",
      "train loss: 4.308397829532623 - val Loss: 1.3086614161729813\n",
      "train loss: 4.3075161427259445 - val Loss: 1.3083724677562714\n",
      "train loss: 4.30667470395565 - val Loss: 1.3085586577653885\n",
      "train loss: 4.305926963686943 - val Loss: 1.308040291070938\n",
      "train loss: 4.305012419819832 - val Loss: 1.3080431520938873\n",
      "train loss: 4.304300993680954 - val Loss: 1.3075280487537384\n",
      "train loss: 4.303412601351738 - val Loss: 1.3075773566961288\n",
      "train loss: 4.302652209997177 - val Loss: 1.3070261925458908\n",
      "train loss: 4.301842823624611 - val Loss: 1.3071531653404236\n",
      "train loss: 4.301046252250671 - val Loss: 1.3069096505641937\n",
      "train loss: 4.300243556499481 - val Loss: 1.3063956201076508\n",
      "train loss: 4.2993825525045395 - val Loss: 1.3064732104539871\n",
      "train loss: 4.298640578985214 - val Loss: 1.3062149733304977\n",
      "train loss: 4.297855123877525 - val Loss: 1.3056587129831314\n",
      "train loss: 4.297084271907806 - val Loss: 1.305759072303772\n",
      "train loss: 4.296341091394424 - val Loss: 1.3055217415094376\n",
      "train loss: 4.295588582754135 - val Loss: 1.3049855828285217\n",
      "train loss: 4.29470981657505 - val Loss: 1.3047979027032852\n",
      "train loss: 4.293971240520477 - val Loss: 1.3048835098743439\n",
      "train loss: 4.293218448758125 - val Loss: 1.3046721816062927\n",
      "train loss: 4.292478576302528 - val Loss: 1.3041574954986572\n",
      "train loss: 4.2916160970926285 - val Loss: 1.304237186908722\n",
      "train loss: 4.290868118405342 - val Loss: 1.30405093729496\n",
      "train loss: 4.290125221014023 - val Loss: 1.303841844201088\n",
      "train loss: 4.289359822869301 - val Loss: 1.3036499917507172\n",
      "train loss: 4.288530394434929 - val Loss: 1.3034348487854004\n",
      "train loss: 4.287775233387947 - val Loss: 1.3032379895448685\n",
      "train loss: 4.286954998970032 - val Loss: 1.303021177649498\n",
      "train loss: 4.286162570118904 - val Loss: 1.3028318732976913\n",
      "train loss: 4.2853473871946335 - val Loss: 1.3026155829429626\n",
      "train loss: 4.2845879048109055 - val Loss: 1.3024460226297379\n",
      "train loss: 4.283844783902168 - val Loss: 1.302222028374672\n",
      "train loss: 4.282995358109474 - val Loss: 1.3017065674066544\n",
      "train loss: 4.282265290617943 - val Loss: 1.3018507361412048\n",
      "train loss: 4.281457170844078 - val Loss: 1.3016389310359955\n",
      "train loss: 4.280674114823341 - val Loss: 1.3011417239904404\n",
      "train loss: 4.279814004898071 - val Loss: 1.3012668192386627\n",
      "train loss: 4.279159888625145 - val Loss: 1.3011136651039124\n",
      "train loss: 4.278375446796417 - val Loss: 1.300613060593605\n",
      "train loss: 4.277562707662582 - val Loss: 1.3007353395223618\n",
      "train loss: 4.276869207620621 - val Loss: 1.300514966249466\n",
      "train loss: 4.276191771030426 - val Loss: 1.30039943754673\n",
      "train loss: 4.275325581431389 - val Loss: 1.300139918923378\n",
      "train loss: 4.274609670042992 - val Loss: 1.2998320311307907\n",
      "train loss: 4.273874402046204 - val Loss: 1.2994033247232437\n",
      "train loss: 4.273079946637154 - val Loss: 1.299507975578308\n",
      "train loss: 4.272373124957085 - val Loss: 1.2993071973323822\n",
      "train loss: 4.271627739071846 - val Loss: 1.2990757077932358\n",
      "train loss: 4.270873591303825 - val Loss: 1.298885628581047\n",
      "train loss: 4.270109578967094 - val Loss: 1.2986676394939423\n",
      "train loss: 4.2693320363759995 - val Loss: 1.2985952943563461\n",
      "train loss: 4.26860386133194 - val Loss: 1.2983414977788925\n",
      "train loss: 4.26784111559391 - val Loss: 1.2981847524642944\n",
      "train loss: 4.267066925764084 - val Loss: 1.2979151457548141\n",
      "train loss: 4.266352728009224 - val Loss: 1.297644004225731\n",
      "train loss: 4.2656279951334 - val Loss: 1.2974459677934647\n",
      "train loss: 4.264911457896233 - val Loss: 1.2973414212465286\n",
      "train loss: 4.264113813638687 - val Loss: 1.2970861494541168\n",
      "train loss: 4.263325199484825 - val Loss: 1.2968862354755402\n",
      "train loss: 4.262583240866661 - val Loss: 1.2966926097869873\n",
      "train loss: 4.261881664395332 - val Loss: 1.2965396493673325\n",
      "train loss: 4.261166542768478 - val Loss: 1.296283021569252\n",
      "train loss: 4.2604033797979355 - val Loss: 1.296134740114212\n",
      "train loss: 4.259684577584267 - val Loss: 1.2958706766366959\n",
      "train loss: 4.25891900062561 - val Loss: 1.2955218851566315\n",
      "train loss: 4.258242577314377 - val Loss: 1.2954647988080978\n",
      "train loss: 4.257501348853111 - val Loss: 1.2951416820287704\n",
      "train loss: 4.256692677736282 - val Loss: 1.2948799133300781\n",
      "train loss: 4.256007954478264 - val Loss: 1.2946427315473557\n",
      "train loss: 4.255238816142082 - val Loss: 1.294515699148178\n",
      "train loss: 4.254475101828575 - val Loss: 1.2941827178001404\n",
      "train loss: 4.253726616501808 - val Loss: 1.2939847260713577\n",
      "train loss: 4.253012523055077 - val Loss: 1.2937862277030945\n",
      "train loss: 4.252331927418709 - val Loss: 1.2935899645090103\n",
      "train loss: 4.25153660774231 - val Loss: 1.2934293746948242\n",
      "train loss: 4.2507851123809814 - val Loss: 1.2932560443878174\n",
      "train loss: 4.25007002055645 - val Loss: 1.2930689752101898\n",
      "train loss: 4.24931888282299 - val Loss: 1.292855367064476\n",
      "train loss: 4.248639345169067 - val Loss: 1.2926844656467438\n",
      "train loss: 4.247908040881157 - val Loss: 1.2925130426883698\n",
      "train loss: 4.247238859534264 - val Loss: 1.292299821972847\n",
      "train loss: 4.24649153649807 - val Loss: 1.2920847237110138\n",
      "train loss: 4.245837137103081 - val Loss: 1.2918701320886612\n",
      "train loss: 4.245083659887314 - val Loss: 1.2916643768548965\n",
      "train loss: 4.244397595524788 - val Loss: 1.2914038598537445\n",
      "train loss: 4.243651822209358 - val Loss: 1.2911972850561142\n",
      "train loss: 4.2428969591856 - val Loss: 1.291001796722412\n",
      "train loss: 4.242167145013809 - val Loss: 1.2907601296901703\n",
      "train loss: 4.241442263126373 - val Loss: 1.2905789464712143\n",
      "train loss: 4.240694805979729 - val Loss: 1.2902917861938477\n",
      "train loss: 4.240039750933647 - val Loss: 1.2900993824005127\n",
      "train loss: 4.239292845129967 - val Loss: 1.2898913770914078\n",
      "train loss: 4.2385926097631454 - val Loss: 1.2897718846797943\n",
      "train loss: 4.237879186868668 - val Loss: 1.2894852757453918\n",
      "train loss: 4.237185716629028 - val Loss: 1.2892769128084183\n",
      "train loss: 4.236469015479088 - val Loss: 1.289055734872818\n",
      "train loss: 4.23577980697155 - val Loss: 1.2889237850904465\n",
      "train loss: 4.235105350613594 - val Loss: 1.2887237221002579\n",
      "train loss: 4.234394013881683 - val Loss: 1.2884542793035507\n",
      "train loss: 4.2337252497673035 - val Loss: 1.2883242219686508\n",
      "train loss: 4.232982620596886 - val Loss: 1.2880765944719315\n",
      "train loss: 4.232296407222748 - val Loss: 1.2879210114479065\n",
      "train loss: 4.2316173911094666 - val Loss: 1.287643477320671\n",
      "train loss: 4.2308893501758575 - val Loss: 1.2874589413404465\n",
      "train loss: 4.230249732732773 - val Loss: 1.2872858196496964\n",
      "train loss: 4.2294762134552 - val Loss: 1.2871057242155075\n",
      "train loss: 4.228844091296196 - val Loss: 1.28694349527359\n",
      "train loss: 4.228108897805214 - val Loss: 1.2867745161056519\n",
      "train loss: 4.227392747998238 - val Loss: 1.2865876704454422\n",
      "train loss: 4.2267202734947205 - val Loss: 1.2863923758268356\n",
      "train loss: 4.226000055670738 - val Loss: 1.2861872762441635\n",
      "train loss: 4.225331231951714 - val Loss: 1.2859844118356705\n",
      "train loss: 4.224633261561394 - val Loss: 1.2857971042394638\n",
      "train loss: 4.223986238241196 - val Loss: 1.2856322079896927\n",
      "train loss: 4.223258078098297 - val Loss: 1.2854551672935486\n",
      "train loss: 4.222569540143013 - val Loss: 1.2854163944721222\n",
      "train loss: 4.221861556172371 - val Loss: 1.2851810306310654\n",
      "train loss: 4.221123248338699 - val Loss: 1.2849422097206116\n",
      "train loss: 4.220487505197525 - val Loss: 1.2847868502140045\n",
      "train loss: 4.219739004969597 - val Loss: 1.2845121324062347\n",
      "train loss: 4.2191514521837234 - val Loss: 1.2843539267778397\n",
      "train loss: 4.218379631638527 - val Loss: 1.2841582149267197\n",
      "train loss: 4.217688828706741 - val Loss: 1.2839864641427994\n",
      "train loss: 4.216975182294846 - val Loss: 1.2837881445884705\n",
      "train loss: 4.216343209147453 - val Loss: 1.2836399376392365\n",
      "train loss: 4.215618252754211 - val Loss: 1.2834261357784271\n",
      "train loss: 4.214924171566963 - val Loss: 1.2832424938678741\n",
      "train loss: 4.214280337095261 - val Loss: 1.283041387796402\n",
      "train loss: 4.213573187589645 - val Loss: 1.2829145342111588\n",
      "train loss: 4.2129426300525665 - val Loss: 1.282755047082901\n",
      "train loss: 4.2123415768146515 - val Loss: 1.282558262348175\n",
      "train loss: 4.211628749966621 - val Loss: 1.2823225855827332\n",
      "train loss: 4.211040273308754 - val Loss: 1.282190427184105\n",
      "train loss: 4.2102978229522705 - val Loss: 1.2819742858409882\n",
      "train loss: 4.209681525826454 - val Loss: 1.2818188071250916\n",
      "train loss: 4.208920702338219 - val Loss: 1.281604826450348\n",
      "train loss: 4.208212569355965 - val Loss: 1.2814271450042725\n",
      "train loss: 4.207510024309158 - val Loss: 1.2812588065862656\n",
      "train loss: 4.206888929009438 - val Loss: 1.2810801416635513\n",
      "train loss: 4.206218168139458 - val Loss: 1.2808524519205093\n",
      "train loss: 4.205540969967842 - val Loss: 1.28056600689888\n",
      "train loss: 4.204901769757271 - val Loss: 1.2803263813257217\n",
      "train loss: 4.204246416687965 - val Loss: 1.2800833135843277\n",
      "train loss: 4.203579381108284 - val Loss: 1.2798610627651215\n",
      "train loss: 4.202866822481155 - val Loss: 1.279171660542488\n",
      "train loss: 4.202272325754166 - val Loss: 1.2789923250675201\n",
      "train loss: 4.201619982719421 - val Loss: 1.2787718176841736\n",
      "train loss: 4.200997531414032 - val Loss: 1.2785413712263107\n",
      "train loss: 4.200304791331291 - val Loss: 1.2783178985118866\n",
      "train loss: 4.19964075088501 - val Loss: 1.278115063905716\n",
      "train loss: 4.198981836438179 - val Loss: 1.27797232568264\n",
      "train loss: 4.198303729295731 - val Loss: 1.2777993232011795\n",
      "train loss: 4.197656109929085 - val Loss: 1.2776359170675278\n",
      "train loss: 4.197018876671791 - val Loss: 1.2774240374565125\n",
      "train loss: 4.196323931217194 - val Loss: 1.2772211879491806\n",
      "train loss: 4.195608153939247 - val Loss: 1.2769566774368286\n",
      "train loss: 4.194957822561264 - val Loss: 1.2768156975507736\n",
      "train loss: 4.194305837154388 - val Loss: 1.2766017019748688\n",
      "train loss: 4.1936842650175095 - val Loss: 1.2764254957437515\n",
      "train loss: 4.193027690052986 - val Loss: 1.2761086821556091\n",
      "train loss: 4.192375913262367 - val Loss: 1.2759090214967728\n",
      "train loss: 4.191710397601128 - val Loss: 1.275719791650772\n",
      "train loss: 4.19104927778244 - val Loss: 1.2754847258329391\n",
      "train loss: 4.190375864505768 - val Loss: 1.2753063291311264\n",
      "train loss: 4.1897242069244385 - val Loss: 1.2751116901636124\n",
      "train loss: 4.189030855894089 - val Loss: 1.2748694121837616\n",
      "train loss: 4.188397765159607 - val Loss: 1.2746595442295074\n",
      "train loss: 4.187705814838409 - val Loss: 1.2744606733322144\n",
      "train loss: 4.187058374285698 - val Loss: 1.2743146568536758\n",
      "train loss: 4.186382278800011 - val Loss: 1.2740663141012192\n",
      "train loss: 4.185764417052269 - val Loss: 1.2738212198019028\n",
      "train loss: 4.185056462883949 - val Loss: 1.2736225128173828\n",
      "train loss: 4.1845792680978775 - val Loss: 1.2734688818454742\n",
      "train loss: 4.183923646807671 - val Loss: 1.273335948586464\n",
      "train loss: 4.183238133788109 - val Loss: 1.273097738623619\n",
      "train loss: 4.182563289999962 - val Loss: 1.272931084036827\n",
      "train loss: 4.181978821754456 - val Loss: 1.2727749794721603\n",
      "train loss: 4.1813298016786575 - val Loss: 1.2726467996835709\n",
      "train loss: 4.1805519461631775 - val Loss: 1.2724710553884506\n",
      "train loss: 4.179915398359299 - val Loss: 1.2723504900932312\n",
      "train loss: 4.179214537143707 - val Loss: 1.2721902579069138\n",
      "train loss: 4.178608775138855 - val Loss: 1.2719466537237167\n",
      "train loss: 4.177912414073944 - val Loss: 1.2718016356229782\n",
      "train loss: 4.177320688962936 - val Loss: 1.2715616673231125\n",
      "train loss: 4.176640659570694 - val Loss: 1.2713695913553238\n",
      "train loss: 4.1759741604328156 - val Loss: 1.2712289839982986\n",
      "train loss: 4.175388157367706 - val Loss: 1.2710412442684174\n",
      "train loss: 4.174739807844162 - val Loss: 1.270912230014801\n",
      "train loss: 4.174046635627747 - val Loss: 1.2706844210624695\n",
      "train loss: 4.173451319336891 - val Loss: 1.2704933434724808\n",
      "train loss: 4.172816231846809 - val Loss: 1.270302638411522\n",
      "train loss: 4.172150298953056 - val Loss: 1.2697838842868805\n",
      "train loss: 4.171515807509422 - val Loss: 1.2695672512054443\n",
      "train loss: 4.170900970697403 - val Loss: 1.2693356722593307\n",
      "train loss: 4.170272424817085 - val Loss: 1.269119754433632\n",
      "train loss: 4.169678673148155 - val Loss: 1.269166886806488\n",
      "train loss: 4.169019386172295 - val Loss: 1.2689829766750336\n",
      "train loss: 4.168422639369965 - val Loss: 1.268532931804657\n",
      "train loss: 4.167522177100182 - val Loss: 1.2682860791683197\n",
      "train loss: 4.167050689458847 - val Loss: 1.2681300193071365\n",
      "train loss: 4.166391149163246 - val Loss: 1.2679243236780167\n",
      "train loss: 4.165760308504105 - val Loss: 1.2677378803491592\n",
      "train loss: 4.165011778473854 - val Loss: 1.2674742490053177\n",
      "train loss: 4.164305672049522 - val Loss: 1.2672380656003952\n",
      "train loss: 4.163667559623718 - val Loss: 1.2672908753156662\n",
      "train loss: 4.1630701422691345 - val Loss: 1.2668657153844833\n",
      "train loss: 4.162330582737923 - val Loss: 1.2669526487588882\n",
      "train loss: 4.161708056926727 - val Loss: 1.2667504400014877\n",
      "train loss: 4.161111995577812 - val Loss: 1.2665686309337616\n",
      "train loss: 4.160448431968689 - val Loss: 1.266429677605629\n",
      "train loss: 4.159799441695213 - val Loss: 1.2660046815872192\n",
      "train loss: 4.159084498882294 - val Loss: 1.2659494280815125\n",
      "train loss: 4.158407628536224 - val Loss: 1.2659398317337036\n",
      "train loss: 4.157800242304802 - val Loss: 1.2656345665454865\n",
      "train loss: 4.157277092337608 - val Loss: 1.2654829323291779\n",
      "train loss: 4.1566353142261505 - val Loss: 1.2652838081121445\n",
      "train loss: 4.156057074666023 - val Loss: 1.2651381492614746\n",
      "train loss: 4.155407562851906 - val Loss: 1.2650043964385986\n",
      "train loss: 4.154823526740074 - val Loss: 1.2647867500782013\n",
      "train loss: 4.154217183589935 - val Loss: 1.2646145820617676\n",
      "train loss: 4.1536098420619965 - val Loss: 1.2644028812646866\n",
      "train loss: 4.1530014127492905 - val Loss: 1.2641918808221817\n",
      "train loss: 4.152201250195503 - val Loss: 1.2641206234693527\n",
      "train loss: 4.1516235917806625 - val Loss: 1.2639321237802505\n",
      "train loss: 4.150984138250351 - val Loss: 1.263750046491623\n",
      "train loss: 4.150394186377525 - val Loss: 1.2635543793439865\n",
      "train loss: 4.149786219000816 - val Loss: 1.2633678019046783\n",
      "train loss: 4.14911675453186 - val Loss: 1.2631766051054\n",
      "train loss: 4.148438259959221 - val Loss: 1.263099491596222\n",
      "train loss: 4.147802412509918 - val Loss: 1.2629327774047852\n",
      "train loss: 4.1472282111644745 - val Loss: 1.2627399563789368\n",
      "train loss: 4.1466028690338135 - val Loss: 1.2625356018543243\n",
      "train loss: 4.14596951007843 - val Loss: 1.2623746693134308\n",
      "train loss: 4.145361572504044 - val Loss: 1.262296199798584\n",
      "train loss: 4.144744887948036 - val Loss: 1.26210056245327\n",
      "train loss: 4.144143313169479 - val Loss: 1.2619003802537918\n",
      "train loss: 4.1436043083667755 - val Loss: 1.2617505937814713\n",
      "train loss: 4.142923504114151 - val Loss: 1.2615266293287277\n",
      "train loss: 4.142392575740814 - val Loss: 1.261370599269867\n",
      "train loss: 4.141782060265541 - val Loss: 1.2611690908670425\n",
      "train loss: 4.141198813915253 - val Loss: 1.2609514445066452\n",
      "train loss: 4.1406567096710205 - val Loss: 1.2607505917549133\n",
      "train loss: 4.140045091509819 - val Loss: 1.2606691718101501\n",
      "train loss: 4.139377683401108 - val Loss: 1.260319858789444\n",
      "train loss: 4.138817057013512 - val Loss: 1.2603508085012436\n",
      "train loss: 4.138182386755943 - val Loss: 1.2599883675575256\n",
      "train loss: 4.137632861733437 - val Loss: 1.2600271999835968\n",
      "train loss: 4.1369476318359375 - val Loss: 1.259651467204094\n",
      "train loss: 4.136402755975723 - val Loss: 1.259671911597252\n",
      "train loss: 4.135821148753166 - val Loss: 1.2593016028404236\n",
      "train loss: 4.135088950395584 - val Loss: 1.2591954469680786\n",
      "train loss: 4.1347024738788605 - val Loss: 1.2595026046037674\n",
      "train loss: 4.1341598480939865 - val Loss: 1.259532243013382\n",
      "train loss: 4.1336550116539 - val Loss: 1.2589557021856308\n",
      "train loss: 4.133030101656914 - val Loss: 1.2590584009885788\n",
      "train loss: 4.132443785667419 - val Loss: 1.2585371732711792\n",
      "train loss: 4.131849750876427 - val Loss: 1.2587317824363708\n",
      "train loss: 4.131253883242607 - val Loss: 1.258122518658638\n",
      "train loss: 4.130587786436081 - val Loss: 1.258508250117302\n",
      "train loss: 4.13006104528904 - val Loss: 1.2582154422998428\n",
      "train loss: 4.129361122846603 - val Loss: 1.258154183626175\n",
      "train loss: 4.128777459263802 - val Loss: 1.2578894346952438\n",
      "train loss: 4.128162577748299 - val Loss: 1.2578787356615067\n",
      "train loss: 4.127674877643585 - val Loss: 1.2578021883964539\n",
      "train loss: 4.127071753144264 - val Loss: 1.2575224936008453\n",
      "train loss: 4.1264563500881195 - val Loss: 1.257490649819374\n",
      "train loss: 4.125855311751366 - val Loss: 1.2572752386331558\n",
      "train loss: 4.125315070152283 - val Loss: 1.2570680677890778\n",
      "train loss: 4.12467759847641 - val Loss: 1.2568873018026352\n",
      "train loss: 4.124081522226334 - val Loss: 1.2567259520292282\n",
      "train loss: 4.12337863445282 - val Loss: 1.2566459625959396\n",
      "train loss: 4.122812703251839 - val Loss: 1.256511703133583\n",
      "train loss: 4.122174441814423 - val Loss: 1.2563282400369644\n",
      "train loss: 4.1216165870428085 - val Loss: 1.256199762225151\n",
      "train loss: 4.121015354990959 - val Loss: 1.256020039319992\n",
      "train loss: 4.120416387915611 - val Loss: 1.2558088600635529\n",
      "train loss: 4.1197962909936905 - val Loss: 1.2555980533361435\n",
      "train loss: 4.119257986545563 - val Loss: 1.2554131150245667\n",
      "train loss: 4.1186515390872955 - val Loss: 1.2553312927484512\n",
      "train loss: 4.118104189634323 - val Loss: 1.2551517933607101\n",
      "train loss: 4.117535158991814 - val Loss: 1.2548607885837555\n",
      "train loss: 4.116816386580467 - val Loss: 1.254731222987175\n",
      "train loss: 4.116175398230553 - val Loss: 1.2545526325702667\n",
      "train loss: 4.115593358874321 - val Loss: 1.2543734163045883\n",
      "train loss: 4.114983648061752 - val Loss: 1.2542380541563034\n",
      "train loss: 4.114447727799416 - val Loss: 1.2541412264108658\n",
      "train loss: 4.113882347941399 - val Loss: 1.2540211081504822\n",
      "train loss: 4.1132480055093765 - val Loss: 1.253898486495018\n",
      "train loss: 4.1127239018678665 - val Loss: 1.2536584436893463\n",
      "train loss: 4.112041607499123 - val Loss: 1.2534757852554321\n",
      "train loss: 4.111467346549034 - val Loss: 1.2533578127622604\n",
      "train loss: 4.110883489251137 - val Loss: 1.2531895488500595\n",
      "train loss: 4.110331475734711 - val Loss: 1.2530265301465988\n",
      "train loss: 4.109772190451622 - val Loss: 1.252816066145897\n",
      "train loss: 4.109192043542862 - val Loss: 1.2526901811361313\n",
      "train loss: 4.1086361557245255 - val Loss: 1.2525306940078735\n",
      "train loss: 4.1082092970609665 - val Loss: 1.251430705189705\n",
      "train loss: 4.1075296103954315 - val Loss: 1.2513227760791779\n",
      "train loss: 4.1069929748773575 - val Loss: 1.2513315081596375\n",
      "train loss: 4.106385067105293 - val Loss: 1.2511047422885895\n",
      "train loss: 4.105649217963219 - val Loss: 1.2507304251194\n",
      "train loss: 4.1052354872226715 - val Loss: 1.2507397532463074\n",
      "train loss: 4.104443460702896 - val Loss: 1.2503892183303833\n",
      "train loss: 4.103943154215813 - val Loss: 1.250208541750908\n",
      "train loss: 4.103343188762665 - val Loss: 1.2500899136066437\n",
      "train loss: 4.102797299623489 - val Loss: 1.249960482120514\n",
      "train loss: 4.102265387773514 - val Loss: 1.2498827129602432\n",
      "train loss: 4.101714998483658 - val Loss: 1.2497219443321228\n",
      "train loss: 4.101146638393402 - val Loss: 1.2496146708726883\n",
      "train loss: 4.100622445344925 - val Loss: 1.2494367212057114\n",
      "train loss: 4.099967300891876 - val Loss: 1.2492752373218536\n",
      "train loss: 4.099391981959343 - val Loss: 1.2491201609373093\n",
      "train loss: 4.098802104592323 - val Loss: 1.2490132749080658\n",
      "train loss: 4.098244145512581 - val Loss: 1.2488704770803452\n",
      "train loss: 4.097660705447197 - val Loss: 1.24872425198555\n",
      "train loss: 4.097108259797096 - val Loss: 1.2485364824533463\n",
      "train loss: 4.0965601950883865 - val Loss: 1.2483464479446411\n",
      "train loss: 4.095988690853119 - val Loss: 1.2482233345508575\n",
      "train loss: 4.095346301794052 - val Loss: 1.2480655312538147\n",
      "train loss: 4.094671353697777 - val Loss: 1.247984603047371\n",
      "train loss: 4.094184786081314 - val Loss: 1.2478755861520767\n",
      "train loss: 4.093548223376274 - val Loss: 1.247724488377571\n",
      "train loss: 4.09298712015152 - val Loss: 1.2475716918706894\n",
      "train loss: 4.092427313327789 - val Loss: 1.2473793178796768\n",
      "train loss: 4.091802269220352 - val Loss: 1.2472152560949326\n",
      "train loss: 4.0913417637348175 - val Loss: 1.2471122294664383\n",
      "train loss: 4.090669795870781 - val Loss: 1.2469448298215866\n",
      "train loss: 4.090202271938324 - val Loss: 1.246803343296051\n",
      "train loss: 4.089614778757095 - val Loss: 1.2466676235198975\n",
      "train loss: 4.089060336351395 - val Loss: 1.2464994043111801\n",
      "train loss: 4.088480517268181 - val Loss: 1.24632328748703\n",
      "train loss: 4.087897896766663 - val Loss: 1.2462358325719833\n",
      "train loss: 4.087357312440872 - val Loss: 1.2460346668958664\n",
      "train loss: 4.0867078602313995 - val Loss: 1.2459310740232468\n",
      "train loss: 4.0862026065588 - val Loss: 1.2457712441682816\n",
      "train loss: 4.085631176829338 - val Loss: 1.2456211596727371\n",
      "train loss: 4.085113272070885 - val Loss: 1.2454979568719864\n",
      "train loss: 4.084536299109459 - val Loss: 1.2453204542398453\n",
      "train loss: 4.083974257111549 - val Loss: 1.245198830962181\n",
      "train loss: 4.083406031131744 - val Loss: 1.2450667321681976\n",
      "train loss: 4.082880929112434 - val Loss: 1.2448839843273163\n",
      "train loss: 4.082317978143692 - val Loss: 1.2447342723608017\n",
      "train loss: 4.081881269812584 - val Loss: 1.244690865278244\n",
      "train loss: 4.081333890557289 - val Loss: 1.244527667760849\n",
      "train loss: 4.080620646476746 - val Loss: 1.2444137185811996\n",
      "train loss: 4.080062299966812 - val Loss: 1.2442805171012878\n",
      "train loss: 4.0795731991529465 - val Loss: 1.244203895330429\n",
      "train loss: 4.079002320766449 - val Loss: 1.244035080075264\n",
      "train loss: 4.078450188040733 - val Loss: 1.243879646062851\n",
      "train loss: 4.077892020344734 - val Loss: 1.2438363283872604\n",
      "train loss: 4.0773495733737946 - val Loss: 1.243592843413353\n",
      "train loss: 4.076759651303291 - val Loss: 1.243447870016098\n",
      "train loss: 4.076257362961769 - val Loss: 1.2432857602834702\n",
      "train loss: 4.075647279620171 - val Loss: 1.2431227415800095\n",
      "train loss: 4.075111821293831 - val Loss: 1.243050530552864\n",
      "train loss: 4.074573680758476 - val Loss: 1.2429173588752747\n",
      "train loss: 4.074024021625519 - val Loss: 1.2428246438503265\n",
      "train loss: 4.07345075905323 - val Loss: 1.2426287680864334\n",
      "train loss: 4.072941482067108 - val Loss: 1.2424673736095428\n",
      "train loss: 4.072390258312225 - val Loss: 1.2422506660223007\n",
      "train loss: 4.071848839521408 - val Loss: 1.2421128004789352\n",
      "train loss: 4.071265026926994 - val Loss: 1.241992563009262\n",
      "train loss: 4.070768356323242 - val Loss: 1.2418585121631622\n",
      "train loss: 4.070218235254288 - val Loss: 1.2417462319135666\n",
      "train loss: 4.069657891988754 - val Loss: 1.241661697626114\n",
      "train loss: 4.069084152579308 - val Loss: 1.241465613245964\n",
      "train loss: 4.068587243556976 - val Loss: 1.2413165420293808\n",
      "train loss: 4.06806118786335 - val Loss: 1.2411682456731796\n",
      "train loss: 4.067541614174843 - val Loss: 1.2410172671079636\n",
      "train loss: 4.0669766664505005 - val Loss: 1.2408826649188995\n",
      "train loss: 4.06645929813385 - val Loss: 1.2408238649368286\n",
      "train loss: 4.065895766019821 - val Loss: 1.2406900823116302\n",
      "train loss: 4.065283566713333 - val Loss: 1.2405368834733963\n",
      "train loss: 4.0647009164094925 - val Loss: 1.2404226511716843\n",
      "train loss: 4.064180836081505 - val Loss: 1.2403282821178436\n",
      "train loss: 4.063523471355438 - val Loss: 1.2401051223278046\n",
      "train loss: 4.062940135598183 - val Loss: 1.2399629205465317\n",
      "train loss: 4.062376648187637 - val Loss: 1.2397855818271637\n",
      "train loss: 4.061814218759537 - val Loss: 1.2396444529294968\n",
      "train loss: 4.0612790286540985 - val Loss: 1.2395123094320297\n",
      "train loss: 4.060755804181099 - val Loss: 1.2393959164619446\n",
      "train loss: 4.060213714838028 - val Loss: 1.2392441928386688\n",
      "train loss: 4.059647157788277 - val Loss: 1.2392323166131973\n",
      "train loss: 4.059201747179031 - val Loss: 1.239108756184578\n",
      "train loss: 4.0586444437503815 - val Loss: 1.2388980686664581\n",
      "train loss: 4.058118686079979 - val Loss: 1.2387579530477524\n",
      "train loss: 4.057587549090385 - val Loss: 1.238635778427124\n",
      "train loss: 4.057077810168266 - val Loss: 1.2384956777095795\n",
      "train loss: 4.0565662533044815 - val Loss: 1.2383743971586227\n",
      "train loss: 4.056029811501503 - val Loss: 1.2382211983203888\n",
      "train loss: 4.055517867207527 - val Loss: 1.2381222397089005\n",
      "train loss: 4.05508029460907 - val Loss: 1.238050252199173\n",
      "train loss: 4.054465875029564 - val Loss: 1.2379654049873352\n",
      "train loss: 4.05397430062294 - val Loss: 1.2377878576517105\n",
      "train loss: 4.053436487913132 - val Loss: 1.2381969541311264\n",
      "train loss: 4.052964746952057 - val Loss: 1.238116130232811\n",
      "train loss: 4.052566587924957 - val Loss: 1.2380009442567825\n",
      "train loss: 4.052035868167877 - val Loss: 1.2371645271778107\n",
      "train loss: 4.051404282450676 - val Loss: 1.237643226981163\n",
      "train loss: 4.050958335399628 - val Loss: 1.2374265044927597\n",
      "train loss: 4.050397142767906 - val Loss: 1.2371997684240341\n",
      "train loss: 4.049870982766151 - val Loss: 1.2370059192180634\n",
      "train loss: 4.049383357167244 - val Loss: 1.2368906438350677\n",
      "train loss: 4.048868238925934 - val Loss: 1.2367189228534698\n",
      "train loss: 4.0483706295490265 - val Loss: 1.2365998476743698\n",
      "train loss: 4.047851204872131 - val Loss: 1.2364502549171448\n",
      "train loss: 4.047345399856567 - val Loss: 1.236352264881134\n",
      "train loss: 4.046887636184692 - val Loss: 1.2361738681793213\n",
      "train loss: 4.046396791934967 - val Loss: 1.2360433787107468\n",
      "train loss: 4.0458551198244095 - val Loss: 1.2358723133802414\n",
      "train loss: 4.045257478952408 - val Loss: 1.2358211874961853\n",
      "train loss: 4.044763192534447 - val Loss: 1.2356770932674408\n",
      "train loss: 4.0442709773778915 - val Loss: 1.2355912923812866\n",
      "train loss: 4.043727234005928 - val Loss: 1.2353923171758652\n",
      "train loss: 4.043175712227821 - val Loss: 1.2351901531219482\n",
      "train loss: 4.042635306715965 - val Loss: 1.235056534409523\n",
      "train loss: 4.042184814810753 - val Loss: 1.234928458929062\n",
      "train loss: 4.041716903448105 - val Loss: 1.2348937541246414\n",
      "train loss: 4.041192814707756 - val Loss: 1.234726756811142\n",
      "train loss: 4.040657535195351 - val Loss: 1.23466357588768\n",
      "train loss: 4.040169894695282 - val Loss: 1.2345507591962814\n",
      "train loss: 4.0396794229745865 - val Loss: 1.2344638258218765\n",
      "train loss: 4.0392322689294815 - val Loss: 1.2343196123838425\n",
      "train loss: 4.038697227835655 - val Loss: 1.2341021597385406\n",
      "train loss: 4.038193941116333 - val Loss: 1.2340290695428848\n",
      "train loss: 4.037834212183952 - val Loss: 1.2338436841964722\n",
      "train loss: 4.037265360355377 - val Loss: 1.2335944175720215\n",
      "train loss: 4.036746263504028 - val Loss: 1.2333678752183914\n",
      "train loss: 4.036113858222961 - val Loss: 1.2331847846508026\n",
      "train loss: 4.035630330443382 - val Loss: 1.2330301105976105\n",
      "train loss: 4.035129576921463 - val Loss: 1.232863113284111\n",
      "train loss: 4.034641236066818 - val Loss: 1.232733130455017\n",
      "train loss: 4.034116134047508 - val Loss: 1.2325751334428787\n",
      "train loss: 4.033660635352135 - val Loss: 1.2324661314487457\n",
      "train loss: 4.033128380775452 - val Loss: 1.232316717505455\n",
      "train loss: 4.032623201608658 - val Loss: 1.2321980744600296\n",
      "train loss: 4.032134801149368 - val Loss: 1.232050135731697\n",
      "train loss: 4.031636998057365 - val Loss: 1.231952726840973\n",
      "train loss: 4.0312997698783875 - val Loss: 1.2319130152463913\n",
      "train loss: 4.030736207962036 - val Loss: 1.2318301796913147\n",
      "train loss: 4.030344054102898 - val Loss: 1.231637865304947\n",
      "train loss: 4.029777094721794 - val Loss: 1.2314821928739548\n",
      "train loss: 4.029308944940567 - val Loss: 1.231306105852127\n",
      "train loss: 4.028809815645218 - val Loss: 1.2311851680278778\n",
      "train loss: 4.028263077139854 - val Loss: 1.2310372740030289\n",
      "train loss: 4.0277834087610245 - val Loss: 1.2305980175733566\n",
      "train loss: 4.027194544672966 - val Loss: 1.2304601222276688\n",
      "train loss: 4.026673823595047 - val Loss: 1.2294214516878128\n",
      "train loss: 4.025845631957054 - val Loss: 1.2293353825807571\n",
      "train loss: 4.025368645787239 - val Loss: 1.2292022556066513\n",
      "train loss: 4.024855434894562 - val Loss: 1.2290841341018677\n",
      "train loss: 4.0243513733148575 - val Loss: 1.2289268970489502\n",
      "train loss: 4.023822650313377 - val Loss: 1.2287458926439285\n",
      "train loss: 4.023314759135246 - val Loss: 1.2285553365945816\n",
      "train loss: 4.022776395082474 - val Loss: 1.228416234254837\n",
      "train loss: 4.022238805890083 - val Loss: 1.228220596909523\n",
      "train loss: 4.021758422255516 - val Loss: 1.2280095517635345\n",
      "train loss: 4.0212347358465195 - val Loss: 1.227772369980812\n",
      "train loss: 4.020714685320854 - val Loss: 1.2276850789785385\n",
      "train loss: 4.020196199417114 - val Loss: 1.2275353223085403\n",
      "train loss: 4.019770547747612 - val Loss: 1.2273857593536377\n",
      "train loss: 4.019188478589058 - val Loss: 1.2269480377435684\n",
      "train loss: 4.018688380718231 - val Loss: 1.226805031299591\n",
      "train loss: 4.018300354480743 - val Loss: 1.2267120033502579\n",
      "train loss: 4.017869740724564 - val Loss: 1.226523756980896\n",
      "train loss: 4.017341792583466 - val Loss: 1.2263290733098984\n",
      "train loss: 4.016892611980438 - val Loss: 1.2261619418859482\n",
      "train loss: 4.016453802585602 - val Loss: 1.2259870022535324\n",
      "train loss: 4.01590770483017 - val Loss: 1.2258536964654922\n",
      "train loss: 4.015505686402321 - val Loss: 1.2257471531629562\n",
      "train loss: 4.015034332871437 - val Loss: 1.2256058156490326\n",
      "train loss: 4.014615789055824 - val Loss: 1.225488156080246\n",
      "train loss: 4.014099344611168 - val Loss: 1.2253261655569077\n",
      "train loss: 4.013655468821526 - val Loss: 1.22587850689888\n",
      "train loss: 4.0134032517671585 - val Loss: 1.2250512838363647\n",
      "train loss: 4.012701660394669 - val Loss: 1.2255285680294037\n",
      "train loss: 4.012488350272179 - val Loss: 1.2247457206249237\n",
      "train loss: 4.01192270219326 - val Loss: 1.2253020107746124\n",
      "train loss: 4.011615827679634 - val Loss: 1.2251768559217453\n",
      "train loss: 4.011044561862946 - val Loss: 1.224839225411415\n",
      "train loss: 4.010580494999886 - val Loss: 1.2247982025146484\n",
      "train loss: 4.010085076093674 - val Loss: 1.2246317118406296\n",
      "train loss: 4.009714424610138 - val Loss: 1.2246140390634537\n",
      "train loss: 4.009301945567131 - val Loss: 1.2245049476623535\n",
      "train loss: 4.008805721998215 - val Loss: 1.2243099808692932\n",
      "train loss: 4.008383333683014 - val Loss: 1.2242948859930038\n",
      "train loss: 4.007909283041954 - val Loss: 1.224182277917862\n",
      "train loss: 4.007434904575348 - val Loss: 1.22405344247818\n",
      "train loss: 4.006955534219742 - val Loss: 1.2239204943180084\n",
      "train loss: 4.0064481645822525 - val Loss: 1.2236523628234863\n",
      "train loss: 4.006112664937973 - val Loss: 1.2236910909414291\n",
      "train loss: 4.005711928009987 - val Loss: 1.223579227924347\n",
      "train loss: 4.005211502313614 - val Loss: 1.2235225290060043\n",
      "train loss: 4.0047624707221985 - val Loss: 1.2232311367988586\n",
      "train loss: 4.004292130470276 - val Loss: 1.2231670022010803\n",
      "train loss: 4.003891333937645 - val Loss: 1.223205253481865\n",
      "train loss: 4.003535985946655 - val Loss: 1.2230536192655563\n",
      "train loss: 4.003004848957062 - val Loss: 1.2230441719293594\n",
      "train loss: 4.002533242106438 - val Loss: 1.2230117470026016\n",
      "train loss: 4.002022922039032 - val Loss: 1.2228540927171707\n",
      "train loss: 4.001562982797623 - val Loss: 1.2227826118469238\n",
      "train loss: 4.001013278961182 - val Loss: 1.2226202189922333\n",
      "train loss: 4.000583112239838 - val Loss: 1.2225370556116104\n",
      "train loss: 4.000075712800026 - val Loss: 1.2224070578813553\n",
      "train loss: 3.999656707048416 - val Loss: 1.222349852323532\n",
      "train loss: 3.999157041311264 - val Loss: 1.2221885323524475\n",
      "train loss: 3.998687908053398 - val Loss: 1.2220942825078964\n",
      "train loss: 3.998157277703285 - val Loss: 1.221934050321579\n",
      "train loss: 3.997716650366783 - val Loss: 1.2218058407306671\n",
      "train loss: 3.9973060339689255 - val Loss: 1.221785083413124\n",
      "train loss: 3.996744140982628 - val Loss: 1.221670776605606\n",
      "train loss: 3.9962894171476364 - val Loss: 1.2215648740530014\n",
      "train loss: 3.9958206862211227 - val Loss: 1.2215359956026077\n",
      "train loss: 3.995406687259674 - val Loss: 1.2214171290397644\n",
      "train loss: 3.99492147564888 - val Loss: 1.2212816625833511\n",
      "train loss: 3.994472250342369 - val Loss: 1.2211918532848358\n",
      "train loss: 3.9939653873443604 - val Loss: 1.221079260110855\n",
      "train loss: 3.993565082550049 - val Loss: 1.2209541946649551\n",
      "train loss: 3.9930923730134964 - val Loss: 1.2208164483308792\n",
      "train loss: 3.9926054626703262 - val Loss: 1.2207759022712708\n",
      "train loss: 3.992235094308853 - val Loss: 1.2206348031759262\n",
      "train loss: 3.991774782538414 - val Loss: 1.2205272614955902\n",
      "train loss: 3.991400122642517 - val Loss: 1.220464825630188\n",
      "train loss: 3.9909191578626633 - val Loss: 1.2203114479780197\n",
      "train loss: 3.9904966205358505 - val Loss: 1.2201859951019287\n",
      "train loss: 3.99001145362854 - val Loss: 1.2200662642717361\n",
      "train loss: 3.9896541088819504 - val Loss: 1.2198389768600464\n",
      "train loss: 3.9891543984413147 - val Loss: 1.2198369204998016\n",
      "train loss: 3.9887538254261017 - val Loss: 1.2197089344263077\n",
      "train loss: 3.988286003470421 - val Loss: 1.2194949239492416\n",
      "train loss: 3.9878681302070618 - val Loss: 1.2194534987211227\n",
      "train loss: 3.9873615205287933 - val Loss: 1.2193036377429962\n",
      "train loss: 3.9869267493486404 - val Loss: 1.219082847237587\n",
      "train loss: 3.986513525247574 - val Loss: 1.219090610742569\n",
      "train loss: 3.9860713183879852 - val Loss: 1.2189145684242249\n",
      "train loss: 3.9855819046497345 - val Loss: 1.2188370376825333\n",
      "train loss: 3.9851236790418625 - val Loss: 1.218755528330803\n",
      "train loss: 3.9846585988998413 - val Loss: 1.2185132950544357\n",
      "train loss: 3.984219044446945 - val Loss: 1.218549206852913\n",
      "train loss: 3.9838603287935257 - val Loss: 1.2184194177389145\n",
      "train loss: 3.983435019850731 - val Loss: 1.2183161675930023\n",
      "train loss: 3.982967108488083 - val Loss: 1.218201071023941\n",
      "train loss: 3.982513412833214 - val Loss: 1.2181289941072464\n",
      "train loss: 3.982276976108551 - val Loss: 1.2180815637111664\n",
      "train loss: 3.9819733649492264 - val Loss: 1.2186823040246964\n",
      "train loss: 3.9817006587982178 - val Loss: 1.2184509187936783\n",
      "train loss: 3.981261432170868 - val Loss: 1.2182949930429459\n",
      "train loss: 3.980764791369438 - val Loss: 1.2181496024131775\n",
      "train loss: 3.9802879095077515 - val Loss: 1.2180365771055222\n",
      "train loss: 3.9798754155635834 - val Loss: 1.2180385887622833\n",
      "train loss: 3.97944638133049 - val Loss: 1.217809185385704\n",
      "train loss: 3.9790503829717636 - val Loss: 1.2176772952079773\n",
      "train loss: 3.978688210248947 - val Loss: 1.2177223563194275\n",
      "train loss: 3.9784139841794968 - val Loss: 1.2175011038780212\n",
      "train loss: 3.977975830435753 - val Loss: 1.2176134139299393\n",
      "train loss: 3.9776387363672256 - val Loss: 1.218706876039505\n",
      "train loss: 3.977387949824333 - val Loss: 1.2184665203094482\n",
      "train loss: 3.976935639977455 - val Loss: 1.218350663781166\n",
      "train loss: 3.9764596968889236 - val Loss: 1.2182110399007797\n",
      "train loss: 3.975981667637825 - val Loss: 1.2180169820785522\n",
      "train loss: 3.9755537509918213 - val Loss: 1.2178492546081543\n",
      "train loss: 3.975020185112953 - val Loss: 1.2177108973264694\n",
      "train loss: 3.9745411425828934 - val Loss: 1.2176432758569717\n",
      "train loss: 3.974016025662422 - val Loss: 1.2175252139568329\n",
      "train loss: 3.97356878221035 - val Loss: 1.2172584384679794\n",
      "train loss: 3.973089337348938 - val Loss: 1.217152550816536\n",
      "train loss: 3.9727692157030106 - val Loss: 1.2172960937023163\n",
      "train loss: 3.9724215865135193 - val Loss: 1.2171388417482376\n",
      "train loss: 3.9719369411468506 - val Loss: 1.2170623391866684\n",
      "train loss: 3.971497103571892 - val Loss: 1.2168387174606323\n",
      "train loss: 3.971081167459488 - val Loss: 1.2167887389659882\n",
      "train loss: 3.9706293493509293 - val Loss: 1.2166223675012589\n",
      "train loss: 3.9700957536697388 - val Loss: 1.2164234220981598\n",
      "train loss: 3.96968013048172 - val Loss: 1.2163599878549576\n",
      "train loss: 3.9692112505435944 - val Loss: 1.216221034526825\n",
      "train loss: 3.968800589442253 - val Loss: 1.2160269618034363\n",
      "train loss: 3.96831713616848 - val Loss: 1.2159326672554016\n",
      "train loss: 3.9679062217473984 - val Loss: 1.2158187329769135\n",
      "train loss: 3.9674240052700043 - val Loss: 1.2155932635068893\n",
      "train loss: 3.966952309012413 - val Loss: 1.2153317630290985\n",
      "train loss: 3.966434493660927 - val Loss: 1.2153027206659317\n",
      "train loss: 3.9660200774669647 - val Loss: 1.2150119990110397\n",
      "train loss: 3.965584844350815 - val Loss: 1.2157982736825943\n",
      "train loss: 3.9652779698371887 - val Loss: 1.2154599577188492\n",
      "train loss: 3.9647021889686584 - val Loss: 1.215470403432846\n",
      "train loss: 3.9642841666936874 - val Loss: 1.21511510014534\n",
      "train loss: 3.963772550225258 - val Loss: 1.2151652127504349\n",
      "train loss: 3.963400050997734 - val Loss: 1.2150148749351501\n",
      "train loss: 3.962934449315071 - val Loss: 1.2143376469612122\n",
      "train loss: 3.9623833894729614 - val Loss: 1.2144012153148651\n",
      "train loss: 3.9620427936315536 - val Loss: 1.2140676826238632\n",
      "train loss: 3.9615458250045776 - val Loss: 1.214133307337761\n",
      "train loss: 3.9611252695322037 - val Loss: 1.2137536257505417\n",
      "train loss: 3.960586905479431 - val Loss: 1.2138284742832184\n",
      "train loss: 3.960172489285469 - val Loss: 1.213478460907936\n",
      "train loss: 3.9597040563821793 - val Loss: 1.2134435921907425\n",
      "train loss: 3.959307998418808 - val Loss: 1.213059812784195\n",
      "train loss: 3.9588117003440857 - val Loss: 1.2130213230848312\n",
      "train loss: 3.9583872109651566 - val Loss: 1.2126215398311615\n",
      "train loss: 3.9578383415937424 - val Loss: 1.2127861380577087\n",
      "train loss: 3.9574279487133026 - val Loss: 1.2124585509300232\n",
      "train loss: 3.9569809436798096 - val Loss: 1.2124858051538467\n",
      "train loss: 3.9566823542118073 - val Loss: 1.212124153971672\n",
      "train loss: 3.956099435687065 - val Loss: 1.2121224254369736\n",
      "train loss: 3.9558146744966507 - val Loss: 1.2117824405431747\n",
      "train loss: 3.955208942294121 - val Loss: 1.2119186073541641\n",
      "train loss: 3.954909935593605 - val Loss: 1.2115459740161896\n",
      "train loss: 3.9542857706546783 - val Loss: 1.2116007208824158\n",
      "train loss: 3.953891396522522 - val Loss: 1.2112696319818497\n",
      "train loss: 3.9532484859228134 - val Loss: 1.2112655192613602\n",
      "train loss: 3.9527888149023056 - val Loss: 1.2111345827579498\n",
      "train loss: 3.9525508731603622 - val Loss: 1.210912674665451\n",
      "train loss: 3.9518178701400757 - val Loss: 1.2109277546405792\n",
      "train loss: 3.9515632539987564 - val Loss: 1.210621401667595\n",
      "train loss: 3.950953021645546 - val Loss: 1.21064992249012\n",
      "train loss: 3.9507241249084473 - val Loss: 1.2103234976530075\n",
      "train loss: 3.950130984187126 - val Loss: 1.2103594541549683\n",
      "train loss: 3.9498223811388016 - val Loss: 1.2100216150283813\n",
      "train loss: 3.9490878134965897 - val Loss: 1.2101564407348633\n",
      "train loss: 3.948916956782341 - val Loss: 1.2097856849431992\n",
      "train loss: 3.94819276034832 - val Loss: 1.2098682671785355\n",
      "train loss: 3.9479975551366806 - val Loss: 1.2095289081335068\n",
      "train loss: 3.9473746567964554 - val Loss: 1.209578961133957\n",
      "train loss: 3.9470116794109344 - val Loss: 1.2093518525362015\n",
      "train loss: 3.9465045481920242 - val Loss: 1.2094588577747345\n",
      "train loss: 3.946084573864937 - val Loss: 1.2091854065656662\n",
      "train loss: 3.9456031024456024 - val Loss: 1.209117665886879\n",
      "train loss: 3.9452321976423264 - val Loss: 1.2088662832975388\n",
      "train loss: 3.944704622030258 - val Loss: 1.208818644285202\n",
      "train loss: 3.944303572177887 - val Loss: 1.2086619138717651\n",
      "train loss: 3.9439137279987335 - val Loss: 1.2085740715265274\n",
      "train loss: 3.9434393793344498 - val Loss: 1.2084947675466537\n",
      "train loss: 3.9430011957883835 - val Loss: 1.2081024944782257\n",
      "train loss: 3.9426345378160477 - val Loss: 1.2082206159830093\n",
      "train loss: 3.9422281831502914 - val Loss: 1.208017736673355\n",
      "train loss: 3.9417223781347275 - val Loss: 1.2077379822731018\n",
      "train loss: 3.941285789012909 - val Loss: 1.2078291326761246\n",
      "train loss: 3.940920978784561 - val Loss: 1.2074982225894928\n",
      "train loss: 3.9404370337724686 - val Loss: 1.2076456844806671\n",
      "train loss: 3.9401213377714157 - val Loss: 1.207320511341095\n",
      "train loss: 3.939537450671196 - val Loss: 1.2073577791452408\n",
      "train loss: 3.939157545566559 - val Loss: 1.207074984908104\n",
      "train loss: 3.9385416507720947 - val Loss: 1.2071593403816223\n",
      "train loss: 3.938203901052475 - val Loss: 1.20686773955822\n",
      "train loss: 3.9377138167619705 - val Loss: 1.206903725862503\n",
      "train loss: 3.9373849779367447 - val Loss: 1.2066635191440582\n",
      "train loss: 3.9369436502456665 - val Loss: 1.2067325860261917\n",
      "train loss: 3.9365836530923843 - val Loss: 1.2064517587423325\n",
      "train loss: 3.936091721057892 - val Loss: 1.2065304666757584\n",
      "train loss: 3.9357323050498962 - val Loss: 1.2063200622797012\n",
      "train loss: 3.93530635535717 - val Loss: 1.2063076049089432\n",
      "train loss: 3.9348399192094803 - val Loss: 1.205980271100998\n",
      "train loss: 3.934273272752762 - val Loss: 1.2060681283473969\n",
      "train loss: 3.9338314086198807 - val Loss: 1.2057837694883347\n",
      "train loss: 3.933324635028839 - val Loss: 1.20588618516922\n",
      "train loss: 3.932988315820694 - val Loss: 1.2055442035198212\n",
      "train loss: 3.9324833750724792 - val Loss: 1.2055015563964844\n",
      "train loss: 3.9320141673088074 - val Loss: 1.205145463347435\n",
      "train loss: 3.931506410241127 - val Loss: 1.2051856815814972\n",
      "train loss: 3.9311528354883194 - val Loss: 1.2051009386777878\n",
      "train loss: 3.9307249784469604 - val Loss: 1.2050275355577469\n",
      "train loss: 3.930334448814392 - val Loss: 1.2047100067138672\n",
      "train loss: 3.92966665327549 - val Loss: 1.2047309875488281\n",
      "train loss: 3.9293658286333084 - val Loss: 1.2046003639698029\n",
      "train loss: 3.928898647427559 - val Loss: 1.2044287025928497\n",
      "train loss: 3.928383395075798 - val Loss: 1.204297497868538\n",
      "train loss: 3.9279791563749313 - val Loss: 1.2041080445051193\n",
      "train loss: 3.9274025708436966 - val Loss: 1.2039509564638138\n",
      "train loss: 3.926982820034027 - val Loss: 1.203596591949463\n",
      "train loss: 3.9264550507068634 - val Loss: 1.2036910653114319\n",
      "train loss: 3.9260850846767426 - val Loss: 1.203563153743744\n",
      "train loss: 3.925726041197777 - val Loss: 1.203456625342369\n",
      "train loss: 3.9252763986587524 - val Loss: 1.203106313943863\n",
      "train loss: 3.92468498647213 - val Loss: 1.2032175660133362\n",
      "train loss: 3.9243195354938507 - val Loss: 1.2031239122152328\n",
      "train loss: 3.92387917637825 - val Loss: 1.2028008252382278\n",
      "train loss: 3.9233257472515106 - val Loss: 1.2028533071279526\n",
      "train loss: 3.9229311645030975 - val Loss: 1.2027582973241806\n",
      "train loss: 3.9224410206079483 - val Loss: 1.2024640887975693\n",
      "train loss: 3.922048196196556 - val Loss: 1.202580451965332\n",
      "train loss: 3.921594336628914 - val Loss: 1.2023159712553024\n",
      "train loss: 3.92114195227623 - val Loss: 1.2025331109762192\n",
      "train loss: 3.9207082092761993 - val Loss: 1.2022343873977661\n",
      "train loss: 3.920264482498169 - val Loss: 1.2023641169071198\n",
      "train loss: 3.9198542833328247 - val Loss: 1.2022475004196167\n",
      "train loss: 3.9193630516529083 - val Loss: 1.201885923743248\n",
      "train loss: 3.91887067258358 - val Loss: 1.202050805091858\n",
      "train loss: 3.918493330478668 - val Loss: 1.2019014209508896\n",
      "train loss: 3.9179812371730804 - val Loss: 1.2014904171228409\n",
      "train loss: 3.9174979478120804 - val Loss: 1.2019213140010834\n",
      "train loss: 3.9171734303236008 - val Loss: 1.2015025615692139\n",
      "train loss: 3.916666805744171 - val Loss: 1.2016973793506622\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x3025beca0>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train,val,over,cnt=[],[],float('inf'),0\n",
    "while(cnt<100):\n",
    "    t,v = trainAI(trainLoader,valLoader, model, loss_fn, optimizer)\n",
    "    if over<v:\n",
    "        cnt+=1\n",
    "    train.append(t)\n",
    "    val.append(v)\n",
    "    over = v\n",
    "print(\"Done!\")\n",
    "\n",
    "plt.plot(train,label='train')\n",
    "plt.plot(val, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2],\n",
       " [2, 0],\n",
       " [3, 8],\n",
       " [4, 7],\n",
       " [5, 3],\n",
       " [6, 7],\n",
       " [7, 0],\n",
       " [8, 3],\n",
       " [9, 0],\n",
       " [10, 3],\n",
       " [11, 5],\n",
       " [12, 7],\n",
       " [13, 4],\n",
       " [14, 0],\n",
       " [15, 4],\n",
       " [16, 3],\n",
       " [17, 3],\n",
       " [18, 1],\n",
       " [19, 9],\n",
       " [20, 0],\n",
       " [21, 9],\n",
       " [22, 1],\n",
       " [23, 1],\n",
       " [24, 5],\n",
       " [25, 7],\n",
       " [26, 4],\n",
       " [27, 2],\n",
       " [28, 7],\n",
       " [29, 7],\n",
       " [30, 7],\n",
       " [31, 7],\n",
       " [32, 9],\n",
       " [33, 4],\n",
       " [34, 2],\n",
       " [35, 6],\n",
       " [36, 2],\n",
       " [37, 5],\n",
       " [38, 5],\n",
       " [39, 1],\n",
       " [40, 5],\n",
       " [41, 7],\n",
       " [42, 7],\n",
       " [43, 4],\n",
       " [44, 9],\n",
       " [45, 8],\n",
       " [46, 7],\n",
       " [47, 8],\n",
       " [48, 2],\n",
       " [49, 6],\n",
       " [50, 7],\n",
       " [51, 6],\n",
       " [52, 8],\n",
       " [53, 8],\n",
       " [54, 3],\n",
       " [55, 8],\n",
       " [56, 2],\n",
       " [57, 1],\n",
       " [58, 2],\n",
       " [59, 2],\n",
       " [60, 5],\n",
       " [61, 4],\n",
       " [62, 1],\n",
       " [63, 7],\n",
       " [64, 0],\n",
       " [65, 0],\n",
       " [66, 0],\n",
       " [67, 1],\n",
       " [68, 9],\n",
       " [69, 0],\n",
       " [70, 1],\n",
       " [71, 6],\n",
       " [72, 5],\n",
       " [73, 8],\n",
       " [74, 8],\n",
       " [75, 2],\n",
       " [76, 8],\n",
       " [77, 9],\n",
       " [78, 9],\n",
       " [79, 2],\n",
       " [80, 3],\n",
       " [81, 5],\n",
       " [82, 4],\n",
       " [83, 1],\n",
       " [84, 0],\n",
       " [85, 9],\n",
       " [86, 2],\n",
       " [87, 4],\n",
       " [88, 3],\n",
       " [89, 6],\n",
       " [90, 7],\n",
       " [91, 2],\n",
       " [92, 0],\n",
       " [93, 6],\n",
       " [94, 6],\n",
       " [95, 1],\n",
       " [96, 4],\n",
       " [97, 3],\n",
       " [98, 9],\n",
       " [99, 7],\n",
       " [100, 4],\n",
       " [101, 0],\n",
       " [102, 3],\n",
       " [103, 2],\n",
       " [104, 0],\n",
       " [105, 7],\n",
       " [106, 3],\n",
       " [107, 0],\n",
       " [108, 5],\n",
       " [109, 0],\n",
       " [110, 9],\n",
       " [111, 0],\n",
       " [112, 0],\n",
       " [113, 4],\n",
       " [114, 7],\n",
       " [115, 1],\n",
       " [116, 7],\n",
       " [117, 1],\n",
       " [118, 1],\n",
       " [119, 3],\n",
       " [120, 3],\n",
       " [121, 3],\n",
       " [122, 7],\n",
       " [123, 2],\n",
       " [124, 8],\n",
       " [125, 6],\n",
       " [126, 3],\n",
       " [127, 8],\n",
       " [128, 7],\n",
       " [129, 1],\n",
       " [130, 4],\n",
       " [131, 3],\n",
       " [132, 5],\n",
       " [133, 6],\n",
       " [134, 0],\n",
       " [135, 0],\n",
       " [136, 0],\n",
       " [137, 3],\n",
       " [138, 1],\n",
       " [139, 5],\n",
       " [140, 6],\n",
       " [141, 4],\n",
       " [142, 3],\n",
       " [143, 4],\n",
       " [144, 5],\n",
       " [145, 5],\n",
       " [146, 8],\n",
       " [147, 7],\n",
       " [148, 7],\n",
       " [149, 2],\n",
       " [150, 8],\n",
       " [151, 4],\n",
       " [152, 3],\n",
       " [153, 5],\n",
       " [154, 6],\n",
       " [155, 5],\n",
       " [156, 3],\n",
       " [157, 7],\n",
       " [158, 5],\n",
       " [159, 7],\n",
       " [160, 8],\n",
       " [161, 3],\n",
       " [162, 0],\n",
       " [163, 4],\n",
       " [164, 5],\n",
       " [165, 1],\n",
       " [166, 3],\n",
       " [167, 7],\n",
       " [168, 6],\n",
       " [169, 3],\n",
       " [170, 0],\n",
       " [171, 2],\n",
       " [172, 7],\n",
       " [173, 8],\n",
       " [174, 6],\n",
       " [175, 1],\n",
       " [176, 3],\n",
       " [177, 7],\n",
       " [178, 4],\n",
       " [179, 1],\n",
       " [180, 2],\n",
       " [181, 4],\n",
       " [182, 8],\n",
       " [183, 5],\n",
       " [184, 2],\n",
       " [185, 4],\n",
       " [186, 9],\n",
       " [187, 2],\n",
       " [188, 1],\n",
       " [189, 6],\n",
       " [190, 0],\n",
       " [191, 6],\n",
       " [192, 1],\n",
       " [193, 4],\n",
       " [194, 9],\n",
       " [195, 6],\n",
       " [196, 0],\n",
       " [197, 9],\n",
       " [198, 7],\n",
       " [199, 6],\n",
       " [200, 4],\n",
       " [201, 1],\n",
       " [202, 9],\n",
       " [203, 0],\n",
       " [204, 9],\n",
       " [205, 9],\n",
       " [206, 0],\n",
       " [207, 8],\n",
       " [208, 4],\n",
       " [209, 6],\n",
       " [210, 2],\n",
       " [211, 0],\n",
       " [212, 9],\n",
       " [213, 3],\n",
       " [214, 6],\n",
       " [215, 8],\n",
       " [216, 2],\n",
       " [217, 1],\n",
       " [218, 6],\n",
       " [219, 3],\n",
       " [220, 4],\n",
       " [221, 2],\n",
       " [222, 3],\n",
       " [223, 1],\n",
       " [224, 2],\n",
       " [225, 2],\n",
       " [226, 9],\n",
       " [227, 4],\n",
       " [228, 6],\n",
       " [229, 1],\n",
       " [230, 0],\n",
       " [231, 0],\n",
       " [232, 4],\n",
       " [233, 9],\n",
       " [234, 1],\n",
       " [235, 7],\n",
       " [236, 3],\n",
       " [237, 2],\n",
       " [238, 3],\n",
       " [239, 8],\n",
       " [240, 6],\n",
       " [241, 8],\n",
       " [242, 6],\n",
       " [243, 2],\n",
       " [244, 8],\n",
       " [245, 5],\n",
       " [246, 5],\n",
       " [247, 9],\n",
       " [248, 8],\n",
       " [249, 3],\n",
       " [250, 5],\n",
       " [251, 9],\n",
       " [252, 7],\n",
       " [253, 1],\n",
       " [254, 3],\n",
       " [255, 8],\n",
       " [256, 4],\n",
       " [257, 5],\n",
       " [258, 1],\n",
       " [259, 4],\n",
       " [260, 5],\n",
       " [261, 6],\n",
       " [262, 3],\n",
       " [263, 3],\n",
       " [264, 5],\n",
       " [265, 7],\n",
       " [266, 0],\n",
       " [267, 6],\n",
       " [268, 8],\n",
       " [269, 3],\n",
       " [270, 1],\n",
       " [271, 6],\n",
       " [272, 0],\n",
       " [273, 6],\n",
       " [274, 3],\n",
       " [275, 9],\n",
       " [276, 5],\n",
       " [277, 1],\n",
       " [278, 5],\n",
       " [279, 8],\n",
       " [280, 4],\n",
       " [281, 0],\n",
       " [282, 9],\n",
       " [283, 2],\n",
       " [284, 0],\n",
       " [285, 5],\n",
       " [286, 3],\n",
       " [287, 7],\n",
       " [288, 8],\n",
       " [289, 9],\n",
       " [290, 9],\n",
       " [291, 5],\n",
       " [292, 7],\n",
       " [293, 7],\n",
       " [294, 9],\n",
       " [295, 9],\n",
       " [296, 6],\n",
       " [297, 3],\n",
       " [298, 0],\n",
       " [299, 3],\n",
       " [300, 3],\n",
       " [301, 6],\n",
       " [302, 9],\n",
       " [303, 8],\n",
       " [304, 2],\n",
       " [305, 6],\n",
       " [306, 3],\n",
       " [307, 7],\n",
       " [308, 1],\n",
       " [309, 4],\n",
       " [310, 5],\n",
       " [311, 8],\n",
       " [312, 5],\n",
       " [313, 9],\n",
       " [314, 0],\n",
       " [315, 0],\n",
       " [316, 3],\n",
       " [317, 8],\n",
       " [318, 8],\n",
       " [319, 1],\n",
       " [320, 8],\n",
       " [321, 4],\n",
       " [322, 1],\n",
       " [323, 1],\n",
       " [324, 9],\n",
       " [325, 8],\n",
       " [326, 4],\n",
       " [327, 5],\n",
       " [328, 1],\n",
       " [329, 5],\n",
       " [330, 7],\n",
       " [331, 6],\n",
       " [332, 3],\n",
       " [333, 1],\n",
       " [334, 3],\n",
       " [335, 0],\n",
       " [336, 9],\n",
       " [337, 5],\n",
       " [338, 0],\n",
       " [339, 6],\n",
       " [340, 0],\n",
       " [341, 6],\n",
       " [342, 7],\n",
       " [343, 1],\n",
       " [344, 8],\n",
       " [345, 6],\n",
       " [346, 0],\n",
       " [347, 6],\n",
       " [348, 5],\n",
       " [349, 2],\n",
       " [350, 2],\n",
       " [351, 6],\n",
       " [352, 7],\n",
       " [353, 7],\n",
       " [354, 2],\n",
       " [355, 5],\n",
       " [356, 8],\n",
       " [357, 8],\n",
       " [358, 9],\n",
       " [359, 2],\n",
       " [360, 7],\n",
       " [361, 8],\n",
       " [362, 6],\n",
       " [363, 3],\n",
       " [364, 8],\n",
       " [365, 4],\n",
       " [366, 2],\n",
       " [367, 3],\n",
       " [368, 8],\n",
       " [369, 1],\n",
       " [370, 6],\n",
       " [371, 4],\n",
       " [372, 8],\n",
       " [373, 4],\n",
       " [374, 9],\n",
       " [375, 7],\n",
       " [376, 6],\n",
       " [377, 9],\n",
       " [378, 5],\n",
       " [379, 3],\n",
       " [380, 7],\n",
       " [381, 6],\n",
       " [382, 5],\n",
       " [383, 5],\n",
       " [384, 4],\n",
       " [385, 2],\n",
       " [386, 6],\n",
       " [387, 2],\n",
       " [388, 1],\n",
       " [389, 3],\n",
       " [390, 7],\n",
       " [391, 1],\n",
       " [392, 7],\n",
       " [393, 9],\n",
       " [394, 9],\n",
       " [395, 6],\n",
       " [396, 1],\n",
       " [397, 1],\n",
       " [398, 1],\n",
       " [399, 7],\n",
       " [400, 3],\n",
       " [401, 9],\n",
       " [402, 7],\n",
       " [403, 6],\n",
       " [404, 1],\n",
       " [405, 1],\n",
       " [406, 1],\n",
       " [407, 9],\n",
       " [408, 3],\n",
       " [409, 5],\n",
       " [410, 5],\n",
       " [411, 5],\n",
       " [412, 0],\n",
       " [413, 4],\n",
       " [414, 1],\n",
       " [415, 2],\n",
       " [416, 3],\n",
       " [417, 1],\n",
       " [418, 1],\n",
       " [419, 3],\n",
       " [420, 5],\n",
       " [421, 9],\n",
       " [422, 6],\n",
       " [423, 6],\n",
       " [424, 5],\n",
       " [425, 3],\n",
       " [426, 1],\n",
       " [427, 4],\n",
       " [428, 7],\n",
       " [429, 7],\n",
       " [430, 7],\n",
       " [431, 4],\n",
       " [432, 8],\n",
       " [433, 5],\n",
       " [434, 2],\n",
       " [435, 6],\n",
       " [436, 1],\n",
       " [437, 3],\n",
       " [438, 9],\n",
       " [439, 5],\n",
       " [440, 0],\n",
       " [441, 8],\n",
       " [442, 4],\n",
       " [443, 7],\n",
       " [444, 4],\n",
       " [445, 4],\n",
       " [446, 4],\n",
       " [447, 1],\n",
       " [448, 5],\n",
       " [449, 3],\n",
       " [450, 9],\n",
       " [451, 5],\n",
       " [452, 7],\n",
       " [453, 6],\n",
       " [454, 9],\n",
       " [455, 5],\n",
       " [456, 9],\n",
       " [457, 2],\n",
       " [458, 3],\n",
       " [459, 5],\n",
       " [460, 6],\n",
       " [461, 1],\n",
       " [462, 7],\n",
       " [463, 5],\n",
       " [464, 0],\n",
       " [465, 5],\n",
       " [466, 1],\n",
       " [467, 7],\n",
       " [468, 4],\n",
       " [469, 4],\n",
       " [470, 1],\n",
       " [471, 1],\n",
       " [472, 4],\n",
       " [473, 9],\n",
       " [474, 5],\n",
       " [475, 6],\n",
       " [476, 0],\n",
       " [477, 1],\n",
       " [478, 3],\n",
       " [479, 1],\n",
       " [480, 0],\n",
       " [481, 4],\n",
       " [482, 8],\n",
       " [483, 1],\n",
       " [484, 2],\n",
       " [485, 9],\n",
       " [486, 9],\n",
       " [487, 4],\n",
       " [488, 8],\n",
       " [489, 3],\n",
       " [490, 7],\n",
       " [491, 0],\n",
       " [492, 4],\n",
       " [493, 2],\n",
       " [494, 4],\n",
       " [495, 6],\n",
       " [496, 7],\n",
       " [497, 5],\n",
       " [498, 3],\n",
       " [499, 2],\n",
       " [500, 0],\n",
       " [501, 6],\n",
       " [502, 5],\n",
       " [503, 9],\n",
       " [504, 4],\n",
       " [505, 1],\n",
       " [506, 8],\n",
       " [507, 3],\n",
       " [508, 3],\n",
       " [509, 0],\n",
       " [510, 6],\n",
       " [511, 7],\n",
       " [512, 5],\n",
       " [513, 8],\n",
       " [514, 7],\n",
       " [515, 5],\n",
       " [516, 3],\n",
       " [517, 5],\n",
       " [518, 7],\n",
       " [519, 4],\n",
       " [520, 3],\n",
       " [521, 4],\n",
       " [522, 9],\n",
       " [523, 0],\n",
       " [524, 7],\n",
       " [525, 7],\n",
       " [526, 1],\n",
       " [527, 0],\n",
       " [528, 1],\n",
       " [529, 1],\n",
       " [530, 7],\n",
       " [531, 0],\n",
       " [532, 5],\n",
       " [533, 3],\n",
       " [534, 8],\n",
       " [535, 8],\n",
       " [536, 5],\n",
       " [537, 6],\n",
       " [538, 5],\n",
       " [539, 4],\n",
       " [540, 3],\n",
       " [541, 0],\n",
       " [542, 2],\n",
       " [543, 8],\n",
       " [544, 2],\n",
       " [545, 0],\n",
       " [546, 3],\n",
       " [547, 0],\n",
       " [548, 9],\n",
       " [549, 2],\n",
       " [550, 1],\n",
       " [551, 1],\n",
       " [552, 3],\n",
       " [553, 0],\n",
       " [554, 5],\n",
       " [555, 0],\n",
       " [556, 0],\n",
       " [557, 7],\n",
       " [558, 5],\n",
       " [559, 6],\n",
       " [560, 2],\n",
       " [561, 0],\n",
       " [562, 3],\n",
       " [563, 8],\n",
       " [564, 1],\n",
       " [565, 6],\n",
       " [566, 5],\n",
       " [567, 4],\n",
       " [568, 1],\n",
       " [569, 1],\n",
       " [570, 4],\n",
       " [571, 6],\n",
       " [572, 5],\n",
       " [573, 3],\n",
       " [574, 6],\n",
       " [575, 0],\n",
       " [576, 4],\n",
       " [577, 8],\n",
       " [578, 2],\n",
       " [579, 4],\n",
       " [580, 2],\n",
       " [581, 5],\n",
       " [582, 1],\n",
       " [583, 7],\n",
       " [584, 6],\n",
       " [585, 9],\n",
       " [586, 1],\n",
       " [587, 7],\n",
       " [588, 3],\n",
       " [589, 8],\n",
       " [590, 0],\n",
       " [591, 8],\n",
       " [592, 8],\n",
       " [593, 4],\n",
       " [594, 5],\n",
       " [595, 3],\n",
       " [596, 6],\n",
       " [597, 6],\n",
       " [598, 6],\n",
       " [599, 0],\n",
       " [600, 3],\n",
       " [601, 5],\n",
       " [602, 1],\n",
       " [603, 7],\n",
       " [604, 1],\n",
       " [605, 6],\n",
       " [606, 2],\n",
       " [607, 8],\n",
       " [608, 5],\n",
       " [609, 6],\n",
       " [610, 4],\n",
       " [611, 7],\n",
       " [612, 4],\n",
       " [613, 3],\n",
       " [614, 3],\n",
       " [615, 2],\n",
       " [616, 4],\n",
       " [617, 7],\n",
       " [618, 0],\n",
       " [619, 0],\n",
       " [620, 9],\n",
       " [621, 8],\n",
       " [622, 5],\n",
       " [623, 9],\n",
       " [624, 4],\n",
       " [625, 0],\n",
       " [626, 8],\n",
       " [627, 3],\n",
       " [628, 3],\n",
       " [629, 6],\n",
       " [630, 8],\n",
       " [631, 6],\n",
       " [632, 1],\n",
       " [633, 8],\n",
       " [634, 6],\n",
       " [635, 1],\n",
       " [636, 4],\n",
       " [637, 7],\n",
       " [638, 7],\n",
       " [639, 8],\n",
       " [640, 3],\n",
       " [641, 0],\n",
       " [642, 9],\n",
       " [643, 9],\n",
       " [644, 6],\n",
       " [645, 4],\n",
       " [646, 7],\n",
       " [647, 4],\n",
       " [648, 4],\n",
       " [649, 1],\n",
       " [650, 8],\n",
       " [651, 4],\n",
       " [652, 8],\n",
       " [653, 0],\n",
       " [654, 2],\n",
       " [655, 8],\n",
       " [656, 2],\n",
       " [657, 4],\n",
       " [658, 3],\n",
       " [659, 3],\n",
       " [660, 7],\n",
       " [661, 2],\n",
       " [662, 3],\n",
       " [663, 4],\n",
       " [664, 0],\n",
       " [665, 9],\n",
       " [666, 8],\n",
       " [667, 1],\n",
       " [668, 3],\n",
       " [669, 3],\n",
       " [670, 6],\n",
       " [671, 3],\n",
       " [672, 9],\n",
       " [673, 4],\n",
       " [674, 3],\n",
       " [675, 8],\n",
       " [676, 7],\n",
       " [677, 7],\n",
       " [678, 2],\n",
       " [679, 6],\n",
       " [680, 0],\n",
       " [681, 6],\n",
       " [682, 9],\n",
       " [683, 8],\n",
       " [684, 1],\n",
       " [685, 1],\n",
       " [686, 3],\n",
       " [687, 9],\n",
       " [688, 6],\n",
       " [689, 9],\n",
       " [690, 9],\n",
       " [691, 2],\n",
       " [692, 6],\n",
       " [693, 0],\n",
       " [694, 1],\n",
       " [695, 8],\n",
       " [696, 4],\n",
       " [697, 3],\n",
       " [698, 9],\n",
       " [699, 8],\n",
       " [700, 8],\n",
       " [701, 4],\n",
       " [702, 0],\n",
       " [703, 5],\n",
       " [704, 0],\n",
       " [705, 6],\n",
       " [706, 0],\n",
       " [707, 9],\n",
       " [708, 4],\n",
       " [709, 6],\n",
       " [710, 5],\n",
       " [711, 3],\n",
       " [712, 8],\n",
       " [713, 1],\n",
       " [714, 5],\n",
       " [715, 3],\n",
       " [716, 6],\n",
       " [717, 2],\n",
       " [718, 3],\n",
       " [719, 7],\n",
       " [720, 8],\n",
       " [721, 9],\n",
       " [722, 3],\n",
       " [723, 1],\n",
       " [724, 0],\n",
       " [725, 1],\n",
       " [726, 0],\n",
       " [727, 6],\n",
       " [728, 4],\n",
       " [729, 7],\n",
       " [730, 5],\n",
       " [731, 7],\n",
       " [732, 1],\n",
       " [733, 3],\n",
       " [734, 2],\n",
       " [735, 7],\n",
       " [736, 7],\n",
       " [737, 1],\n",
       " [738, 5],\n",
       " [739, 1],\n",
       " [740, 5],\n",
       " [741, 4],\n",
       " [742, 2],\n",
       " [743, 3],\n",
       " [744, 4],\n",
       " [745, 3],\n",
       " [746, 9],\n",
       " [747, 0],\n",
       " [748, 7],\n",
       " [749, 8],\n",
       " [750, 6],\n",
       " [751, 4],\n",
       " [752, 9],\n",
       " [753, 4],\n",
       " [754, 4],\n",
       " [755, 1],\n",
       " [756, 4],\n",
       " [757, 7],\n",
       " [758, 1],\n",
       " [759, 1],\n",
       " [760, 8],\n",
       " [761, 7],\n",
       " [762, 0],\n",
       " [763, 4],\n",
       " [764, 0],\n",
       " [765, 4],\n",
       " [766, 0],\n",
       " [767, 0],\n",
       " [768, 5],\n",
       " [769, 1],\n",
       " [770, 8],\n",
       " [771, 6],\n",
       " [772, 5],\n",
       " [773, 0],\n",
       " [774, 1],\n",
       " [775, 5],\n",
       " [776, 3],\n",
       " [777, 4],\n",
       " [778, 6],\n",
       " [779, 3],\n",
       " [780, 1],\n",
       " [781, 1],\n",
       " [782, 6],\n",
       " [783, 9],\n",
       " [784, 8],\n",
       " [785, 3],\n",
       " [786, 5],\n",
       " [787, 5],\n",
       " [788, 4],\n",
       " [789, 8],\n",
       " [790, 5],\n",
       " [791, 5],\n",
       " [792, 0],\n",
       " [793, 4],\n",
       " [794, 0],\n",
       " [795, 4],\n",
       " [796, 3],\n",
       " [797, 1],\n",
       " [798, 6],\n",
       " [799, 9],\n",
       " [800, 7],\n",
       " [801, 1],\n",
       " [802, 1],\n",
       " [803, 3],\n",
       " [804, 3],\n",
       " [805, 1],\n",
       " [806, 4],\n",
       " [807, 9],\n",
       " [808, 6],\n",
       " [809, 9],\n",
       " [810, 1],\n",
       " [811, 5],\n",
       " [812, 4],\n",
       " [813, 2],\n",
       " [814, 3],\n",
       " [815, 2],\n",
       " [816, 4],\n",
       " [817, 0],\n",
       " [818, 9],\n",
       " [819, 7],\n",
       " [820, 4],\n",
       " [821, 3],\n",
       " [822, 0],\n",
       " [823, 5],\n",
       " [824, 0],\n",
       " [825, 1],\n",
       " [826, 9],\n",
       " [827, 0],\n",
       " [828, 4],\n",
       " [829, 5],\n",
       " [830, 2],\n",
       " [831, 8],\n",
       " [832, 4],\n",
       " [833, 5],\n",
       " [834, 9],\n",
       " [835, 3],\n",
       " [836, 9],\n",
       " [837, 6],\n",
       " [838, 1],\n",
       " [839, 5],\n",
       " [840, 1],\n",
       " [841, 1],\n",
       " [842, 9],\n",
       " [843, 0],\n",
       " [844, 8],\n",
       " [845, 4],\n",
       " [846, 6],\n",
       " [847, 7],\n",
       " [848, 2],\n",
       " [849, 8],\n",
       " [850, 5],\n",
       " [851, 8],\n",
       " [852, 9],\n",
       " [853, 9],\n",
       " [854, 7],\n",
       " [855, 2],\n",
       " [856, 5],\n",
       " [857, 1],\n",
       " [858, 3],\n",
       " [859, 4],\n",
       " [860, 5],\n",
       " [861, 0],\n",
       " [862, 4],\n",
       " [863, 1],\n",
       " [864, 4],\n",
       " [865, 3],\n",
       " [866, 3],\n",
       " [867, 6],\n",
       " [868, 9],\n",
       " [869, 2],\n",
       " [870, 3],\n",
       " [871, 4],\n",
       " [872, 5],\n",
       " [873, 4],\n",
       " [874, 2],\n",
       " [875, 3],\n",
       " [876, 9],\n",
       " [877, 1],\n",
       " [878, 1],\n",
       " [879, 0],\n",
       " [880, 1],\n",
       " [881, 4],\n",
       " [882, 9],\n",
       " [883, 1],\n",
       " [884, 1],\n",
       " [885, 2],\n",
       " [886, 7],\n",
       " [887, 1],\n",
       " [888, 5],\n",
       " [889, 4],\n",
       " [890, 9],\n",
       " [891, 1],\n",
       " [892, 7],\n",
       " [893, 6],\n",
       " [894, 0],\n",
       " [895, 4],\n",
       " [896, 2],\n",
       " [897, 9],\n",
       " [898, 4],\n",
       " [899, 1],\n",
       " [900, 1],\n",
       " [901, 5],\n",
       " [902, 3],\n",
       " [903, 5],\n",
       " [904, 7],\n",
       " [905, 4],\n",
       " [906, 7],\n",
       " [907, 8],\n",
       " [908, 3],\n",
       " [909, 2],\n",
       " [910, 7],\n",
       " [911, 2],\n",
       " [912, 0],\n",
       " [913, 4],\n",
       " [914, 7],\n",
       " [915, 1],\n",
       " [916, 6],\n",
       " [917, 4],\n",
       " [918, 5],\n",
       " [919, 1],\n",
       " [920, 5],\n",
       " [921, 7],\n",
       " [922, 3],\n",
       " [923, 8],\n",
       " [924, 9],\n",
       " [925, 4],\n",
       " [926, 7],\n",
       " [927, 9],\n",
       " [928, 6],\n",
       " [929, 6],\n",
       " [930, 3],\n",
       " [931, 3],\n",
       " [932, 2],\n",
       " [933, 1],\n",
       " [934, 4],\n",
       " [935, 5],\n",
       " [936, 3],\n",
       " [937, 7],\n",
       " [938, 7],\n",
       " [939, 9],\n",
       " [940, 5],\n",
       " [941, 6],\n",
       " [942, 0],\n",
       " [943, 6],\n",
       " [944, 1],\n",
       " [945, 0],\n",
       " [946, 9],\n",
       " [947, 3],\n",
       " [948, 2],\n",
       " [949, 9],\n",
       " [950, 2],\n",
       " [951, 6],\n",
       " [952, 7],\n",
       " [953, 5],\n",
       " [954, 2],\n",
       " [955, 3],\n",
       " [956, 2],\n",
       " [957, 8],\n",
       " [958, 3],\n",
       " [959, 0],\n",
       " [960, 2],\n",
       " [961, 7],\n",
       " [962, 9],\n",
       " [963, 4],\n",
       " [964, 0],\n",
       " [965, 9],\n",
       " [966, 5],\n",
       " [967, 1],\n",
       " [968, 8],\n",
       " [969, 8],\n",
       " [970, 5],\n",
       " [971, 3],\n",
       " [972, 2],\n",
       " [973, 9],\n",
       " [974, 6],\n",
       " [975, 7],\n",
       " [976, 0],\n",
       " [977, 8],\n",
       " [978, 0],\n",
       " [979, 7],\n",
       " [980, 4],\n",
       " [981, 5],\n",
       " [982, 8],\n",
       " [983, 7],\n",
       " [984, 9],\n",
       " [985, 7],\n",
       " [986, 7],\n",
       " [987, 0],\n",
       " [988, 5],\n",
       " [989, 3],\n",
       " [990, 2],\n",
       " [991, 1],\n",
       " [992, 4],\n",
       " [993, 0],\n",
       " [994, 6],\n",
       " [995, 8],\n",
       " [996, 3],\n",
       " [997, 6],\n",
       " [998, 2],\n",
       " [999, 2],\n",
       " [1000, 9],\n",
       " ...]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testAI(dataloader, model):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    y=1\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            X  = X.to(device)\n",
    "            pred = model(X)\n",
    "            out.append([y, torch.argmax(pred).item()])\n",
    "            y+=1\n",
    "    \n",
    "    return out\n",
    "\n",
    "result = testAI(testLoader, model)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      8\n",
       "3            4      7\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(result)\n",
    "result = result.astype(int)\n",
    "result.columns=['ImageId','Label']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
